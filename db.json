{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/uploads/images/Bias variance on mismatched training and dev test sets.png","path":"uploads/images/Bias variance on mismatched training and dev test sets.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Capsule-squash.png","path":"uploads/images/Capsule-squash.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Capsule-tree.png","path":"uploads/images/Capsule-tree.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Capsule-tree_parts_diagram.png","path":"uploads/images/Capsule-tree_parts_diagram.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Improving your model performance.png","path":"uploads/images/Improving your model performance.png","modified":1,"renderable":0},{"_id":"source/uploads/images/RNN unit.png","path":"uploads/images/RNN unit.png","modified":1,"renderable":0},{"_id":"source/uploads/images/RNNunit.png","path":"uploads/images/RNNunit.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Routing by agreement, step 1.png","path":"uploads/images/Routing by agreement, step 1.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Routing by agreement, step 2.png","path":"uploads/images/Routing by agreement, step 2.png","modified":1,"renderable":0},{"_id":"source/uploads/images/TextCNN-multichannel.svg","path":"uploads/images/TextCNN-multichannel.svg","modified":1,"renderable":0},{"_id":"source/uploads/images/avatar.jpeg","path":"uploads/images/avatar.jpeg","modified":1,"renderable":0},{"_id":"source/uploads/images/rnn cell backprop.png","path":"uploads/images/rnn cell backprop.png","modified":1,"renderable":0},{"_id":"source/uploads/images/RNN architectures.png","path":"uploads/images/RNN architectures.png","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"source/uploads/images/Analogies using word vectors.png","path":"uploads/images/Analogies using word vectors.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Beam search(B=3).png","path":"uploads/images/Beam search(B=3).png","modified":1,"renderable":0},{"_id":"source/uploads/images/Chain of assumptions in ML.png","path":"uploads/images/Chain of assumptions in ML.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Glove Model.png","path":"uploads/images/Glove Model.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Machine translation.png","path":"uploads/images/Machine translation.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Negative Sampling.png","path":"uploads/images/Negative Sampling.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Other context-target pairs.png","path":"uploads/images/Other context-target pairs.png","modified":1,"renderable":0},{"_id":"source/uploads/images/RNN Forward Propagation.png","path":"uploads/images/RNN Forward Propagation.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Forward propagation and backpropagation.png","path":"uploads/images/Forward propagation and backpropagation.png","modified":1,"renderable":0},{"_id":"source/uploads/images/RNN model.png","path":"uploads/images/RNN model.png","modified":1,"renderable":0},{"_id":"source/uploads/images/RNN sentiment classification.png","path":"uploads/images/RNN sentiment classification.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Routing by agreement can parse crowded scenes.png","path":"uploads/images/Routing by agreement can parse crowded scenes.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Sampling a sequence from a trained RNN.png","path":"uploads/images/Sampling a sequence from a trained RNN.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Simple sentiment classification model.png","path":"uploads/images/Simple sentiment classification model.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Word2Vec.png","path":"uploads/images/Word2Vec.png","modified":1,"renderable":0},{"_id":"source/uploads/images/fullGRU.png","path":"uploads/images/fullGRU.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Bleu score.png","path":"uploads/images/Bleu score.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Attention.png","path":"uploads/images/Attention.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Computing attention alpha.png","path":"uploads/images/Computing attention alpha.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Beam search algorithm.png","path":"uploads/images/Beam search algorithm.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Embending matrix.png","path":"uploads/images/Embending matrix.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Error analysis on beam search.png","path":"uploads/images/Error analysis on beam search.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Neural language model.png","path":"uploads/images/Neural language model.png","modified":1,"renderable":0},{"_id":"source/uploads/images/TextCNN.png","path":"uploads/images/TextCNN.png","modified":1,"renderable":0},{"_id":"source/uploads/images/Word Embedding.png","path":"uploads/images/Word Embedding.png","modified":1,"renderable":0},{"_id":"source/uploads/images/deepRNN.png","path":"uploads/images/deepRNN.png","modified":1,"renderable":0},{"_id":"source/uploads/images/bias in word embedding.png","path":"uploads/images/bias in word embedding.png","modified":1,"renderable":0},{"_id":"source/uploads/images/two-layer CapsNet.png","path":"uploads/images/two-layer CapsNet.png","modified":1,"renderable":0},{"_id":"source/uploads/images/featurization view of word embeddings.png","path":"uploads/images/featurization view of word embeddings.png","modified":1,"renderable":0},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/clicklove.js","path":"js/src/clicklove.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/crash_cheat.js","path":"js/src/crash_cheat.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"source/uploads/images/GRU(simplified).png","path":"uploads/images/GRU(simplified).png","modified":1,"renderable":0},{"_id":"source/uploads/images/TextCnn.png","path":"uploads/images/TextCnn.png","modified":1,"renderable":0},{"_id":"source/uploads/images/LSTM.png","path":"uploads/images/LSTM.png","modified":1,"renderable":0},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1544925409610},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1544925409610},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1544925409614},{"_id":"themes/next/.gitignore","hash":"0b5c2ffd41f66eb1849d6426ba8cf9649eeed329","modified":1544925409614},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1544925409614},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1544925409614},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1544925409614},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1544925409614},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1544925409614},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1544925409614},{"_id":"themes/next/README.cn.md","hash":"2c766b3369ed477bce134a5450dab45bef161504","modified":1544925409614},{"_id":"themes/next/README.md","hash":"8ce60ce578963eb4e1eb5e33e1efc2fc4779af9c","modified":1544925409614},{"_id":"themes/next/_config.yml","hash":"228b6e44c7fa6f44aaca9792644613b2092ec5f8","modified":1553007341122},{"_id":"themes/next/bower.json","hash":"0674f11d3d514e087a176da0e1d85c2286aa5fba","modified":1544925409614},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1544925409614},{"_id":"themes/next/package.json","hash":"036d3a1346203d2f1a3958024df7f74e7ac07bfe","modified":1544925409618},{"_id":"source/_posts/Dynamic-Routing-Between-Capsules.md","hash":"2f78eecdbd731d59b7dcd0720a4784765d27acee","modified":1553007804855},{"_id":"source/_posts/Convolutional-Neural-Networks-for-Sentence-Classification.md","hash":"1a02ed003a4bd49366c8288eddf282b0c892b879","modified":1553005172040},{"_id":"source/_posts/Hello-World.md","hash":"5ea7c6cdb11d1a5d689430e587858a1e7d379a44","modified":1553004902105},{"_id":"source/_posts/Structuring-Machine-Learning-Projects.md","hash":"75805a80ccf6d36ae9f1b8a59767049622bf756e","modified":1553004288685},{"_id":"source/categories/index.md","hash":"89d94e7b1926e4a41019cd91e98ad1531df0fe75","modified":1545029459553},{"_id":"source/_posts/Sequence-Models.md","hash":"d2dd009bd2fd86d230dc4d96ea54e28236ac69e6","modified":1553004848682},{"_id":"source/top10/index.md","hash":"2e1e1dea0ba963fd90bb7dd958387eea83904340","modified":1545046284547},{"_id":"source/tags/index.md","hash":"a6c5239685ebd56d515905563d7043f2cc45cf4a","modified":1545029469257},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1544925409598},{"_id":"themes/next/.git/config","hash":"91b6a53b2a7f929b698734717a38d4ac169f0c1f","modified":1544925409598},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1544925381526},{"_id":"themes/next/.git/packed-refs","hash":"69237944e31c16fe545d1f47b0b1e5b1d99660da","modified":1544925409598},{"_id":"themes/next/.git/index","hash":"49ded55c3eae60fc1609d82eec19b0a515441501","modified":1544968579445},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1544925409614},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1544925409614},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1544925409614},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1544925409614},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1544925409614},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1544925409614},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1544925409614},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1544925409614},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1544925409614},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1544925409614},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1544925409614},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1544925409614},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1544925409614},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1544925409614},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1544925409614},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1544925409614},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1544925409614},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"50d48c47162817a3810a9d9ad51104e83947419a","modified":1544925409614},{"_id":"themes/next/layout/_layout.swig","hash":"f8f0f19a66bf52e7a99fad3deb1da3fc0240d763","modified":1545032737892},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1544925409618},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1544925409618},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1544925409618},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1544925409618},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1544925409618},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1544925409618},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1544925409618},{"_id":"themes/next/scripts/merge-configs.js","hash":"81e86717ecfb775986b945d17f0a4ba27532ef07","modified":1544925409618},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1544925409618},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1544925409646},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1544925409646},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1544925409646},{"_id":"themes/next/languages/zh-Hans.yml","hash":"d892ee0db381f08e96d5cbfe0323c6018bbeb754","modified":1545023186578},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1544925409614},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544925409626},{"_id":"source/uploads/images/Bias variance on mismatched training and dev test sets.png","hash":"2036476b514dfa1afa59152b0a40a6635a87d7f4","modified":1545287667367},{"_id":"source/uploads/images/Capsule-squash.png","hash":"e1ac6ba5ce3bf7df6f0896a6d1dca4cbc563900e","modified":1546850071442},{"_id":"source/uploads/images/Capsule-tree.png","hash":"85bf5ec1c0198a00485a88080d678e36c40b34e3","modified":1546843635575},{"_id":"source/uploads/images/Capsule-tree_parts_diagram.png","hash":"c1766c414905d195e6802ef7ab4d31d525e1735f","modified":1546843609039},{"_id":"source/uploads/images/Improving your model performance.png","hash":"c9478db19cf2aff4789f122a7dae0c7e528ca0c4","modified":1545303465261},{"_id":"source/uploads/images/RNN unit.png","hash":"d65f7e78fda2068fd225bb7312fdfaa27205bfe2","modified":1545371414332},{"_id":"source/uploads/images/RNNunit.png","hash":"dc2858ddc54b7ea438976716b5dc47a92a2e139f","modified":1545371444300},{"_id":"source/uploads/images/Routing by agreement, step 1.png","hash":"462836c8e654265d0ec23a4301009f2419839dc4","modified":1546847853316},{"_id":"source/uploads/images/Routing by agreement, step 2.png","hash":"9561aa05a0a4054409d481f9cf4467d231f4178f","modified":1546847896276},{"_id":"source/uploads/images/TextCNN-multichannel.svg","hash":"04527a3e69227bc60ec0197ba946b998419f7456","modified":1547264553567},{"_id":"source/uploads/images/avatar.jpeg","hash":"8b1bcec1e1cc77f031290c27af3186835e632cbc","modified":1547565456053},{"_id":"source/uploads/images/rnn cell backprop.png","hash":"2827be8aad5f3ec20b079966c3fee393a48d35fa","modified":1545370993269},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1544925381522},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1544925381522},{"_id":"themes/next/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1544925381522},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1544925381522},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1544925381522},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1544925381522},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1544925381522},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1544925381522},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1544925381522},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1544925381522},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1544925381522},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1544925381526},{"_id":"themes/next/.git/logs/HEAD","hash":"7f24553d61807c6cacd8d72c0d6fa4e121acfcdc","modified":1544925409598},{"_id":"source/uploads/images/RNN architectures.png","hash":"00bd00375de60d8fdade858542f417041d43b6d1","modified":1545364219334},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1544925409614},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1544925409614},{"_id":"themes/next/layout/_macro/passage-end-tag.swig","hash":"959d10b460e1d32acc427f5ff53b4c1a06ffedd7","modified":1544969032154},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1544925409614},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1544925409614},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"6a54c3c85ff6b19d275827a327abbf4bd99b2ebf","modified":1544925409614},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1544925409614},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1544925409614},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1544925409614},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1544925409618},{"_id":"themes/next/layout/_partials/comments.swig","hash":"959b1734cf4904f22e73de2f99fa55f0569ea080","modified":1545018283930},{"_id":"themes/next/layout/_partials/footer.swig","hash":"549ad241a2fc416e87728344a8a36d826802c27d","modified":1545312995843},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1544925409614},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1544925409614},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1544925409614},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1544925409614},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1544925409614},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1544925409618},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1544925409618},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1544925409618},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1544925409618},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1544925409618},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1544925409618},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1544925409618},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1544925409626},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1544925409618},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1544925409618},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1544925409618},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1544925409618},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1544925409618},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1544925409618},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1544925409618},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1544925409618},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1544925409618},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1544925409626},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1544925409626},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1544925409626},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1544925409626},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1544925409626},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1544925409626},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1544925409626},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1544925409626},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1544925409626},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1544925409626},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1544925409626},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1544925409626},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1544925409626},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1544925409626},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1544925409626},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1544925409626},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1544925409626},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1544925409626},{"_id":"source/uploads/images/Analogies using word vectors.png","hash":"81d015515f0abf3d243b1c160681e7034fed4cac","modified":1545566452309},{"_id":"source/uploads/images/Beam search(B=3).png","hash":"737f9e47fda4dc20e2864536ac04763a65a79951","modified":1545729422178},{"_id":"source/uploads/images/Chain of assumptions in ML.png","hash":"1bdd85455dcc364b18c20f83f02737c3be31dd65","modified":1545305884039},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1544925409614},{"_id":"themes/next/layout/_macro/post.swig","hash":"30358e7f1758f83cb513afb6df32cc7f1a576615","modified":1545036484037},{"_id":"source/uploads/images/Glove Model.png","hash":"8bb43c477fe3472dd73b26e40e2bfe411b555e2e","modified":1545575851373},{"_id":"source/uploads/images/Machine translation.png","hash":"99fb149798782ff8e1afba9dc71135a402c8cc8b","modified":1545728598721},{"_id":"source/uploads/images/Negative Sampling.png","hash":"d5728be0c92d3055b1e89d95459f96796f55808c","modified":1545572851484},{"_id":"source/uploads/images/Other context-target pairs.png","hash":"8f7f053d6734651864c1616a0b8149ff6c3a3330","modified":1545567926055},{"_id":"source/uploads/images/RNN Forward Propagation.png","hash":"4ce0cc388677d7075c88f9bac98b923762e8ad49","modified":1545363151002},{"_id":"source/uploads/images/Forward propagation and backpropagation.png","hash":"eae4b283733b9c29cba2e341735bcc74f7e2a7bf","modified":1545363558488},{"_id":"source/uploads/images/RNN model.png","hash":"a369c8aac94932899b845d6b50388bf70753aee5","modified":1545364840175},{"_id":"source/uploads/images/RNN sentiment classification.png","hash":"7792c188db11b177ccdf838f04e2a906dfa12934","modified":1545577642216},{"_id":"source/uploads/images/Routing by agreement can parse crowded scenes.png","hash":"c146ded9298ad4de95f8af969bebb60456c4913b","modified":1546848756261},{"_id":"source/uploads/images/Sampling a sequence from a trained RNN.png","hash":"2b2f26ea86b26e0ae2d51367fc1ab52828eb0bd4","modified":1545365379962},{"_id":"source/uploads/images/Simple sentiment classification model.png","hash":"178d74502c40499b70ebbffac07e5daf8cf10808","modified":1545577487858},{"_id":"source/uploads/images/Word2Vec.png","hash":"35a7b42454e7b916eac274a71598705f5e972954","modified":1545572040227},{"_id":"source/uploads/images/fullGRU.png","hash":"3788a7ceb6242548536196c3c0681eca45201c80","modified":1545371543815},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544925409618},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544925409618},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544925409622},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544925409622},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544925409626},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544925409626},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544925409626},{"_id":"source/uploads/images/Bleu score.png","hash":"fca81403c6bde8cffca29078643bbb0e9c180a94","modified":1545741776761},{"_id":"source/uploads/images/Attention.png","hash":"6598b94abfdd9af299af21aea24d6fb743f7e36f","modified":1545744573016},{"_id":"source/uploads/images/Computing attention alpha.png","hash":"7c6c33d8a50d0de1eb181ba96524fcc7dd611584","modified":1545744611099},{"_id":"source/uploads/images/Beam search algorithm.png","hash":"2d1780b8ea9221059f7cee3cf26d751cddf116be","modified":1545729389038},{"_id":"source/uploads/images/Embending matrix.png","hash":"36aa63034360df36ee3c9a9c654812fca9305b60","modified":1545566908612},{"_id":"source/uploads/images/Error analysis on beam search.png","hash":"679bb641401e07b64d0082b8f5aee3b64ee2a932","modified":1545740742018},{"_id":"source/uploads/images/Neural language model.png","hash":"bccb0869f264cfad1f9e682c0c8af157b6daf2ca","modified":1545567917931},{"_id":"source/uploads/images/TextCNN.png","hash":"03b3d81c02b0e692c79649729ba4515d8bc97e00","modified":1547264006825},{"_id":"source/uploads/images/Word Embedding.png","hash":"a7b56ee4b23b826061d68ed495bdb7de81dc0e2f","modified":1545564923207},{"_id":"source/uploads/images/deepRNN.png","hash":"149f21ac2ae6e2498dfc3440d93bd420ec70f104","modified":1545374081092},{"_id":"source/uploads/images/bias in word embedding.png","hash":"60475df7278df6bfa9d4ea277c5fd5de6d95f75e","modified":1545578772531},{"_id":"source/uploads/images/two-layer CapsNet.png","hash":"3cc3b791e480dbe24ea9f99388ee2d7546710ac7","modified":1546844850292},{"_id":"source/uploads/images/featurization view of word embeddings.png","hash":"71b03aa532d83328e45176ad55cd636e3a52d87e","modified":1545576593874},{"_id":"themes/next/.git/refs/heads/master","hash":"7999da428ebb87e5a2b27315d8d5123c1ccdfaa5","modified":1544925409598},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1544925409614},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1544925409614},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1544925409618},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1544925409614},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1544925409614},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1544925409614},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1544925409614},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1544925409614},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1544925409614},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1544925409614},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1544925409614},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1544925409614},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1544925409618},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1544925409618},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1544925409618},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1544925409618},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1544925409618},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1544925409618},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1544925409618},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1544925409618},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1544925409618},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1544925409618},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1544925409618},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1544925409618},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1544925409618},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1544925409618},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1544925409618},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1544925409618},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1544925409618},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1544925409618},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1544925409618},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"4631f162a033548219a8ac21d921f80dc94420ec","modified":1545017639259},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1544925409618},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1544925409618},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1544925409618},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1544925409618},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1544925409618},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1544925409618},{"_id":"themes/next/source/css/_custom/custom(复件).styl","hash":"1033f5790d80c1627029d675a854d75b766a4e23","modified":1545032160376},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"d085dd9e4151037f8104a78ef000e177633952b9","modified":1545035499853},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1544925409622},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1544925409622},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1544925409626},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1544925409626},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"1987af9e0821a58f65adc58ecc96859a064309bb","modified":1545034431522},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1544925409626},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1544925409626},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1544925409626},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1544925409626},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1544925409626},{"_id":"themes/next/source/js/src/clicklove.js","hash":"effa770d8085f7e7fb903de217ce521dff163780","modified":1544970086161},{"_id":"themes/next/source/js/src/crash_cheat.js","hash":"38450bee383082cab5fe63002af5c8855327a157","modified":1544968608965},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1544925409626},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1544925409626},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1544925409626},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1544925409626},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1544925409626},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1544925409630},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1544925409630},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1544925409630},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1544925409630},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1544925409630},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1544925409630},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1544925409634},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1544925409634},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1544925409634},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1544925409634},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1544925409634},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1544925409634},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1544925409634},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1544925409634},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1544925409634},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1544925409638},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1544925409642},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1544925409642},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1544925409642},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1544925409638},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1544925409638},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1544925409638},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1544925409642},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1544925409642},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1544925409642},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1544925409642},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1544925409638},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1544925409642},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1544925409642},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1544925409642},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1544925409642},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1544925409642},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1544925409642},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1544925409642},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1544925409642},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1544925409642},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1544925409642},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1544925409642},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1544925409642},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1544925409642},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1544925409642},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1544925409642},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1544925409642},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1544925409646},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1544925409646},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1544925409646},{"_id":"source/uploads/images/GRU(simplified).png","hash":"fb1e8877cc004108512935c3e705764479668815","modified":1545371492964},{"_id":"source/uploads/images/TextCnn.png","hash":"65622614aa0fff23d65327b8f5a1b448e0134074","modified":1547265646067},{"_id":"source/uploads/images/LSTM.png","hash":"6e94db4124e58fe63a2686935968195b9b52f638","modified":1545372642240},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1544925409638},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1544925409638},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"7f24553d61807c6cacd8d72c0d6fa4e121acfcdc","modified":1544925409598},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1544925409598},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1544925409618},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1544925409618},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1544925409618},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1544925409618},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1544925409618},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1544925409618},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1544925409618},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1544925409622},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1544925409622},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1544925409622},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1544925409622},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1544925409622},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1544925409622},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1544925409622},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1544925409622},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"7b90722c626bc07c3bdc8ec118a45cbcb2e59b05","modified":1545036204678},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"4aac01962520d60b03b23022ab601ad4bd19c08c","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1544925409626},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1544925409626},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1544925409630},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1544925409634},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1544925409634},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1544925409634},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1544925409634},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1544925409634},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1544925409634},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1544925409634},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1544925409634},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1544925409642},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1544925409642},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1544925409630},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1544925409638},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1544925409638},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1544925409646},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"7f24553d61807c6cacd8d72c0d6fa4e121acfcdc","modified":1544925409598},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"10ea0a68a923c10b8bdfc507666540e8aeb25afe","modified":1545035873180},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"e5faaf64d52bbfc102d7a086b24ad07120a7e621","modified":1545032900171},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1544925409622},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1544925409622},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1544925409626},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1544925409626},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1544925409630},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1544925409630},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1544925409630},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1544925409630},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1544925409630},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1544925409630},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1544925409630},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1544925409638},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1544925409634},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1544925409634},{"_id":"themes/next/.git/objects/pack/pack-e6ce7212de84e2efbe5edd1d76c67d8f85e0c64b.idx","hash":"b779adcea0d2d44907696488f5a5736d93697a21","modified":1544925409382},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1544925409630},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1544925409642},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1544925409638},{"_id":"themes/next/.git/objects/pack/pack-e6ce7212de84e2efbe5edd1d76c67d8f85e0c64b.pack","hash":"8cdb507cc872b5a3eb5bca4807d4faa43e185188","modified":1544925409378}],"Category":[{"name":"深度学习","_id":"cjtfwr9hw000449zfrz0x17mw"},{"name":"其他","_id":"cjtfwr9i2000849zfj54eczz7"},{"name":"NLP","parent":"cjtfwr9hw000449zfrz0x17mw","_id":"cjtfwr9i9000c49zf5c54r7y8"},{"name":"CV","parent":"cjtfwr9hw000449zfrz0x17mw","_id":"cjtfwr9ow001149zfbcrvas7k"}],"Data":[],"Page":[{"title":"top10","date":"2018-12-15T15:20:16.000Z","type":"top10","comments":0,"mathjax":false,"_content":"\n<div id=\"top\"></div>\n<script src=\"https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js\"></script>\n<script>AV.initialize(\"mW6MTVtJX0wR4pitdPFEST98-gzGzoHsz\", \"hH4sKpU65t4ptsnIyX0z34l6\");</script>\n<script type=\"text/javascript\">\n  var time=0\n  var title=\"\"\n  var url=\"\"\n  var query = new AV.Query('Counter');\n  query.notEqualTo('id',0);\n  query.descending('time');\n  query.limit(1000);\n  query.find().then(function (todo) {\n    for (var i=0;i<1000;i++){\n      var result=todo[i].attributes;\n      time=result.time;\n      title=result.title;\n      url=result.url;\n      // var content=\"<a href='\"+\"https://tw19941212.github.io\"+url+\"'>\"+title+\"</a>\"+\"<br>\"+\"<font color='#fff'>\"+\"阅读次数：\"+time+\"</font>\"+\"<br><br>\";\n      var content=\"<p>\"+\"<font color='#1C1C1C'>\"+\"【文章热度:\"+time+\"℃】\"+\"</font>\"+\"<a href='\"+\"https://tw19941212.github.io\"+url+\"'>\"+title+\"</a>\"+\"</p>\";\n      document.getElementById(\"top\").innerHTML+=content\n    }\n  }, function (error) {\n    console.log(\"error\");\n  });\n</script>","source":"top10/index.md","raw":"---\ntitle: top10\ndate: 2018-12-15 23:20:16\ntype:  \"top10\"\ncomments: false\nmathjax: false\n---\n\n<div id=\"top\"></div>\n<script src=\"https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js\"></script>\n<script>AV.initialize(\"mW6MTVtJX0wR4pitdPFEST98-gzGzoHsz\", \"hH4sKpU65t4ptsnIyX0z34l6\");</script>\n<script type=\"text/javascript\">\n  var time=0\n  var title=\"\"\n  var url=\"\"\n  var query = new AV.Query('Counter');\n  query.notEqualTo('id',0);\n  query.descending('time');\n  query.limit(1000);\n  query.find().then(function (todo) {\n    for (var i=0;i<1000;i++){\n      var result=todo[i].attributes;\n      time=result.time;\n      title=result.title;\n      url=result.url;\n      // var content=\"<a href='\"+\"https://tw19941212.github.io\"+url+\"'>\"+title+\"</a>\"+\"<br>\"+\"<font color='#fff'>\"+\"阅读次数：\"+time+\"</font>\"+\"<br><br>\";\n      var content=\"<p>\"+\"<font color='#1C1C1C'>\"+\"【文章热度:\"+time+\"℃】\"+\"</font>\"+\"<a href='\"+\"https://tw19941212.github.io\"+url+\"'>\"+title+\"</a>\"+\"</p>\";\n      document.getElementById(\"top\").innerHTML+=content\n    }\n  }, function (error) {\n    console.log(\"error\");\n  });\n</script>","updated":"2018-12-17T11:31:24.547Z","path":"top10/index.html","layout":"page","_id":"cjtfwr9hp000149zfm4edpono","content":"<p></p><div id=\"top\"></div><p></p><script src=\"https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js\"></script><script>AV.initialize(\"mW6MTVtJX0wR4pitdPFEST98-gzGzoHsz\",\"hH4sKpU65t4ptsnIyX0z34l6\")</script><script type=\"text/javascript\">var time=0,title=\"\",url=\"\",query=new AV.Query(\"Counter\");query.notEqualTo(\"id\",0),query.descending(\"time\"),query.limit(1e3),query.find().then(function(t){for(var e=0;e<1e3;e++){var r=t[e].attributes;time=r.time,title=r.title,url=r.url;var i=\"<p><font color='#1C1C1C'>【文章热度:\"+time+\"℃】</font><a href='https://tw19941212.github.io\"+url+\"'>\"+title+\"</a></p>\";document.getElementById(\"top\").innerHTML+=i}},function(t){console.log(\"error\")})</script>","site":{"data":{}},"excerpt":"","more":"<p></p><div id=\"top\"></div><p></p><script src=\"https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js\"></script><script>AV.initialize(\"mW6MTVtJX0wR4pitdPFEST98-gzGzoHsz\",\"hH4sKpU65t4ptsnIyX0z34l6\")</script><script type=\"text/javascript\">var time=0,title=\"\",url=\"\",query=new AV.Query(\"Counter\");query.notEqualTo(\"id\",0),query.descending(\"time\"),query.limit(1e3),query.find().then(function(t){for(var e=0;e<1e3;e++){var r=t[e].attributes;time=r.time,title=r.title,url=r.url;var i=\"<p><font color='#1C1C1C'>【文章热度:\"+time+\"℃】</font><a href='https://tw19941212.github.io\"+url+\"'>\"+title+\"</a></p>\";document.getElementById(\"top\").innerHTML+=i}},function(t){console.log(\"error\")})</script>"},{"title":"categories","date":"2018-12-15T15:24:55.000Z","type":"categories","comments":0,"mathjax":false,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-12-15 23:24:55\ntype:  \"categories\"\ncomments: false\nmathjax: false\n---\n","updated":"2018-12-17T06:50:59.553Z","path":"categories/index.html","layout":"page","_id":"cjtfwr9hu000349zfu0s40pjt","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2018-12-15T15:20:16.000Z","type":"tags","comments":0,"mathjax":false,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-12-15 23:20:16\ntype:  \"tags\"\ncomments: false\nmathjax: false\n---\n","updated":"2018-12-17T06:51:09.257Z","path":"tags/index.html","layout":"page","_id":"cjtfwr9i0000749zfv3og5bsv","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Convolutional Neural Networks for Sentence Classification","mathjax":true,"comments":1,"abbrlink":"f07060b1","date":"2019-01-12T02:41:26.000Z","description":null,"_content":"本文从四篇CNN用于文本分类的论文概括的介绍了什么是TextCNN,以及如何调整TextCNN的超参数.介绍了CharCNN,与TextCNN的不同之处就在于使用的是字符向量嵌入,最后介绍了CharCNN的deep版本,这在很多文本分类任务上达到了state-of-art.\n<!-- more -->\n## 什么是TextCNN\n\n[Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)使用将词向量嵌入与CNN相结合的方式来区分文本,词向量是一个很好的特征提取器.CNN的作用是提取对预测任务有意义的子结构,利用多个不同size的kernel来提取句子中的关键信息(类似于多窗口大小的ngram),能够更好地捕捉局部相关性.MaxPool以相对输入序列的位置不变的方式选择显著特征.\n\n### 单层CNN架构\n\n文中指出,使用单层CNN即可获得用于文档分类的良好结果,使用词向量能提供用于自然语言处理的良好通用特征.若文本数据较大,对单词向量进行进一步的任务特定调整可以提供额外的性能提升.\n\n<center>\n    <img src=\"/uploads/images/TextCNN.png\" title='An example of a CNN Filter and Polling Architecture for Natural Language Processing' width='600'>\n</center>\n\n* 激活函数: relu\n* 卷积核大小: 2, 4, 5\n* fliters: 100\n* Dropout: 0.5\n* L2正则化: 3\n* Batch Size: 50\n* 优化器: Adadelta\n\n<center>\n    <img src=\"/uploads/images/TextCNN-multichannel.svg\" title='TextCNN-multichannel' width='600'>\n</center>\n\n论文结果表明除了随机初始化Embedding layer的外,使用预训练的word2vec初始化的效果都更加好.非静态(fine tune词向量)的比静态的效果好一些,multichannel表现亦不错(可参考论文数据).可在训练几个epoch后fine tune(一开始的时候卷积层都是随机初始化的,反向传播得到的Embedding层的梯度受到卷积层的影响,相当于噪声）\n\n## 深入 TextCNN 超参数\n\n[A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1510.03820) 对 TextCNN 进行文档分类所需的超参数进行了灵敏度分析,认为模型对配置很敏感.(建议阅读原文)\n\n<center>\n    <img src=\"/uploads/images/TextCnn.png\" title='Convolutional Neural Network Architecture for Sentence Classification' width='600'>\n</center>\n\n* 预训练的word2vec和GloVe嵌入的选择因问题而异,但两者都比使用 one-hot 的单词向量表现更好,而 concatenate 俩者效果更优.\n* 可以使用Grid Search不同的卷积核大小,一般若文本长度在50左右,则1-10是个不错的区间,若文本长度>100,则10-30可能更好.使用多个靠近最优卷积核大小的多卷积效果亦不错.建议先找最优卷积核大小,再尝试多卷积层.\n* 激活函数建议选择tanh,relu,或者不使用激活函数.tanh比sigmoid有更好的原点对称性,relu比sigmoid有不饱和性,这可能是俩者比sigmoid好的原因.\n* 特征映射(fliters)的数量可以选择100-600,dropout选择在0.0-0.5之间, L2正则化不加或弱L2正则化较好.未来若模型变得更加复杂导致过拟合,可以试着加大dropout.\n*  实验比较了平均池化比最大化池化效果差很多,局部最大化策略和k-max pooling比 1-max pooling 效果稍差, 这可能是因为预测文本所在位置不重要, n-gram 比联合考虑整个句子更具有预测性.\n\n## CharCNN\n\nConv用于文本分类表明ConvNets可以不需要了解语言的句法或语法结构.[Character-level Convolutional Networks for TextClassification](https://arxiv.org/abs/1509.01626)发现大数据集上的文本分类亦可不需要word级别知识.\n\n文中几个比较有意思的点:\n1. 适当的数据增强(文中使用的是替换近义词)能提高模型泛化能力\n2. 不区分大小效果更好,可能原因是不区分单词含义不变,可以看成是一种正则化\n3. CharCNN表明语言也可以看成是一种信号形式\n4. n-gram TFIDF在小数据集(50K~)左右效果很不错\n5. ChaarCNN可以学习非常规字符组合比如misspellings和emoticons(文中指出待进一步验证)\n\n## 文本分类的深度卷积网络(VDCNN)\n\n[Very Deep Convolutional Networks for Text Classification](https://arxiv.org/abs/1606.01781)是上一篇论文CharCNN的very deep版本,在多个数据集上达到state-of-art.在计算机视觉方面深度网络已经证实是成功的,因此作者尝试了深度网络用于文本分类.实验表明使用非常深的卷积神经网络模型(称为VDCNN)对分层特征学习有益.最关键的在于使用字符嵌入向量,而不是词嵌入向量.\n\n文中结论:\n1. 非常深的网络也可以很好用于小数据集\n2. 深层网络减少了分类错误\n3. 最大池化效果最好.比起其他复杂池化如k-max-pooling\n4. 当网络越深错误率增大时,shortcut连接结构是重要的\n\n## 总结\n\n* TextCNN关键是使用词向量嵌入\n* 单层TextCNN能在中等规模数据集上表现良好,并提供了调参依据\n* 深层网络用于文本可能是未来发展方向\n\n## 参考链接\n\n* [Best Practices for Document Classification with Deep Learning](https://machinelearningmastery.com/best-practices-document-classification-deep-learning/) \n* [Convolutional Neural Networks for Text Classification](http://www.davidsbatista.net/blog/2018/03/31/SentenceClassificationConvNets/) \n* [Understanding Convolutional Neural Networks for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/) ","source":"_posts/Convolutional-Neural-Networks-for-Sentence-Classification.md","raw":"---\ntitle: Convolutional Neural Networks for Sentence Classification\ncategories:\n  - 深度学习\n  - NLP\nmathjax: true\ncomments: true\ntags:\n  - paper\n  - TextCNN\n  - CharCNN\n  - 深度学习\n  - 文本分类\nabbrlink: f07060b1\ndate: 2019-01-12 10:41:26\ndescription:\n---\n本文从四篇CNN用于文本分类的论文概括的介绍了什么是TextCNN,以及如何调整TextCNN的超参数.介绍了CharCNN,与TextCNN的不同之处就在于使用的是字符向量嵌入,最后介绍了CharCNN的deep版本,这在很多文本分类任务上达到了state-of-art.\n<!-- more -->\n## 什么是TextCNN\n\n[Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)使用将词向量嵌入与CNN相结合的方式来区分文本,词向量是一个很好的特征提取器.CNN的作用是提取对预测任务有意义的子结构,利用多个不同size的kernel来提取句子中的关键信息(类似于多窗口大小的ngram),能够更好地捕捉局部相关性.MaxPool以相对输入序列的位置不变的方式选择显著特征.\n\n### 单层CNN架构\n\n文中指出,使用单层CNN即可获得用于文档分类的良好结果,使用词向量能提供用于自然语言处理的良好通用特征.若文本数据较大,对单词向量进行进一步的任务特定调整可以提供额外的性能提升.\n\n<center>\n    <img src=\"/uploads/images/TextCNN.png\" title='An example of a CNN Filter and Polling Architecture for Natural Language Processing' width='600'>\n</center>\n\n* 激活函数: relu\n* 卷积核大小: 2, 4, 5\n* fliters: 100\n* Dropout: 0.5\n* L2正则化: 3\n* Batch Size: 50\n* 优化器: Adadelta\n\n<center>\n    <img src=\"/uploads/images/TextCNN-multichannel.svg\" title='TextCNN-multichannel' width='600'>\n</center>\n\n论文结果表明除了随机初始化Embedding layer的外,使用预训练的word2vec初始化的效果都更加好.非静态(fine tune词向量)的比静态的效果好一些,multichannel表现亦不错(可参考论文数据).可在训练几个epoch后fine tune(一开始的时候卷积层都是随机初始化的,反向传播得到的Embedding层的梯度受到卷积层的影响,相当于噪声）\n\n## 深入 TextCNN 超参数\n\n[A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1510.03820) 对 TextCNN 进行文档分类所需的超参数进行了灵敏度分析,认为模型对配置很敏感.(建议阅读原文)\n\n<center>\n    <img src=\"/uploads/images/TextCnn.png\" title='Convolutional Neural Network Architecture for Sentence Classification' width='600'>\n</center>\n\n* 预训练的word2vec和GloVe嵌入的选择因问题而异,但两者都比使用 one-hot 的单词向量表现更好,而 concatenate 俩者效果更优.\n* 可以使用Grid Search不同的卷积核大小,一般若文本长度在50左右,则1-10是个不错的区间,若文本长度>100,则10-30可能更好.使用多个靠近最优卷积核大小的多卷积效果亦不错.建议先找最优卷积核大小,再尝试多卷积层.\n* 激活函数建议选择tanh,relu,或者不使用激活函数.tanh比sigmoid有更好的原点对称性,relu比sigmoid有不饱和性,这可能是俩者比sigmoid好的原因.\n* 特征映射(fliters)的数量可以选择100-600,dropout选择在0.0-0.5之间, L2正则化不加或弱L2正则化较好.未来若模型变得更加复杂导致过拟合,可以试着加大dropout.\n*  实验比较了平均池化比最大化池化效果差很多,局部最大化策略和k-max pooling比 1-max pooling 效果稍差, 这可能是因为预测文本所在位置不重要, n-gram 比联合考虑整个句子更具有预测性.\n\n## CharCNN\n\nConv用于文本分类表明ConvNets可以不需要了解语言的句法或语法结构.[Character-level Convolutional Networks for TextClassification](https://arxiv.org/abs/1509.01626)发现大数据集上的文本分类亦可不需要word级别知识.\n\n文中几个比较有意思的点:\n1. 适当的数据增强(文中使用的是替换近义词)能提高模型泛化能力\n2. 不区分大小效果更好,可能原因是不区分单词含义不变,可以看成是一种正则化\n3. CharCNN表明语言也可以看成是一种信号形式\n4. n-gram TFIDF在小数据集(50K~)左右效果很不错\n5. ChaarCNN可以学习非常规字符组合比如misspellings和emoticons(文中指出待进一步验证)\n\n## 文本分类的深度卷积网络(VDCNN)\n\n[Very Deep Convolutional Networks for Text Classification](https://arxiv.org/abs/1606.01781)是上一篇论文CharCNN的very deep版本,在多个数据集上达到state-of-art.在计算机视觉方面深度网络已经证实是成功的,因此作者尝试了深度网络用于文本分类.实验表明使用非常深的卷积神经网络模型(称为VDCNN)对分层特征学习有益.最关键的在于使用字符嵌入向量,而不是词嵌入向量.\n\n文中结论:\n1. 非常深的网络也可以很好用于小数据集\n2. 深层网络减少了分类错误\n3. 最大池化效果最好.比起其他复杂池化如k-max-pooling\n4. 当网络越深错误率增大时,shortcut连接结构是重要的\n\n## 总结\n\n* TextCNN关键是使用词向量嵌入\n* 单层TextCNN能在中等规模数据集上表现良好,并提供了调参依据\n* 深层网络用于文本可能是未来发展方向\n\n## 参考链接\n\n* [Best Practices for Document Classification with Deep Learning](https://machinelearningmastery.com/best-practices-document-classification-deep-learning/) \n* [Convolutional Neural Networks for Text Classification](http://www.davidsbatista.net/blog/2018/03/31/SentenceClassificationConvNets/) \n* [Understanding Convolutional Neural Networks for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/) ","slug":"Convolutional-Neural-Networks-for-Sentence-Classification","published":1,"updated":"2019-03-19T14:19:32.040Z","layout":"post","photos":[],"link":"","_id":"cjtfwr9hj000049zfj5ywgtis","content":"<p>本文从四篇CNN用于文本分类的论文概括的介绍了什么是TextCNN,以及如何调整TextCNN的超参数.介绍了CharCNN,与TextCNN的不同之处就在于使用的是字符向量嵌入,最后介绍了CharCNN的deep版本,这在很多文本分类任务上达到了state-of-art.<br><a id=\"more\"></a></p><h2 id=\"什么是TextCNN\"><a href=\"#什么是TextCNN\" class=\"headerlink\" title=\"什么是TextCNN\"></a>什么是TextCNN</h2><p><a href=\"https://arxiv.org/abs/1408.5882\" target=\"_blank\" rel=\"noopener\">Convolutional Neural Networks for Sentence Classification</a>使用将词向量嵌入与CNN相结合的方式来区分文本,词向量是一个很好的特征提取器.CNN的作用是提取对预测任务有意义的子结构,利用多个不同size的kernel来提取句子中的关键信息(类似于多窗口大小的ngram),能够更好地捕捉局部相关性.MaxPool以相对输入序列的位置不变的方式选择显著特征.</p><h3 id=\"单层CNN架构\"><a href=\"#单层CNN架构\" class=\"headerlink\" title=\"单层CNN架构\"></a>单层CNN架构</h3><p>文中指出,使用单层CNN即可获得用于文档分类的良好结果,使用词向量能提供用于自然语言处理的良好通用特征.若文本数据较大,对单词向量进行进一步的任务特定调整可以提供额外的性能提升.</p><center><br><img src=\"/uploads/images/TextCNN.png\" title=\"An example of a CNN Filter and Polling Architecture for Natural Language Processing\" width=\"600\"><br></center><ul><li>激活函数: relu</li><li>卷积核大小: 2, 4, 5</li><li>fliters: 100</li><li>Dropout: 0.5</li><li>L2正则化: 3</li><li>Batch Size: 50</li><li>优化器: Adadelta</li></ul><center><br><img src=\"/uploads/images/TextCNN-multichannel.svg\" title=\"TextCNN-multichannel\" width=\"600\"><br></center><p>论文结果表明除了随机初始化Embedding layer的外,使用预训练的word2vec初始化的效果都更加好.非静态(fine tune词向量)的比静态的效果好一些,multichannel表现亦不错(可参考论文数据).可在训练几个epoch后fine tune(一开始的时候卷积层都是随机初始化的,反向传播得到的Embedding层的梯度受到卷积层的影响,相当于噪声）</p><h2 id=\"深入-TextCNN-超参数\"><a href=\"#深入-TextCNN-超参数\" class=\"headerlink\" title=\"深入 TextCNN 超参数\"></a>深入 TextCNN 超参数</h2><p><a href=\"https://arxiv.org/abs/1510.03820\" target=\"_blank\" rel=\"noopener\">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification</a> 对 TextCNN 进行文档分类所需的超参数进行了灵敏度分析,认为模型对配置很敏感.(建议阅读原文)</p><center><br><img src=\"/uploads/images/TextCnn.png\" title=\"Convolutional Neural Network Architecture for Sentence Classification\" width=\"600\"><br></center><ul><li>预训练的word2vec和GloVe嵌入的选择因问题而异,但两者都比使用 one-hot 的单词向量表现更好,而 concatenate 俩者效果更优.</li><li>可以使用Grid Search不同的卷积核大小,一般若文本长度在50左右,则1-10是个不错的区间,若文本长度&gt;100,则10-30可能更好.使用多个靠近最优卷积核大小的多卷积效果亦不错.建议先找最优卷积核大小,再尝试多卷积层.</li><li>激活函数建议选择tanh,relu,或者不使用激活函数.tanh比sigmoid有更好的原点对称性,relu比sigmoid有不饱和性,这可能是俩者比sigmoid好的原因.</li><li>特征映射(fliters)的数量可以选择100-600,dropout选择在0.0-0.5之间, L2正则化不加或弱L2正则化较好.未来若模型变得更加复杂导致过拟合,可以试着加大dropout.</li><li>实验比较了平均池化比最大化池化效果差很多,局部最大化策略和k-max pooling比 1-max pooling 效果稍差, 这可能是因为预测文本所在位置不重要, n-gram 比联合考虑整个句子更具有预测性.</li></ul><h2 id=\"CharCNN\"><a href=\"#CharCNN\" class=\"headerlink\" title=\"CharCNN\"></a>CharCNN</h2><p>Conv用于文本分类表明ConvNets可以不需要了解语言的句法或语法结构.<a href=\"https://arxiv.org/abs/1509.01626\" target=\"_blank\" rel=\"noopener\">Character-level Convolutional Networks for TextClassification</a>发现大数据集上的文本分类亦可不需要word级别知识.</p><p>文中几个比较有意思的点:</p><ol><li>适当的数据增强(文中使用的是替换近义词)能提高模型泛化能力</li><li>不区分大小效果更好,可能原因是不区分单词含义不变,可以看成是一种正则化</li><li>CharCNN表明语言也可以看成是一种信号形式</li><li>n-gram TFIDF在小数据集(50K~)左右效果很不错</li><li>ChaarCNN可以学习非常规字符组合比如misspellings和emoticons(文中指出待进一步验证)</li></ol><h2 id=\"文本分类的深度卷积网络-VDCNN\"><a href=\"#文本分类的深度卷积网络-VDCNN\" class=\"headerlink\" title=\"文本分类的深度卷积网络(VDCNN)\"></a>文本分类的深度卷积网络(VDCNN)</h2><p><a href=\"https://arxiv.org/abs/1606.01781\" target=\"_blank\" rel=\"noopener\">Very Deep Convolutional Networks for Text Classification</a>是上一篇论文CharCNN的very deep版本,在多个数据集上达到state-of-art.在计算机视觉方面深度网络已经证实是成功的,因此作者尝试了深度网络用于文本分类.实验表明使用非常深的卷积神经网络模型(称为VDCNN)对分层特征学习有益.最关键的在于使用字符嵌入向量,而不是词嵌入向量.</p><p>文中结论:</p><ol><li>非常深的网络也可以很好用于小数据集</li><li>深层网络减少了分类错误</li><li>最大池化效果最好.比起其他复杂池化如k-max-pooling</li><li>当网络越深错误率增大时,shortcut连接结构是重要的</li></ol><h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul><li>TextCNN关键是使用词向量嵌入</li><li>单层TextCNN能在中等规模数据集上表现良好,并提供了调参依据</li><li>深层网络用于文本可能是未来发展方向</li></ul><h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><ul><li><a href=\"https://machinelearningmastery.com/best-practices-document-classification-deep-learning/\" target=\"_blank\" rel=\"noopener\">Best Practices for Document Classification with Deep Learning</a></li><li><a href=\"http://www.davidsbatista.net/blog/2018/03/31/SentenceClassificationConvNets/\" target=\"_blank\" rel=\"noopener\">Convolutional Neural Networks for Text Classification</a></li><li><a href=\"http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\" target=\"_blank\" rel=\"noopener\">Understanding Convolutional Neural Networks for NLP</a></li></ul>","site":{"data":{}},"excerpt":"<p>本文从四篇CNN用于文本分类的论文概括的介绍了什么是TextCNN,以及如何调整TextCNN的超参数.介绍了CharCNN,与TextCNN的不同之处就在于使用的是字符向量嵌入,最后介绍了CharCNN的deep版本,这在很多文本分类任务上达到了state-of-art.<br>","more":"</p><h2 id=\"什么是TextCNN\"><a href=\"#什么是TextCNN\" class=\"headerlink\" title=\"什么是TextCNN\"></a>什么是TextCNN</h2><p><a href=\"https://arxiv.org/abs/1408.5882\" target=\"_blank\" rel=\"noopener\">Convolutional Neural Networks for Sentence Classification</a>使用将词向量嵌入与CNN相结合的方式来区分文本,词向量是一个很好的特征提取器.CNN的作用是提取对预测任务有意义的子结构,利用多个不同size的kernel来提取句子中的关键信息(类似于多窗口大小的ngram),能够更好地捕捉局部相关性.MaxPool以相对输入序列的位置不变的方式选择显著特征.</p><h3 id=\"单层CNN架构\"><a href=\"#单层CNN架构\" class=\"headerlink\" title=\"单层CNN架构\"></a>单层CNN架构</h3><p>文中指出,使用单层CNN即可获得用于文档分类的良好结果,使用词向量能提供用于自然语言处理的良好通用特征.若文本数据较大,对单词向量进行进一步的任务特定调整可以提供额外的性能提升.</p><center><br><img src=\"/uploads/images/TextCNN.png\" title=\"An example of a CNN Filter and Polling Architecture for Natural Language Processing\" width=\"600\"><br></center><ul><li>激活函数: relu</li><li>卷积核大小: 2, 4, 5</li><li>fliters: 100</li><li>Dropout: 0.5</li><li>L2正则化: 3</li><li>Batch Size: 50</li><li>优化器: Adadelta</li></ul><center><br><img src=\"/uploads/images/TextCNN-multichannel.svg\" title=\"TextCNN-multichannel\" width=\"600\"><br></center><p>论文结果表明除了随机初始化Embedding layer的外,使用预训练的word2vec初始化的效果都更加好.非静态(fine tune词向量)的比静态的效果好一些,multichannel表现亦不错(可参考论文数据).可在训练几个epoch后fine tune(一开始的时候卷积层都是随机初始化的,反向传播得到的Embedding层的梯度受到卷积层的影响,相当于噪声）</p><h2 id=\"深入-TextCNN-超参数\"><a href=\"#深入-TextCNN-超参数\" class=\"headerlink\" title=\"深入 TextCNN 超参数\"></a>深入 TextCNN 超参数</h2><p><a href=\"https://arxiv.org/abs/1510.03820\" target=\"_blank\" rel=\"noopener\">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification</a> 对 TextCNN 进行文档分类所需的超参数进行了灵敏度分析,认为模型对配置很敏感.(建议阅读原文)</p><center><br><img src=\"/uploads/images/TextCnn.png\" title=\"Convolutional Neural Network Architecture for Sentence Classification\" width=\"600\"><br></center><ul><li>预训练的word2vec和GloVe嵌入的选择因问题而异,但两者都比使用 one-hot 的单词向量表现更好,而 concatenate 俩者效果更优.</li><li>可以使用Grid Search不同的卷积核大小,一般若文本长度在50左右,则1-10是个不错的区间,若文本长度&gt;100,则10-30可能更好.使用多个靠近最优卷积核大小的多卷积效果亦不错.建议先找最优卷积核大小,再尝试多卷积层.</li><li>激活函数建议选择tanh,relu,或者不使用激活函数.tanh比sigmoid有更好的原点对称性,relu比sigmoid有不饱和性,这可能是俩者比sigmoid好的原因.</li><li>特征映射(fliters)的数量可以选择100-600,dropout选择在0.0-0.5之间, L2正则化不加或弱L2正则化较好.未来若模型变得更加复杂导致过拟合,可以试着加大dropout.</li><li>实验比较了平均池化比最大化池化效果差很多,局部最大化策略和k-max pooling比 1-max pooling 效果稍差, 这可能是因为预测文本所在位置不重要, n-gram 比联合考虑整个句子更具有预测性.</li></ul><h2 id=\"CharCNN\"><a href=\"#CharCNN\" class=\"headerlink\" title=\"CharCNN\"></a>CharCNN</h2><p>Conv用于文本分类表明ConvNets可以不需要了解语言的句法或语法结构.<a href=\"https://arxiv.org/abs/1509.01626\" target=\"_blank\" rel=\"noopener\">Character-level Convolutional Networks for TextClassification</a>发现大数据集上的文本分类亦可不需要word级别知识.</p><p>文中几个比较有意思的点:</p><ol><li>适当的数据增强(文中使用的是替换近义词)能提高模型泛化能力</li><li>不区分大小效果更好,可能原因是不区分单词含义不变,可以看成是一种正则化</li><li>CharCNN表明语言也可以看成是一种信号形式</li><li>n-gram TFIDF在小数据集(50K~)左右效果很不错</li><li>ChaarCNN可以学习非常规字符组合比如misspellings和emoticons(文中指出待进一步验证)</li></ol><h2 id=\"文本分类的深度卷积网络-VDCNN\"><a href=\"#文本分类的深度卷积网络-VDCNN\" class=\"headerlink\" title=\"文本分类的深度卷积网络(VDCNN)\"></a>文本分类的深度卷积网络(VDCNN)</h2><p><a href=\"https://arxiv.org/abs/1606.01781\" target=\"_blank\" rel=\"noopener\">Very Deep Convolutional Networks for Text Classification</a>是上一篇论文CharCNN的very deep版本,在多个数据集上达到state-of-art.在计算机视觉方面深度网络已经证实是成功的,因此作者尝试了深度网络用于文本分类.实验表明使用非常深的卷积神经网络模型(称为VDCNN)对分层特征学习有益.最关键的在于使用字符嵌入向量,而不是词嵌入向量.</p><p>文中结论:</p><ol><li>非常深的网络也可以很好用于小数据集</li><li>深层网络减少了分类错误</li><li>最大池化效果最好.比起其他复杂池化如k-max-pooling</li><li>当网络越深错误率增大时,shortcut连接结构是重要的</li></ol><h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul><li>TextCNN关键是使用词向量嵌入</li><li>单层TextCNN能在中等规模数据集上表现良好,并提供了调参依据</li><li>深层网络用于文本可能是未来发展方向</li></ul><h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><ul><li><a href=\"https://machinelearningmastery.com/best-practices-document-classification-deep-learning/\" target=\"_blank\" rel=\"noopener\">Best Practices for Document Classification with Deep Learning</a></li><li><a href=\"http://www.davidsbatista.net/blog/2018/03/31/SentenceClassificationConvNets/\" target=\"_blank\" rel=\"noopener\">Convolutional Neural Networks for Text Classification</a></li><li><a href=\"http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\" target=\"_blank\" rel=\"noopener\">Understanding Convolutional Neural Networks for NLP</a></li></ul>"},{"title":"Hello World","mathjax":false,"abbrlink":"4a17b156","date":"2018-12-17T02:54:33.000Z","_content":"\n耗费时间俩天,终于把博客给搭建起来了.作为一个什么都不懂的机械狗,这个过程无比是痛苦的.遇到任何问题都是Google,广大网友的力量是强大的.前端的都厉害,界面挺美观.这里记录了搭建过程中所有参考的文章,让其他想搭建博客的小白不用繁琐的Google了,^_^\n\n<!-- more -->\n## 为什么搭建个人博客\n\nemmm.之前就看到别人博客高端大气上档次,总想着自己也要弄一个,又觉得太麻烦.这里给我以后文章提几点:\n\n1. 精髓短小\n2. 坚持\n3. 尽量短\n\nOver!\n\n{% note info %} \n> 人生苦短,开心最重要\n> \n> <cite>[庄子]({% post_link Hello-word %})</cite> \n{% endnote %}\n\n\n\n\n## 参考链接\n\n### Hexo\n* [NexT 主题文档](https://theme-next.iissnan.com/getting-started.html)\n* [Hexo 官方文档](https://github.com/iissnan/hexo-theme-next/blob/master/README.cn.md) \n\n### 搭建博客\n* [Hexo 博客搭建笔记](https://eibitme.github.io/2017/12/17/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E7%AC%94%E8%AE%B0/) \n* [Hexo+Github 博客搭建小白教程](https://godweiyang.com/2018/04/13/hexo-blog/) \n* [Hexo + NexT 站点配置与主题配置](https://shfanzie.github.io/201704/Site-Set-and-Theme-Set/) \n\n### 七牛云图床设置\n* [基本配置](https://yuchen-lea.github.io/2016-01-21-use-qiniu-store-file-for-hexo/) \n* [hexo-qiniu-sync 插件](https://github.com/gyk001/hexo-qiniu-sync) \n\n### algolia 搜索设置\n* [基本配置](https://juejin.im/post/5af3f9d1518825673e35a6eb) \n* [Hexo+algolia 搜索不成功解决](https://arstead.github.io/%E6%8A%80%E6%9C%AF/Hexo/How_to_add_algolia_into_hexo/) \n* [algolia搜索添加content](https://github.com/iissnan/hexo-theme-next/issues/1084) \n\n之前文章假如删除了更新index好像只能手动官网删除.改正:hexo a --flush\n\n### 其他\n* [Hexo Next主题添加版权信息](http://smartsi.club/hexo-next-add-copyright-information.html) \n* [自定义Hexo博客文章模板](http://www.mashangxue123.com/Hexo/353523292.html) \n* [编写和发布草稿](http://www.cubemister.com/Blog/2016/10/04/Hexo-%E7%BC%96%E5%86%99%E5%92%8C%E5%8F%91%E5%B8%83%E8%8D%89%E7%A8%BF/) \n* [hexo 摸爬滚打之进阶教程(博客压缩,链接唯一)](http://muyunyun.cn/posts/f55182c5/) \n* [Hexo有趣功能盘点](http://zine-fj.coding.me/2018/06/12/Hexo%E6%9C%89%E8%B6%A3%E5%8A%9F%E8%83%BD%E7%9B%98%E7%82%B9/#comments) \n* [主题优化](https://www.simon96.online/2018/10/12/hexo-tutorial/) \n* [Hexo优化](https://zhbit92.github.io/2017/12/04/hexo%E7%9B%B8%E5%85%B3/Hexo%E4%BC%98%E5%8C%96/) \n\n### 自定义功能\n* [Hexo主题底部的版权声明问题](https://kokmmm33.github.io/2017/09/27/Hexo%E4%B8%BB%E9%A2%98%E5%BA%95%E9%83%A8%E7%9A%84%E7%89%88%E6%9D%83%E5%A3%B0%E6%98%8E%E9%97%AE%E9%A2%98/) \n* [新增阅读排行页面](https://hoxis.github.io/hexo-next-read-rank.html) \n* [首页分页和归档分页不同](https://github.com/iissnan/hexo-theme-next/issues/30) \n* [首页文章,只显示标题](https://github.com/iissnan/hexo-theme-next/issues/244) \n* [next主题设置首页显示预览](https://www.jianshu.com/p/393d067dba8d) \n* [首页文章间距过宽](https://blog.csdn.net/weixin_42024255/article/details/82814433) \n* [Mist主题居中](https://anduinwrynn.github.io/2018/02/14/GitHub-Hexo-NexT%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%8F%8A%E4%BC%98%E5%8C%96/) \n* [Github Pages百度索引收录](https://3gods.com/webmaster/Github-Pages-Baidu-Index-Nginx.html#sec-1-2) ","source":"_posts/Hello-World.md","raw":"---\ntitle: Hello World\nmathjax: false\ntags:\n  - Hexo\n  - NexT\ncategories:\n  - 其他\nabbrlink: 4a17b156\ndate: 2018-12-17 10:54:33\n---\n\n耗费时间俩天,终于把博客给搭建起来了.作为一个什么都不懂的机械狗,这个过程无比是痛苦的.遇到任何问题都是Google,广大网友的力量是强大的.前端的都厉害,界面挺美观.这里记录了搭建过程中所有参考的文章,让其他想搭建博客的小白不用繁琐的Google了,^_^\n\n<!-- more -->\n## 为什么搭建个人博客\n\nemmm.之前就看到别人博客高端大气上档次,总想着自己也要弄一个,又觉得太麻烦.这里给我以后文章提几点:\n\n1. 精髓短小\n2. 坚持\n3. 尽量短\n\nOver!\n\n{% note info %} \n> 人生苦短,开心最重要\n> \n> <cite>[庄子]({% post_link Hello-word %})</cite> \n{% endnote %}\n\n\n\n\n## 参考链接\n\n### Hexo\n* [NexT 主题文档](https://theme-next.iissnan.com/getting-started.html)\n* [Hexo 官方文档](https://github.com/iissnan/hexo-theme-next/blob/master/README.cn.md) \n\n### 搭建博客\n* [Hexo 博客搭建笔记](https://eibitme.github.io/2017/12/17/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E7%AC%94%E8%AE%B0/) \n* [Hexo+Github 博客搭建小白教程](https://godweiyang.com/2018/04/13/hexo-blog/) \n* [Hexo + NexT 站点配置与主题配置](https://shfanzie.github.io/201704/Site-Set-and-Theme-Set/) \n\n### 七牛云图床设置\n* [基本配置](https://yuchen-lea.github.io/2016-01-21-use-qiniu-store-file-for-hexo/) \n* [hexo-qiniu-sync 插件](https://github.com/gyk001/hexo-qiniu-sync) \n\n### algolia 搜索设置\n* [基本配置](https://juejin.im/post/5af3f9d1518825673e35a6eb) \n* [Hexo+algolia 搜索不成功解决](https://arstead.github.io/%E6%8A%80%E6%9C%AF/Hexo/How_to_add_algolia_into_hexo/) \n* [algolia搜索添加content](https://github.com/iissnan/hexo-theme-next/issues/1084) \n\n之前文章假如删除了更新index好像只能手动官网删除.改正:hexo a --flush\n\n### 其他\n* [Hexo Next主题添加版权信息](http://smartsi.club/hexo-next-add-copyright-information.html) \n* [自定义Hexo博客文章模板](http://www.mashangxue123.com/Hexo/353523292.html) \n* [编写和发布草稿](http://www.cubemister.com/Blog/2016/10/04/Hexo-%E7%BC%96%E5%86%99%E5%92%8C%E5%8F%91%E5%B8%83%E8%8D%89%E7%A8%BF/) \n* [hexo 摸爬滚打之进阶教程(博客压缩,链接唯一)](http://muyunyun.cn/posts/f55182c5/) \n* [Hexo有趣功能盘点](http://zine-fj.coding.me/2018/06/12/Hexo%E6%9C%89%E8%B6%A3%E5%8A%9F%E8%83%BD%E7%9B%98%E7%82%B9/#comments) \n* [主题优化](https://www.simon96.online/2018/10/12/hexo-tutorial/) \n* [Hexo优化](https://zhbit92.github.io/2017/12/04/hexo%E7%9B%B8%E5%85%B3/Hexo%E4%BC%98%E5%8C%96/) \n\n### 自定义功能\n* [Hexo主题底部的版权声明问题](https://kokmmm33.github.io/2017/09/27/Hexo%E4%B8%BB%E9%A2%98%E5%BA%95%E9%83%A8%E7%9A%84%E7%89%88%E6%9D%83%E5%A3%B0%E6%98%8E%E9%97%AE%E9%A2%98/) \n* [新增阅读排行页面](https://hoxis.github.io/hexo-next-read-rank.html) \n* [首页分页和归档分页不同](https://github.com/iissnan/hexo-theme-next/issues/30) \n* [首页文章,只显示标题](https://github.com/iissnan/hexo-theme-next/issues/244) \n* [next主题设置首页显示预览](https://www.jianshu.com/p/393d067dba8d) \n* [首页文章间距过宽](https://blog.csdn.net/weixin_42024255/article/details/82814433) \n* [Mist主题居中](https://anduinwrynn.github.io/2018/02/14/GitHub-Hexo-NexT%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%8F%8A%E4%BC%98%E5%8C%96/) \n* [Github Pages百度索引收录](https://3gods.com/webmaster/Github-Pages-Baidu-Index-Nginx.html#sec-1-2) ","slug":"Hello-World","published":1,"updated":"2019-03-19T14:15:02.105Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtfwr9hs000249zf9kti1urx","content":"<p>耗费时间俩天,终于把博客给搭建起来了.作为一个什么都不懂的机械狗,这个过程无比是痛苦的.遇到任何问题都是Google,广大网友的力量是强大的.前端的都厉害,界面挺美观.这里记录了搭建过程中所有参考的文章,让其他想搭建博客的小白不用繁琐的Google了,^_^</p><a id=\"more\"></a><h2 id=\"为什么搭建个人博客\"><a href=\"#为什么搭建个人博客\" class=\"headerlink\" title=\"为什么搭建个人博客\"></a>为什么搭建个人博客</h2><p>emmm.之前就看到别人博客高端大气上档次,总想着自己也要弄一个,又觉得太麻烦.这里给我以后文章提几点:</p><ol><li>精髓短小</li><li>坚持</li><li>尽量短</li></ol><p>Over!</p><div class=\"note info\"><blockquote><p>人生苦短,开心最重要</p><p><cite><a href=\"\">庄子</a></cite></p></blockquote></div><h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><h3 id=\"Hexo\"><a href=\"#Hexo\" class=\"headerlink\" title=\"Hexo\"></a>Hexo</h3><ul><li><a href=\"https://theme-next.iissnan.com/getting-started.html\" target=\"_blank\" rel=\"noopener\">NexT 主题文档</a></li><li><a href=\"https://github.com/iissnan/hexo-theme-next/blob/master/README.cn.md\" target=\"_blank\" rel=\"noopener\">Hexo 官方文档</a></li></ul><h3 id=\"搭建博客\"><a href=\"#搭建博客\" class=\"headerlink\" title=\"搭建博客\"></a>搭建博客</h3><ul><li><a href=\"https://eibitme.github.io/2017/12/17/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E7%AC%94%E8%AE%B0/\" target=\"_blank\" rel=\"noopener\">Hexo 博客搭建笔记</a></li><li><a href=\"https://godweiyang.com/2018/04/13/hexo-blog/\" target=\"_blank\" rel=\"noopener\">Hexo+Github 博客搭建小白教程</a></li><li><a href=\"https://shfanzie.github.io/201704/Site-Set-and-Theme-Set/\" target=\"_blank\" rel=\"noopener\">Hexo + NexT 站点配置与主题配置</a></li></ul><h3 id=\"七牛云图床设置\"><a href=\"#七牛云图床设置\" class=\"headerlink\" title=\"七牛云图床设置\"></a>七牛云图床设置</h3><ul><li><a href=\"https://yuchen-lea.github.io/2016-01-21-use-qiniu-store-file-for-hexo/\" target=\"_blank\" rel=\"noopener\">基本配置</a></li><li><a href=\"https://github.com/gyk001/hexo-qiniu-sync\" target=\"_blank\" rel=\"noopener\">hexo-qiniu-sync 插件</a></li></ul><h3 id=\"algolia-搜索设置\"><a href=\"#algolia-搜索设置\" class=\"headerlink\" title=\"algolia 搜索设置\"></a>algolia 搜索设置</h3><ul><li><a href=\"https://juejin.im/post/5af3f9d1518825673e35a6eb\" target=\"_blank\" rel=\"noopener\">基本配置</a></li><li><a href=\"https://arstead.github.io/%E6%8A%80%E6%9C%AF/Hexo/How_to_add_algolia_into_hexo/\" target=\"_blank\" rel=\"noopener\">Hexo+algolia 搜索不成功解决</a></li><li><a href=\"https://github.com/iissnan/hexo-theme-next/issues/1084\" target=\"_blank\" rel=\"noopener\">algolia搜索添加content</a></li></ul><p>之前文章假如删除了更新index好像只能手动官网删除.改正:hexo a –flush</p><h3 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h3><ul><li><a href=\"http://smartsi.club/hexo-next-add-copyright-information.html\" target=\"_blank\" rel=\"noopener\">Hexo Next主题添加版权信息</a></li><li><a href=\"http://www.mashangxue123.com/Hexo/353523292.html\" target=\"_blank\" rel=\"noopener\">自定义Hexo博客文章模板</a></li><li><a href=\"http://www.cubemister.com/Blog/2016/10/04/Hexo-%E7%BC%96%E5%86%99%E5%92%8C%E5%8F%91%E5%B8%83%E8%8D%89%E7%A8%BF/\" target=\"_blank\" rel=\"noopener\">编写和发布草稿</a></li><li><a href=\"http://muyunyun.cn/posts/f55182c5/\" target=\"_blank\" rel=\"noopener\">hexo 摸爬滚打之进阶教程(博客压缩,链接唯一)</a></li><li><a href=\"http://zine-fj.coding.me/2018/06/12/Hexo%E6%9C%89%E8%B6%A3%E5%8A%9F%E8%83%BD%E7%9B%98%E7%82%B9/#comments\" target=\"_blank\" rel=\"noopener\">Hexo有趣功能盘点</a></li><li><a href=\"https://www.simon96.online/2018/10/12/hexo-tutorial/\" target=\"_blank\" rel=\"noopener\">主题优化</a></li><li><a href=\"https://zhbit92.github.io/2017/12/04/hexo%E7%9B%B8%E5%85%B3/Hexo%E4%BC%98%E5%8C%96/\" target=\"_blank\" rel=\"noopener\">Hexo优化</a></li></ul><h3 id=\"自定义功能\"><a href=\"#自定义功能\" class=\"headerlink\" title=\"自定义功能\"></a>自定义功能</h3><ul><li><a href=\"https://kokmmm33.github.io/2017/09/27/Hexo%E4%B8%BB%E9%A2%98%E5%BA%95%E9%83%A8%E7%9A%84%E7%89%88%E6%9D%83%E5%A3%B0%E6%98%8E%E9%97%AE%E9%A2%98/\" target=\"_blank\" rel=\"noopener\">Hexo主题底部的版权声明问题</a></li><li><a href=\"https://hoxis.github.io/hexo-next-read-rank.html\" target=\"_blank\" rel=\"noopener\">新增阅读排行页面</a></li><li><a href=\"https://github.com/iissnan/hexo-theme-next/issues/30\" target=\"_blank\" rel=\"noopener\">首页分页和归档分页不同</a></li><li><a href=\"https://github.com/iissnan/hexo-theme-next/issues/244\" target=\"_blank\" rel=\"noopener\">首页文章,只显示标题</a></li><li><a href=\"https://www.jianshu.com/p/393d067dba8d\" target=\"_blank\" rel=\"noopener\">next主题设置首页显示预览</a></li><li><a href=\"https://blog.csdn.net/weixin_42024255/article/details/82814433\" target=\"_blank\" rel=\"noopener\">首页文章间距过宽</a></li><li><a href=\"https://anduinwrynn.github.io/2018/02/14/GitHub-Hexo-NexT%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%8F%8A%E4%BC%98%E5%8C%96/\" target=\"_blank\" rel=\"noopener\">Mist主题居中</a></li><li><a href=\"https://3gods.com/webmaster/Github-Pages-Baidu-Index-Nginx.html#sec-1-2\" target=\"_blank\" rel=\"noopener\">Github Pages百度索引收录</a></li></ul>","site":{"data":{}},"excerpt":"<p>耗费时间俩天,终于把博客给搭建起来了.作为一个什么都不懂的机械狗,这个过程无比是痛苦的.遇到任何问题都是Google,广大网友的力量是强大的.前端的都厉害,界面挺美观.这里记录了搭建过程中所有参考的文章,让其他想搭建博客的小白不用繁琐的Google了,^_^</p>","more":"<h2 id=\"为什么搭建个人博客\"><a href=\"#为什么搭建个人博客\" class=\"headerlink\" title=\"为什么搭建个人博客\"></a>为什么搭建个人博客</h2><p>emmm.之前就看到别人博客高端大气上档次,总想着自己也要弄一个,又觉得太麻烦.这里给我以后文章提几点:</p><ol><li>精髓短小</li><li>坚持</li><li>尽量短</li></ol><p>Over!</p><div class=\"note info\"><blockquote><p>人生苦短,开心最重要</p><p><cite><a href=\"\">庄子</a></cite></p></blockquote></div><h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><h3 id=\"Hexo\"><a href=\"#Hexo\" class=\"headerlink\" title=\"Hexo\"></a>Hexo</h3><ul><li><a href=\"https://theme-next.iissnan.com/getting-started.html\" target=\"_blank\" rel=\"noopener\">NexT 主题文档</a></li><li><a href=\"https://github.com/iissnan/hexo-theme-next/blob/master/README.cn.md\" target=\"_blank\" rel=\"noopener\">Hexo 官方文档</a></li></ul><h3 id=\"搭建博客\"><a href=\"#搭建博客\" class=\"headerlink\" title=\"搭建博客\"></a>搭建博客</h3><ul><li><a href=\"https://eibitme.github.io/2017/12/17/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E7%AC%94%E8%AE%B0/\" target=\"_blank\" rel=\"noopener\">Hexo 博客搭建笔记</a></li><li><a href=\"https://godweiyang.com/2018/04/13/hexo-blog/\" target=\"_blank\" rel=\"noopener\">Hexo+Github 博客搭建小白教程</a></li><li><a href=\"https://shfanzie.github.io/201704/Site-Set-and-Theme-Set/\" target=\"_blank\" rel=\"noopener\">Hexo + NexT 站点配置与主题配置</a></li></ul><h3 id=\"七牛云图床设置\"><a href=\"#七牛云图床设置\" class=\"headerlink\" title=\"七牛云图床设置\"></a>七牛云图床设置</h3><ul><li><a href=\"https://yuchen-lea.github.io/2016-01-21-use-qiniu-store-file-for-hexo/\" target=\"_blank\" rel=\"noopener\">基本配置</a></li><li><a href=\"https://github.com/gyk001/hexo-qiniu-sync\" target=\"_blank\" rel=\"noopener\">hexo-qiniu-sync 插件</a></li></ul><h3 id=\"algolia-搜索设置\"><a href=\"#algolia-搜索设置\" class=\"headerlink\" title=\"algolia 搜索设置\"></a>algolia 搜索设置</h3><ul><li><a href=\"https://juejin.im/post/5af3f9d1518825673e35a6eb\" target=\"_blank\" rel=\"noopener\">基本配置</a></li><li><a href=\"https://arstead.github.io/%E6%8A%80%E6%9C%AF/Hexo/How_to_add_algolia_into_hexo/\" target=\"_blank\" rel=\"noopener\">Hexo+algolia 搜索不成功解决</a></li><li><a href=\"https://github.com/iissnan/hexo-theme-next/issues/1084\" target=\"_blank\" rel=\"noopener\">algolia搜索添加content</a></li></ul><p>之前文章假如删除了更新index好像只能手动官网删除.改正:hexo a –flush</p><h3 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h3><ul><li><a href=\"http://smartsi.club/hexo-next-add-copyright-information.html\" target=\"_blank\" rel=\"noopener\">Hexo Next主题添加版权信息</a></li><li><a href=\"http://www.mashangxue123.com/Hexo/353523292.html\" target=\"_blank\" rel=\"noopener\">自定义Hexo博客文章模板</a></li><li><a href=\"http://www.cubemister.com/Blog/2016/10/04/Hexo-%E7%BC%96%E5%86%99%E5%92%8C%E5%8F%91%E5%B8%83%E8%8D%89%E7%A8%BF/\" target=\"_blank\" rel=\"noopener\">编写和发布草稿</a></li><li><a href=\"http://muyunyun.cn/posts/f55182c5/\" target=\"_blank\" rel=\"noopener\">hexo 摸爬滚打之进阶教程(博客压缩,链接唯一)</a></li><li><a href=\"http://zine-fj.coding.me/2018/06/12/Hexo%E6%9C%89%E8%B6%A3%E5%8A%9F%E8%83%BD%E7%9B%98%E7%82%B9/#comments\" target=\"_blank\" rel=\"noopener\">Hexo有趣功能盘点</a></li><li><a href=\"https://www.simon96.online/2018/10/12/hexo-tutorial/\" target=\"_blank\" rel=\"noopener\">主题优化</a></li><li><a href=\"https://zhbit92.github.io/2017/12/04/hexo%E7%9B%B8%E5%85%B3/Hexo%E4%BC%98%E5%8C%96/\" target=\"_blank\" rel=\"noopener\">Hexo优化</a></li></ul><h3 id=\"自定义功能\"><a href=\"#自定义功能\" class=\"headerlink\" title=\"自定义功能\"></a>自定义功能</h3><ul><li><a href=\"https://kokmmm33.github.io/2017/09/27/Hexo%E4%B8%BB%E9%A2%98%E5%BA%95%E9%83%A8%E7%9A%84%E7%89%88%E6%9D%83%E5%A3%B0%E6%98%8E%E9%97%AE%E9%A2%98/\" target=\"_blank\" rel=\"noopener\">Hexo主题底部的版权声明问题</a></li><li><a href=\"https://hoxis.github.io/hexo-next-read-rank.html\" target=\"_blank\" rel=\"noopener\">新增阅读排行页面</a></li><li><a href=\"https://github.com/iissnan/hexo-theme-next/issues/30\" target=\"_blank\" rel=\"noopener\">首页分页和归档分页不同</a></li><li><a href=\"https://github.com/iissnan/hexo-theme-next/issues/244\" target=\"_blank\" rel=\"noopener\">首页文章,只显示标题</a></li><li><a href=\"https://www.jianshu.com/p/393d067dba8d\" target=\"_blank\" rel=\"noopener\">next主题设置首页显示预览</a></li><li><a href=\"https://blog.csdn.net/weixin_42024255/article/details/82814433\" target=\"_blank\" rel=\"noopener\">首页文章间距过宽</a></li><li><a href=\"https://anduinwrynn.github.io/2018/02/14/GitHub-Hexo-NexT%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%8F%8A%E4%BC%98%E5%8C%96/\" target=\"_blank\" rel=\"noopener\">Mist主题居中</a></li><li><a href=\"https://3gods.com/webmaster/Github-Pages-Baidu-Index-Nginx.html#sec-1-2\" target=\"_blank\" rel=\"noopener\">Github Pages百度索引收录</a></li></ul>"},{"title":"结构化机器学习项目","mathjax":false,"comments":1,"abbrlink":"563918f4","date":"2018-12-20T08:58:52.000Z","_content":"\ndeeplearning.ai的第三课:Structuring Machine Learning Projects.讲解了如何从误差角度分析改善模型,如何划分训练验证测试集,设定优化目标,偏差方差分析,数据不匹配问题,迁移学习和多任务学习,端到端学习的优缺点.\n<!-- more -->\n\n## 机器学习(ML)策略(1)\n### Introduction to ML Strategy\nChain of assumptions in ML(Orthogonalization正交化):\n1. 训练集上表现好?(可加大网络结构或用更好优化算法)\n2. 验证集上表现好?(正则化或加大训练集)\n3. 测试集上表现好?(加大验证集)\n4. 真实数据上表现好?(验证集设置不正确或损失函数不正确)\n\n<center>\n    <img src=\"/uploads/images/Chain of assumptions in ML.png\" width=\"600\" title = 'Chain of assumptions in ML'>\n</center>\n\n### Setting up your goal\n1. 单一数字评估指标 (方便比较算法优劣)\n2. 满足(Satisficing)的指标的优化(optimizing)的指标\n3. 选择验证/测试集分布反映未来真实数据或期望优化的数据\n4. 验证/测试集划分大小\n\t* 测试集大小需能反映系统性能,具有高置信度\n\t* 验证集需足够大以评估不同方法\n\n|training set|development set|test set|\n|:-|:-|:-|\n|98%|1%|1%|\n\n5. 何时改变验证/测试集大小和评价指标\n\t* 在真实应用中表现不好的时候\n\t* 定义正确的评价指标能更好评估不同分类器\n\t* 优化评估指标\n\n### Comparing to human-level performance\n1. 为什么是人类水平?(若分类器比人类表现差该怎么办)\n\t* 获得更多标注的数据\n\t* 从人工错误分析:为什么人能分类正确?\n\t* 更好的偏差/方差分析\n2. 理解人类表现\n\t* 人工分类误差是贝页斯误差近似\n3. 提高模型表现技巧\n\n<center>\n    <img src=\"/uploads/images/Improving your model performance.png\" title='Improving your model performance' width=\"600\">\n</center>\n\n## 机器学习(ML)策略(2)\n### Error Analysis\n1. 误差分析:人工分析错误来源,统计不同错误类型占总数百分比,优先解决错误率最大\n2. 深度学习算法对训练集随机误差(偶尔标记错误)具有鲁棒性,对系统误差(一直标记错误)无鲁棒性.验证集标记错误若严重影响评估算法能力,则需修正.验证集目标是帮助选择算法A & B.\n\n{% note warning %}\n\n* 对验证/测试集相同处理确保来自同一分布.\n* 考虑检查算法分类正确的样本是否标记错误而不仅是分类错误样本是否标记错误.\n* 训练集和验证/测试集或许来自稍微不同的分布.(这不会有太大影响)\n\n{% endnote %}\n\n3.  基本准则: 快速建立第一个系统,然后迭代优化.\n\n### Mismatched training and dev/test set\n\n1. 在不同划分(使dev和test来自同一分布)上训练和测试\n2. 数据不匹配问题:\n\n|Bayes optimal error|training error|training-dev error|dev error|main problem|\n|:-|:--|:--|:--|:-|\n|0%|1%|9%|10%|variance|\n|0%|1|1.5%|10%|data mismatch|\n|0%|10%|11%|12%|bias|\n|0%|10%|11%|20%|bias and data mismatch|\n\n<center>\n    <img src=\"/uploads/images/Bias variance on mismatched training and dev test sets.png\" width=\"600\" title='Bias/variance on mismatched training and dev/test sets'>\n</center>\n\n\n1. 解决数据不匹配问题:收集更多像验证集的数据,或人工合成数据,但要避免从所有可能性的空间中只选了一小部分去模拟数据,造成过拟合人工合成的数据\n\n### Learning from multiple tasks\n\n迁移学习起作用当\n\n* 任务A和任务B有相同的输入x(like image)\n* 任务A有大量数据可供学习对比任务B\n* 从A学习的低级别特征可能对B有帮助.\n\n多任务学习起作用当:\n\n* 在一系列任务上训练能从共享的低级别特征上收益时\n* 通常:对每一任务的数据量是近似相同的\n* 能训练足够大网络以至于能在所有任务上表现好\n\n### End-to-end deep learning\n优点:\n\n* 让数据自学习\n* 不依赖手工特征,组件\n\t\n缺点:\n\n* 需要大量数据\n* 排除潜在有用手工特征,组件\n\n应用端到端深度学习\n关键问题: 你有足够的数据去学习从x映射到y的复杂性吗?\n\n## 参考链接\n* [网易云课堂](https://mooc.study.163.com/course/2001280004#/info) \n* [Coursera Deep Learning 专项课程](https://www.coursera.org/specializations/deep-learning) \n* [吴恩达《深度学习》系列课程笔记](https://github.com/bighuang624/Andrew-Ng-Deep-Learning-notes/blob/master/docs/README.md) ","source":"_posts/Structuring-Machine-Learning-Projects.md","raw":"---\ntitle: 结构化机器学习项目\ncategories:\n  - 深度学习\nmathjax: false\ncomments: true\nabbrlink: 563918f4\ndate: 2018-12-20 16:58:52\ntags:\n- Coursera\n- 深度学习\n- 机器学习\n---\n\ndeeplearning.ai的第三课:Structuring Machine Learning Projects.讲解了如何从误差角度分析改善模型,如何划分训练验证测试集,设定优化目标,偏差方差分析,数据不匹配问题,迁移学习和多任务学习,端到端学习的优缺点.\n<!-- more -->\n\n## 机器学习(ML)策略(1)\n### Introduction to ML Strategy\nChain of assumptions in ML(Orthogonalization正交化):\n1. 训练集上表现好?(可加大网络结构或用更好优化算法)\n2. 验证集上表现好?(正则化或加大训练集)\n3. 测试集上表现好?(加大验证集)\n4. 真实数据上表现好?(验证集设置不正确或损失函数不正确)\n\n<center>\n    <img src=\"/uploads/images/Chain of assumptions in ML.png\" width=\"600\" title = 'Chain of assumptions in ML'>\n</center>\n\n### Setting up your goal\n1. 单一数字评估指标 (方便比较算法优劣)\n2. 满足(Satisficing)的指标的优化(optimizing)的指标\n3. 选择验证/测试集分布反映未来真实数据或期望优化的数据\n4. 验证/测试集划分大小\n\t* 测试集大小需能反映系统性能,具有高置信度\n\t* 验证集需足够大以评估不同方法\n\n|training set|development set|test set|\n|:-|:-|:-|\n|98%|1%|1%|\n\n5. 何时改变验证/测试集大小和评价指标\n\t* 在真实应用中表现不好的时候\n\t* 定义正确的评价指标能更好评估不同分类器\n\t* 优化评估指标\n\n### Comparing to human-level performance\n1. 为什么是人类水平?(若分类器比人类表现差该怎么办)\n\t* 获得更多标注的数据\n\t* 从人工错误分析:为什么人能分类正确?\n\t* 更好的偏差/方差分析\n2. 理解人类表现\n\t* 人工分类误差是贝页斯误差近似\n3. 提高模型表现技巧\n\n<center>\n    <img src=\"/uploads/images/Improving your model performance.png\" title='Improving your model performance' width=\"600\">\n</center>\n\n## 机器学习(ML)策略(2)\n### Error Analysis\n1. 误差分析:人工分析错误来源,统计不同错误类型占总数百分比,优先解决错误率最大\n2. 深度学习算法对训练集随机误差(偶尔标记错误)具有鲁棒性,对系统误差(一直标记错误)无鲁棒性.验证集标记错误若严重影响评估算法能力,则需修正.验证集目标是帮助选择算法A & B.\n\n{% note warning %}\n\n* 对验证/测试集相同处理确保来自同一分布.\n* 考虑检查算法分类正确的样本是否标记错误而不仅是分类错误样本是否标记错误.\n* 训练集和验证/测试集或许来自稍微不同的分布.(这不会有太大影响)\n\n{% endnote %}\n\n3.  基本准则: 快速建立第一个系统,然后迭代优化.\n\n### Mismatched training and dev/test set\n\n1. 在不同划分(使dev和test来自同一分布)上训练和测试\n2. 数据不匹配问题:\n\n|Bayes optimal error|training error|training-dev error|dev error|main problem|\n|:-|:--|:--|:--|:-|\n|0%|1%|9%|10%|variance|\n|0%|1|1.5%|10%|data mismatch|\n|0%|10%|11%|12%|bias|\n|0%|10%|11%|20%|bias and data mismatch|\n\n<center>\n    <img src=\"/uploads/images/Bias variance on mismatched training and dev test sets.png\" width=\"600\" title='Bias/variance on mismatched training and dev/test sets'>\n</center>\n\n\n1. 解决数据不匹配问题:收集更多像验证集的数据,或人工合成数据,但要避免从所有可能性的空间中只选了一小部分去模拟数据,造成过拟合人工合成的数据\n\n### Learning from multiple tasks\n\n迁移学习起作用当\n\n* 任务A和任务B有相同的输入x(like image)\n* 任务A有大量数据可供学习对比任务B\n* 从A学习的低级别特征可能对B有帮助.\n\n多任务学习起作用当:\n\n* 在一系列任务上训练能从共享的低级别特征上收益时\n* 通常:对每一任务的数据量是近似相同的\n* 能训练足够大网络以至于能在所有任务上表现好\n\n### End-to-end deep learning\n优点:\n\n* 让数据自学习\n* 不依赖手工特征,组件\n\t\n缺点:\n\n* 需要大量数据\n* 排除潜在有用手工特征,组件\n\n应用端到端深度学习\n关键问题: 你有足够的数据去学习从x映射到y的复杂性吗?\n\n## 参考链接\n* [网易云课堂](https://mooc.study.163.com/course/2001280004#/info) \n* [Coursera Deep Learning 专项课程](https://www.coursera.org/specializations/deep-learning) \n* [吴恩达《深度学习》系列课程笔记](https://github.com/bighuang624/Andrew-Ng-Deep-Learning-notes/blob/master/docs/README.md) ","slug":"Structuring-Machine-Learning-Projects","published":1,"updated":"2019-03-19T14:04:48.685Z","layout":"post","photos":[],"link":"","_id":"cjtfwr9hz000649zfu14h29kp","content":"<p>deeplearning.ai的第三课:Structuring Machine Learning Projects.讲解了如何从误差角度分析改善模型,如何划分训练验证测试集,设定优化目标,偏差方差分析,数据不匹配问题,迁移学习和多任务学习,端到端学习的优缺点.<br><a id=\"more\"></a></p><h2 id=\"机器学习-ML-策略-1\"><a href=\"#机器学习-ML-策略-1\" class=\"headerlink\" title=\"机器学习(ML)策略(1)\"></a>机器学习(ML)策略(1)</h2><h3 id=\"Introduction-to-ML-Strategy\"><a href=\"#Introduction-to-ML-Strategy\" class=\"headerlink\" title=\"Introduction to ML Strategy\"></a>Introduction to ML Strategy</h3><p>Chain of assumptions in ML(Orthogonalization正交化):</p><ol><li>训练集上表现好?(可加大网络结构或用更好优化算法)</li><li>验证集上表现好?(正则化或加大训练集)</li><li>测试集上表现好?(加大验证集)</li><li>真实数据上表现好?(验证集设置不正确或损失函数不正确)</li></ol><center><br><img src=\"/uploads/images/Chain of assumptions in ML.png\" width=\"600\" title=\"Chain of assumptions in ML\"><br></center><h3 id=\"Setting-up-your-goal\"><a href=\"#Setting-up-your-goal\" class=\"headerlink\" title=\"Setting up your goal\"></a>Setting up your goal</h3><ol><li>单一数字评估指标 (方便比较算法优劣)</li><li>满足(Satisficing)的指标的优化(optimizing)的指标</li><li>选择验证/测试集分布反映未来真实数据或期望优化的数据</li><li>验证/测试集划分大小<ul><li>测试集大小需能反映系统性能,具有高置信度</li><li>验证集需足够大以评估不同方法</li></ul></li></ol><table><thead><tr><th style=\"text-align:left\">training set</th><th style=\"text-align:left\">development set</th><th style=\"text-align:left\">test set</th></tr></thead><tbody><tr><td style=\"text-align:left\">98%</td><td style=\"text-align:left\">1%</td><td style=\"text-align:left\">1%</td></tr></tbody></table><ol start=\"5\"><li>何时改变验证/测试集大小和评价指标<ul><li>在真实应用中表现不好的时候</li><li>定义正确的评价指标能更好评估不同分类器</li><li>优化评估指标</li></ul></li></ol><h3 id=\"Comparing-to-human-level-performance\"><a href=\"#Comparing-to-human-level-performance\" class=\"headerlink\" title=\"Comparing to human-level performance\"></a>Comparing to human-level performance</h3><ol><li>为什么是人类水平?(若分类器比人类表现差该怎么办)<ul><li>获得更多标注的数据</li><li>从人工错误分析:为什么人能分类正确?</li><li>更好的偏差/方差分析</li></ul></li><li>理解人类表现<ul><li>人工分类误差是贝页斯误差近似</li></ul></li><li>提高模型表现技巧</li></ol><center><br><img src=\"/uploads/images/Improving your model performance.png\" title=\"Improving your model performance\" width=\"600\"><br></center><h2 id=\"机器学习-ML-策略-2\"><a href=\"#机器学习-ML-策略-2\" class=\"headerlink\" title=\"机器学习(ML)策略(2)\"></a>机器学习(ML)策略(2)</h2><h3 id=\"Error-Analysis\"><a href=\"#Error-Analysis\" class=\"headerlink\" title=\"Error Analysis\"></a>Error Analysis</h3><ol><li>误差分析:人工分析错误来源,统计不同错误类型占总数百分比,优先解决错误率最大</li><li>深度学习算法对训练集随机误差(偶尔标记错误)具有鲁棒性,对系统误差(一直标记错误)无鲁棒性.验证集标记错误若严重影响评估算法能力,则需修正.验证集目标是帮助选择算法A &amp; B.</li></ol><div class=\"note warning\"><ul><li>对验证/测试集相同处理确保来自同一分布.</li><li>考虑检查算法分类正确的样本是否标记错误而不仅是分类错误样本是否标记错误.</li><li>训练集和验证/测试集或许来自稍微不同的分布.(这不会有太大影响)</li></ul></div><ol start=\"3\"><li>基本准则: 快速建立第一个系统,然后迭代优化.</li></ol><h3 id=\"Mismatched-training-and-dev-test-set\"><a href=\"#Mismatched-training-and-dev-test-set\" class=\"headerlink\" title=\"Mismatched training and dev/test set\"></a>Mismatched training and dev/test set</h3><ol><li>在不同划分(使dev和test来自同一分布)上训练和测试</li><li>数据不匹配问题:</li></ol><table><thead><tr><th style=\"text-align:left\">Bayes optimal error</th><th style=\"text-align:left\">training error</th><th style=\"text-align:left\">training-dev error</th><th style=\"text-align:left\">dev error</th><th style=\"text-align:left\">main problem</th></tr></thead><tbody><tr><td style=\"text-align:left\">0%</td><td style=\"text-align:left\">1%</td><td style=\"text-align:left\">9%</td><td style=\"text-align:left\">10%</td><td style=\"text-align:left\">variance</td></tr><tr><td style=\"text-align:left\">0%</td><td style=\"text-align:left\">1</td><td style=\"text-align:left\">1.5%</td><td style=\"text-align:left\">10%</td><td style=\"text-align:left\">data mismatch</td></tr><tr><td style=\"text-align:left\">0%</td><td style=\"text-align:left\">10%</td><td style=\"text-align:left\">11%</td><td style=\"text-align:left\">12%</td><td style=\"text-align:left\">bias</td></tr><tr><td style=\"text-align:left\">0%</td><td style=\"text-align:left\">10%</td><td style=\"text-align:left\">11%</td><td style=\"text-align:left\">20%</td><td style=\"text-align:left\">bias and data mismatch</td></tr></tbody></table><center><br><img src=\"/uploads/images/Bias variance on mismatched training and dev test sets.png\" width=\"600\" title=\"Bias/variance on mismatched training and dev/test sets\"><br></center><ol><li>解决数据不匹配问题:收集更多像验证集的数据,或人工合成数据,但要避免从所有可能性的空间中只选了一小部分去模拟数据,造成过拟合人工合成的数据</li></ol><h3 id=\"Learning-from-multiple-tasks\"><a href=\"#Learning-from-multiple-tasks\" class=\"headerlink\" title=\"Learning from multiple tasks\"></a>Learning from multiple tasks</h3><p>迁移学习起作用当</p><ul><li>任务A和任务B有相同的输入x(like image)</li><li>任务A有大量数据可供学习对比任务B</li><li>从A学习的低级别特征可能对B有帮助.</li></ul><p>多任务学习起作用当:</p><ul><li>在一系列任务上训练能从共享的低级别特征上收益时</li><li>通常:对每一任务的数据量是近似相同的</li><li>能训练足够大网络以至于能在所有任务上表现好</li></ul><h3 id=\"End-to-end-deep-learning\"><a href=\"#End-to-end-deep-learning\" class=\"headerlink\" title=\"End-to-end deep learning\"></a>End-to-end deep learning</h3><p>优点:</p><ul><li>让数据自学习</li><li>不依赖手工特征,组件</li></ul><p>缺点:</p><ul><li>需要大量数据</li><li>排除潜在有用手工特征,组件</li></ul><p>应用端到端深度学习<br>关键问题: 你有足够的数据去学习从x映射到y的复杂性吗?</p><h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><ul><li><a href=\"https://mooc.study.163.com/course/2001280004#/info\" target=\"_blank\" rel=\"noopener\">网易云课堂</a></li><li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Coursera Deep Learning 专项课程</a></li><li><a href=\"https://github.com/bighuang624/Andrew-Ng-Deep-Learning-notes/blob/master/docs/README.md\" target=\"_blank\" rel=\"noopener\">吴恩达《深度学习》系列课程笔记</a></li></ul>","site":{"data":{}},"excerpt":"<p>deeplearning.ai的第三课:Structuring Machine Learning Projects.讲解了如何从误差角度分析改善模型,如何划分训练验证测试集,设定优化目标,偏差方差分析,数据不匹配问题,迁移学习和多任务学习,端到端学习的优缺点.<br>","more":"</p><h2 id=\"机器学习-ML-策略-1\"><a href=\"#机器学习-ML-策略-1\" class=\"headerlink\" title=\"机器学习(ML)策略(1)\"></a>机器学习(ML)策略(1)</h2><h3 id=\"Introduction-to-ML-Strategy\"><a href=\"#Introduction-to-ML-Strategy\" class=\"headerlink\" title=\"Introduction to ML Strategy\"></a>Introduction to ML Strategy</h3><p>Chain of assumptions in ML(Orthogonalization正交化):</p><ol><li>训练集上表现好?(可加大网络结构或用更好优化算法)</li><li>验证集上表现好?(正则化或加大训练集)</li><li>测试集上表现好?(加大验证集)</li><li>真实数据上表现好?(验证集设置不正确或损失函数不正确)</li></ol><center><br><img src=\"/uploads/images/Chain of assumptions in ML.png\" width=\"600\" title=\"Chain of assumptions in ML\"><br></center><h3 id=\"Setting-up-your-goal\"><a href=\"#Setting-up-your-goal\" class=\"headerlink\" title=\"Setting up your goal\"></a>Setting up your goal</h3><ol><li>单一数字评估指标 (方便比较算法优劣)</li><li>满足(Satisficing)的指标的优化(optimizing)的指标</li><li>选择验证/测试集分布反映未来真实数据或期望优化的数据</li><li>验证/测试集划分大小<ul><li>测试集大小需能反映系统性能,具有高置信度</li><li>验证集需足够大以评估不同方法</li></ul></li></ol><table><thead><tr><th style=\"text-align:left\">training set</th><th style=\"text-align:left\">development set</th><th style=\"text-align:left\">test set</th></tr></thead><tbody><tr><td style=\"text-align:left\">98%</td><td style=\"text-align:left\">1%</td><td style=\"text-align:left\">1%</td></tr></tbody></table><ol start=\"5\"><li>何时改变验证/测试集大小和评价指标<ul><li>在真实应用中表现不好的时候</li><li>定义正确的评价指标能更好评估不同分类器</li><li>优化评估指标</li></ul></li></ol><h3 id=\"Comparing-to-human-level-performance\"><a href=\"#Comparing-to-human-level-performance\" class=\"headerlink\" title=\"Comparing to human-level performance\"></a>Comparing to human-level performance</h3><ol><li>为什么是人类水平?(若分类器比人类表现差该怎么办)<ul><li>获得更多标注的数据</li><li>从人工错误分析:为什么人能分类正确?</li><li>更好的偏差/方差分析</li></ul></li><li>理解人类表现<ul><li>人工分类误差是贝页斯误差近似</li></ul></li><li>提高模型表现技巧</li></ol><center><br><img src=\"/uploads/images/Improving your model performance.png\" title=\"Improving your model performance\" width=\"600\"><br></center><h2 id=\"机器学习-ML-策略-2\"><a href=\"#机器学习-ML-策略-2\" class=\"headerlink\" title=\"机器学习(ML)策略(2)\"></a>机器学习(ML)策略(2)</h2><h3 id=\"Error-Analysis\"><a href=\"#Error-Analysis\" class=\"headerlink\" title=\"Error Analysis\"></a>Error Analysis</h3><ol><li>误差分析:人工分析错误来源,统计不同错误类型占总数百分比,优先解决错误率最大</li><li>深度学习算法对训练集随机误差(偶尔标记错误)具有鲁棒性,对系统误差(一直标记错误)无鲁棒性.验证集标记错误若严重影响评估算法能力,则需修正.验证集目标是帮助选择算法A &amp; B.</li></ol><div class=\"note warning\"><ul><li>对验证/测试集相同处理确保来自同一分布.</li><li>考虑检查算法分类正确的样本是否标记错误而不仅是分类错误样本是否标记错误.</li><li>训练集和验证/测试集或许来自稍微不同的分布.(这不会有太大影响)</li></ul></div><ol start=\"3\"><li>基本准则: 快速建立第一个系统,然后迭代优化.</li></ol><h3 id=\"Mismatched-training-and-dev-test-set\"><a href=\"#Mismatched-training-and-dev-test-set\" class=\"headerlink\" title=\"Mismatched training and dev/test set\"></a>Mismatched training and dev/test set</h3><ol><li>在不同划分(使dev和test来自同一分布)上训练和测试</li><li>数据不匹配问题:</li></ol><table><thead><tr><th style=\"text-align:left\">Bayes optimal error</th><th style=\"text-align:left\">training error</th><th style=\"text-align:left\">training-dev error</th><th style=\"text-align:left\">dev error</th><th style=\"text-align:left\">main problem</th></tr></thead><tbody><tr><td style=\"text-align:left\">0%</td><td style=\"text-align:left\">1%</td><td style=\"text-align:left\">9%</td><td style=\"text-align:left\">10%</td><td style=\"text-align:left\">variance</td></tr><tr><td style=\"text-align:left\">0%</td><td style=\"text-align:left\">1</td><td style=\"text-align:left\">1.5%</td><td style=\"text-align:left\">10%</td><td style=\"text-align:left\">data mismatch</td></tr><tr><td style=\"text-align:left\">0%</td><td style=\"text-align:left\">10%</td><td style=\"text-align:left\">11%</td><td style=\"text-align:left\">12%</td><td style=\"text-align:left\">bias</td></tr><tr><td style=\"text-align:left\">0%</td><td style=\"text-align:left\">10%</td><td style=\"text-align:left\">11%</td><td style=\"text-align:left\">20%</td><td style=\"text-align:left\">bias and data mismatch</td></tr></tbody></table><center><br><img src=\"/uploads/images/Bias variance on mismatched training and dev test sets.png\" width=\"600\" title=\"Bias/variance on mismatched training and dev/test sets\"><br></center><ol><li>解决数据不匹配问题:收集更多像验证集的数据,或人工合成数据,但要避免从所有可能性的空间中只选了一小部分去模拟数据,造成过拟合人工合成的数据</li></ol><h3 id=\"Learning-from-multiple-tasks\"><a href=\"#Learning-from-multiple-tasks\" class=\"headerlink\" title=\"Learning from multiple tasks\"></a>Learning from multiple tasks</h3><p>迁移学习起作用当</p><ul><li>任务A和任务B有相同的输入x(like image)</li><li>任务A有大量数据可供学习对比任务B</li><li>从A学习的低级别特征可能对B有帮助.</li></ul><p>多任务学习起作用当:</p><ul><li>在一系列任务上训练能从共享的低级别特征上收益时</li><li>通常:对每一任务的数据量是近似相同的</li><li>能训练足够大网络以至于能在所有任务上表现好</li></ul><h3 id=\"End-to-end-deep-learning\"><a href=\"#End-to-end-deep-learning\" class=\"headerlink\" title=\"End-to-end deep learning\"></a>End-to-end deep learning</h3><p>优点:</p><ul><li>让数据自学习</li><li>不依赖手工特征,组件</li></ul><p>缺点:</p><ul><li>需要大量数据</li><li>排除潜在有用手工特征,组件</li></ul><p>应用端到端深度学习<br>关键问题: 你有足够的数据去学习从x映射到y的复杂性吗?</p><h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><ul><li><a href=\"https://mooc.study.163.com/course/2001280004#/info\" target=\"_blank\" rel=\"noopener\">网易云课堂</a></li><li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Coursera Deep Learning 专项课程</a></li><li><a href=\"https://github.com/bighuang624/Andrew-Ng-Deep-Learning-notes/blob/master/docs/README.md\" target=\"_blank\" rel=\"noopener\">吴恩达《深度学习》系列课程笔记</a></li></ul>"},{"title":"Dynamic Routing Between Capsules","mathjax":true,"comments":1,"abbrlink":"9a4ff7b0","date":"2019-01-02T14:20:00.000Z","description":null,"_content":"Capsule是一组**vector in vector out**的神经元,每一个胶囊代表某一特定实体,如对象或对象部分.其向量模长代表实体存在概率,向量参数代表实体实例化参数.低级别Capsule向量通过变化矩阵实现对高级别Capsule实体参数进行预测.当多个预测一致时,该高级别Capsule会更加活跃,借由动态路由算法加强预测.\n<!-- more -->\n> 本文综合多篇中文和外文博客(原文链接均在参考链接中给出),结合自己理解转述而来,[original paper](https://arxiv.org/pdf/1710.09829.pdf) by Hinton\n\n## CNN 的局限性\n1. 需要**大量训练数据**(测试时只能识别已经训练的特征).Capsule能使用更少数据实现更好泛化\n2. 不能处理**复杂场景**(如特征重叠),而Capsule能很好处理复杂场景(crowded scenes,如数字重叠),因为Capsule能很好实现特征分配(通过动态路由)\n3. Pooling层丢失大量信息,降低了空间分辨率(spatial resolution),所以输出对输入的微小变化不敏感,这在需要细节信息的任务(语义分割)上是糟糕的.当然通过构建复杂CNN网络能恢复信息损失.而Capsule能精确捕捉细节信息(pose information,如角度,厚度,大小,位置等)而不是丢失后再恢复信息.因此微小输入变化导致输出变化,这被称为**等效性**(equivariance).因此Capsule能在不同的视觉任务使用相同且简单的结构.\n4. CNN需要额外的组件来自动识别不同物体属于哪个对象(如这个胳膊属于这只羊),而Capsule提供了对象的**层次结构**.\n5. CNN**忽略结构信息**,仅仅考虑'有没有',而没有考虑feature map的结构关系,包含位置,角度等.\n\n## 人类视觉识别\n任何物体都是由多个更小的实体组成.例如,树由树干,树冠和树根组成.这些部分形成层次结构.树冠还包括树枝和树叶.\n\n<center>\n    <img src=\"/uploads/images/Capsule-tree_parts_diagram.png\"  title='Capsule tree parts diagram' width=\"600\">\n</center>\n\n当我们看物体时,我们的眼睛就会形成一些**固定点(fixation points)**,这些固定点的相对位置和性质有助于我们的大脑识别这个物体.因此,我们的大脑不必处理每个细节.只要看到一些树叶和树枝,我们的大脑就会认出树冠.并且树冠在树干上.结合这种层次信息,我们的大脑知道有一棵树.从现在开始,我们将对象的各个部分称为实体.(the parts of the objects as entities)\n\n<center>\n    <img src=\"/uploads/images/Capsule-tree.png\"  title='Capsule-tree' width=\"600\">\n</center>\n\n## CapsNets 到底是什么?\n简短的说,CapsNet由capsule(胶囊)而非神经元组成.胶囊是一小组神经元,其学习检测图像的给定区域内的特定对象(如矩形),并且输出向量(如8维向量).其**长度**表示对象存在的概率,其**方向**编码对象的姿势参数(如位置,旋转等).如果稍微改变输入对象(如移位,旋转,调整大小等),胶囊将输出相同长度的矢量,但方向略微不同.因此.胶囊具有等效性.\n与常规神经网络相似,CapsNet也有多个层.最下层的胶囊称为主胶囊(primary capsules),它们中的每一个都接收图像的一小部分作为输入(receptive field),并且它试图检测特定图案的存在和姿势.更高层的胶囊(routing capsules)可以检测更大和更复杂的物体(如船只).\n\n<center>\n    <img src=\"/uploads/images/two-layer CapsNet.png\"  title='the primary capsule layer has two maps of 5x5 capsules, while the second capsule layer has two maps of 3x3 capsules. Each capsule outputs a vector' width=\"600\">\n</center>\n\n使用常规卷积层即可实现主要胶囊层.例如,论文中,使用两个卷积层,输出256个包含标量的6x6特征映射.重塑此输出以获得包含8维向量的32个6x6特征映射.最后,使用新颖的squash函数来确保这些向量的长度在0和1之间(表示概率).这给出了主胶囊的输出.\n下一层中的胶囊也会尝试检测物体及其姿势,但工作方式却截然不同,使用称为路由协议的算法.\n\n## 路由协议(Routing by agreement)\n假设只有两个主要胶囊:一个矩形胶囊和一个三角形胶囊,并假设它们都检测实体.注意到矩形和三角形在船的姿势上是一致的,而他们对房子的姿势非常不同意.因此,矩形和三角形很可能是同一艘船的一部分.(值得注意的是实体的形状及整体/部分之间的关系是在训练中学习的)\n\n<center>\n    <img src=\"/uploads/images/Routing by agreement, step 1.png\"  title='predict the presence and pose of objects based on the presence and pose of object parts, then look for agreement between the predictions. Image by Aurélien Géron.' width=\"600\">\n</center>\n\n由于现在确信矩形和三角形是船的一部分,因此将矩形和三角形胶囊的输出更多地发送到船舱中是有意义的,而对于房屋舱更少:这样,船舱将接收更有用的输入信号,房屋胶囊将收到更少的噪音.即在达成一致时增加路由权重,并在出现不一致时减少路由权重.\n\n<center>\n    <img src=\"/uploads/images/Routing by agreement, step 2.png\"  title='update the routing weights. Image by Aurélien Géron.' width=\"600\">\n</center>\n\n路由协议算法涉及**一致性检测+路由更新**的几次迭代(注意,这发生在每次预测中,而不仅仅是训练时,不仅仅是一次).这在拥挤的场景中尤其有用.在混淆场景中很可能会收敛到一个更好的解释:底部的船,顶部的房子.模糊性被'解释了':下方的矩形最好用船的存在来解释,这也解释了下三角形,一旦解释了这两个部分,其余部分很容易被解释为房屋.\n\n<center>\n    <img src=\"/uploads/images/Routing by agreement can parse crowded scenes.png\"  title='Routing by agreement can parse crowded scenes, such as this ambiguous image, which could be misinterpreted as an upside-down house plus some unexplained parts. Instead, the lower rectangle will be routed to the boat, and this will also pull the lower triangle into the boat as well. Once that boat is “explained away,” it’s easy to interpret the top part as a house. Image by Aurélien Géron.' width=\"600\">\n</center>\n\n## CapsNet背后的数学\n假设层$l$和$l+1$分别具有$m$和$n$个胶囊.我们的任务是在给定层$l$的激活向量下计算层$l+1$处胶囊的激活向量.设$u$表示第$l$层胶囊的激活向量.我们必须计算$v$,胶囊在$l+1$层的激活向量。\n对于层$l+1$处的胶囊$j$:\n1. 我们首先通过层$l$处的胶囊计算**预测向量**.胶囊$i$($l$层)对胶囊$j$($l+1$层)的预测向量由下式给出\n\n$$\\boldsymbol{\\hat{\\textbf{u}}}\\_{j|i} = \\boldsymbol{\\textbf{W}}\\_{ij}\\boldsymbol{\\textbf{u}}_{i}$$\n\n$W_ {ij}$是权重矩阵\n2. 计算胶囊$j$的**输出矢量**.胶囊$j$输出向量是胶囊层$l$capsules胶囊给出的所有预测向量的加权和\n\n$$s_j = \\sum_{i=1}^{m}{c_{ij}\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}}$$\n\n标量$c_ {ij}$称为胶囊$i$($l$层)对胶囊$j$($l+1$层)之间的**耦合系数**.系数由**迭代动态路由**算法确定\n3. 在输出向量上应用**squashing**函数来获得激活向量\n\n<center>\n    <img src=\"/uploads/images/Capsule-squash.png\"  title='squash' width=\"600\">\n</center>\n\n## 动态路由算法\n层$l+1$的激活向量将反馈信号发送到层$l$处的胶囊.如果胶囊$j$($l+1$层)的激活向量与胶囊$i$($l$层)的预测矢量一致,则它们的点积应该比较大.因此,预测向量的'权重'在$j$的输出向量中增加.换句话说,那些贡献越大的预测向量在输出向量(激活向量)中具有更多的权重.循环持续4-5轮.(这像一种聚类算法,可以参考链接4)\n低级别胶囊对高级别胶囊的的预测权重总和应该为1\n$$c_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{k}{\\exp(b_{ik})}}$$ \n显然\n$$\\sum_{k}{c_{ik}} = 1$$\nlogit$b_{ij}$表示胶囊$i$($l$层)和胶囊$j$($l+1$层)是否具有强耦合.换句话说,它是由胶囊$i$解释胶囊$j$的存在性大小的量度.所有$b_{ij}$初始化应该是相等的.\n\n**Routing algorithm:**\n\n{% note info %} \n\nGiven: 预测向量$\\boldsymbol{\\hat{\\textbf{u}}}\\_{j|i}$, 迭代次数$r$\n对胶囊$i$(层$l$)和胶囊$j$(层$l+1$):$b_{ij} = 0$\nfor $r$ iterations do:\n&emsp; 对所有胶囊$i$(层$l$): $c\\_{i }= softmax(b_i)$ **(对高级别胶囊预测权重总和为1)**\n&emsp; 对所有胶囊$j$(层$l+1$): $s\\_{j} = \\sum\\_{i=1}^{m}{c\\_{ij}\\boldsymbol{\\hat{\\textbf{u}}}\\_{j|i}}$ **(输出向量是预测向量的加权和)**\n&emsp; 对所有胶囊$j$(层$l+1$): $\\textbf{v}\\_{j} = \\textbf{squash}(\\textbf{s}\\_{j})$ **(应用激活函数)**\n&emsp; 对所有胶囊$i$(层$l$)和胶囊$j$(层$l+1$): $b_{ij}=b_{ij}+\\boldsymbol{\\hat{\\textbf{u}}}\\_{j|i} \\cdot \\textbf{v}_{j}$ \n返回 $\\textbf{v}_j$\n{% endnote %}\n\n循环中的最后一行非常重要.这是路由发生的地方.如果乘积$\\boldsymbol{\\hat{\\textbf{u}}}\\_{j|i} \\cdot \\textbf{v}\\_{j}$很大,它将增加$b\\_{ij}$,这将增加相应的耦合系数$c\\_{ij}$,这反过来将使乘积$\\boldsymbol{\\hat{\\textbf{u}}}\\_{j|i} \\cdot \\textbf{v}_{j}$更大.(在链接4里面博主有自己的想法)\n\n## 路由算法优势\n在CNN中,存在池化层.通常使用MaxPool,这是一种非常原始的路由机制.局部池化中最活跃的特征(比如4x4网格)被路由到更高层,而更高级别的检测器在路由中没有发言权.将其与CapsNet中引入的协议路由机制进行比较,只有那些与高级探测器一致的功能才会被路由.这是CapsNet优于CNN的优势.它具有卓越的动态路由机制(动态,因为要路由的信息是实时确定的).\n\n## 优点和缺点\n\n**Prons:**\n1. 更少训练数据\n2. 等效性保留了输入对象的位置信息\n3. 动态路由算法对重叠对象(特征)很有用\n4. 自动计算了输入物体的层次结构\n5. 激活向量可解释性更强\n\n**Cons:**\n1. 训练慢(因为动态路由的内循环)\n2. 没有在大数据集(如ImageNet)上测试\n3. 在复杂数据集CIFAR10上效果不好\n4. 不能区分彼此靠近的俩个相同类型的相同物体(Problem of crowding)\n\n## 参考链接\n\n* [Introducing capsule networks](https://www.oreilly.com/ideas/introducing-capsule-networks) \n* [Beginner's Guide to Capsule Networks](https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-capsule-networks) \n* [Understanding Capsule Network Architecture](https://software.intel.com/en-us/articles/understanding-capsule-network-architecture) \n* [揭开迷雾，来一顿美味的Capsule盛宴](https://kexue.fm/archives/4819)\n* [Understanding Hinton’s Capsule Networks. Part I: Intuition.](https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b) \n* [GitHub:awesome-capsule-networks](https://github.com/sekwiatkowski/awesome-capsule-networks#dynamic-routing-implementations) \n* [一个注释非常详细的tensorflow源码](https://github.com/ageron/handson-ml/blob/master/extra_capsnets-cn.ipynb) \n* [CSDN:Dynamic Routing Between Capsules（NIPS2017）](https://blog.csdn.net/xyj1536214199/article/details/78698326) ","source":"_posts/Dynamic-Routing-Between-Capsules.md","raw":"---\ntitle: Dynamic Routing Between Capsules\ncategories:\n  - 深度学习\n  - CV\nmathjax: true\ncomments: true\ntags:\n  - paper\n  - CNN\n  - 深度学习\n  - Capsule\n  - 图像处理\nabbrlink: 9a4ff7b0\ndate: 2019-01-02 22:20:00\ndescription:\n---\nCapsule是一组**vector in vector out**的神经元,每一个胶囊代表某一特定实体,如对象或对象部分.其向量模长代表实体存在概率,向量参数代表实体实例化参数.低级别Capsule向量通过变化矩阵实现对高级别Capsule实体参数进行预测.当多个预测一致时,该高级别Capsule会更加活跃,借由动态路由算法加强预测.\n<!-- more -->\n> 本文综合多篇中文和外文博客(原文链接均在参考链接中给出),结合自己理解转述而来,[original paper](https://arxiv.org/pdf/1710.09829.pdf) by Hinton\n\n## CNN 的局限性\n1. 需要**大量训练数据**(测试时只能识别已经训练的特征).Capsule能使用更少数据实现更好泛化\n2. 不能处理**复杂场景**(如特征重叠),而Capsule能很好处理复杂场景(crowded scenes,如数字重叠),因为Capsule能很好实现特征分配(通过动态路由)\n3. Pooling层丢失大量信息,降低了空间分辨率(spatial resolution),所以输出对输入的微小变化不敏感,这在需要细节信息的任务(语义分割)上是糟糕的.当然通过构建复杂CNN网络能恢复信息损失.而Capsule能精确捕捉细节信息(pose information,如角度,厚度,大小,位置等)而不是丢失后再恢复信息.因此微小输入变化导致输出变化,这被称为**等效性**(equivariance).因此Capsule能在不同的视觉任务使用相同且简单的结构.\n4. CNN需要额外的组件来自动识别不同物体属于哪个对象(如这个胳膊属于这只羊),而Capsule提供了对象的**层次结构**.\n5. CNN**忽略结构信息**,仅仅考虑'有没有',而没有考虑feature map的结构关系,包含位置,角度等.\n\n## 人类视觉识别\n任何物体都是由多个更小的实体组成.例如,树由树干,树冠和树根组成.这些部分形成层次结构.树冠还包括树枝和树叶.\n\n<center>\n    <img src=\"/uploads/images/Capsule-tree_parts_diagram.png\"  title='Capsule tree parts diagram' width=\"600\">\n</center>\n\n当我们看物体时,我们的眼睛就会形成一些**固定点(fixation points)**,这些固定点的相对位置和性质有助于我们的大脑识别这个物体.因此,我们的大脑不必处理每个细节.只要看到一些树叶和树枝,我们的大脑就会认出树冠.并且树冠在树干上.结合这种层次信息,我们的大脑知道有一棵树.从现在开始,我们将对象的各个部分称为实体.(the parts of the objects as entities)\n\n<center>\n    <img src=\"/uploads/images/Capsule-tree.png\"  title='Capsule-tree' width=\"600\">\n</center>\n\n## CapsNets 到底是什么?\n简短的说,CapsNet由capsule(胶囊)而非神经元组成.胶囊是一小组神经元,其学习检测图像的给定区域内的特定对象(如矩形),并且输出向量(如8维向量).其**长度**表示对象存在的概率,其**方向**编码对象的姿势参数(如位置,旋转等).如果稍微改变输入对象(如移位,旋转,调整大小等),胶囊将输出相同长度的矢量,但方向略微不同.因此.胶囊具有等效性.\n与常规神经网络相似,CapsNet也有多个层.最下层的胶囊称为主胶囊(primary capsules),它们中的每一个都接收图像的一小部分作为输入(receptive field),并且它试图检测特定图案的存在和姿势.更高层的胶囊(routing capsules)可以检测更大和更复杂的物体(如船只).\n\n<center>\n    <img src=\"/uploads/images/two-layer CapsNet.png\"  title='the primary capsule layer has two maps of 5x5 capsules, while the second capsule layer has two maps of 3x3 capsules. Each capsule outputs a vector' width=\"600\">\n</center>\n\n使用常规卷积层即可实现主要胶囊层.例如,论文中,使用两个卷积层,输出256个包含标量的6x6特征映射.重塑此输出以获得包含8维向量的32个6x6特征映射.最后,使用新颖的squash函数来确保这些向量的长度在0和1之间(表示概率).这给出了主胶囊的输出.\n下一层中的胶囊也会尝试检测物体及其姿势,但工作方式却截然不同,使用称为路由协议的算法.\n\n## 路由协议(Routing by agreement)\n假设只有两个主要胶囊:一个矩形胶囊和一个三角形胶囊,并假设它们都检测实体.注意到矩形和三角形在船的姿势上是一致的,而他们对房子的姿势非常不同意.因此,矩形和三角形很可能是同一艘船的一部分.(值得注意的是实体的形状及整体/部分之间的关系是在训练中学习的)\n\n<center>\n    <img src=\"/uploads/images/Routing by agreement, step 1.png\"  title='predict the presence and pose of objects based on the presence and pose of object parts, then look for agreement between the predictions. Image by Aurélien Géron.' width=\"600\">\n</center>\n\n由于现在确信矩形和三角形是船的一部分,因此将矩形和三角形胶囊的输出更多地发送到船舱中是有意义的,而对于房屋舱更少:这样,船舱将接收更有用的输入信号,房屋胶囊将收到更少的噪音.即在达成一致时增加路由权重,并在出现不一致时减少路由权重.\n\n<center>\n    <img src=\"/uploads/images/Routing by agreement, step 2.png\"  title='update the routing weights. Image by Aurélien Géron.' width=\"600\">\n</center>\n\n路由协议算法涉及**一致性检测+路由更新**的几次迭代(注意,这发生在每次预测中,而不仅仅是训练时,不仅仅是一次).这在拥挤的场景中尤其有用.在混淆场景中很可能会收敛到一个更好的解释:底部的船,顶部的房子.模糊性被'解释了':下方的矩形最好用船的存在来解释,这也解释了下三角形,一旦解释了这两个部分,其余部分很容易被解释为房屋.\n\n<center>\n    <img src=\"/uploads/images/Routing by agreement can parse crowded scenes.png\"  title='Routing by agreement can parse crowded scenes, such as this ambiguous image, which could be misinterpreted as an upside-down house plus some unexplained parts. Instead, the lower rectangle will be routed to the boat, and this will also pull the lower triangle into the boat as well. Once that boat is “explained away,” it’s easy to interpret the top part as a house. Image by Aurélien Géron.' width=\"600\">\n</center>\n\n## CapsNet背后的数学\n假设层$l$和$l+1$分别具有$m$和$n$个胶囊.我们的任务是在给定层$l$的激活向量下计算层$l+1$处胶囊的激活向量.设$u$表示第$l$层胶囊的激活向量.我们必须计算$v$,胶囊在$l+1$层的激活向量。\n对于层$l+1$处的胶囊$j$:\n1. 我们首先通过层$l$处的胶囊计算**预测向量**.胶囊$i$($l$层)对胶囊$j$($l+1$层)的预测向量由下式给出\n\n$$\\boldsymbol{\\hat{\\textbf{u}}}\\_{j|i} = \\boldsymbol{\\textbf{W}}\\_{ij}\\boldsymbol{\\textbf{u}}_{i}$$\n\n$W_ {ij}$是权重矩阵\n2. 计算胶囊$j$的**输出矢量**.胶囊$j$输出向量是胶囊层$l$capsules胶囊给出的所有预测向量的加权和\n\n$$s_j = \\sum_{i=1}^{m}{c_{ij}\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}}$$\n\n标量$c_ {ij}$称为胶囊$i$($l$层)对胶囊$j$($l+1$层)之间的**耦合系数**.系数由**迭代动态路由**算法确定\n3. 在输出向量上应用**squashing**函数来获得激活向量\n\n<center>\n    <img src=\"/uploads/images/Capsule-squash.png\"  title='squash' width=\"600\">\n</center>\n\n## 动态路由算法\n层$l+1$的激活向量将反馈信号发送到层$l$处的胶囊.如果胶囊$j$($l+1$层)的激活向量与胶囊$i$($l$层)的预测矢量一致,则它们的点积应该比较大.因此,预测向量的'权重'在$j$的输出向量中增加.换句话说,那些贡献越大的预测向量在输出向量(激活向量)中具有更多的权重.循环持续4-5轮.(这像一种聚类算法,可以参考链接4)\n低级别胶囊对高级别胶囊的的预测权重总和应该为1\n$$c_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{k}{\\exp(b_{ik})}}$$ \n显然\n$$\\sum_{k}{c_{ik}} = 1$$\nlogit$b_{ij}$表示胶囊$i$($l$层)和胶囊$j$($l+1$层)是否具有强耦合.换句话说,它是由胶囊$i$解释胶囊$j$的存在性大小的量度.所有$b_{ij}$初始化应该是相等的.\n\n**Routing algorithm:**\n\n{% note info %} \n\nGiven: 预测向量$\\boldsymbol{\\hat{\\textbf{u}}}\\_{j|i}$, 迭代次数$r$\n对胶囊$i$(层$l$)和胶囊$j$(层$l+1$):$b_{ij} = 0$\nfor $r$ iterations do:\n&emsp; 对所有胶囊$i$(层$l$): $c\\_{i }= softmax(b_i)$ **(对高级别胶囊预测权重总和为1)**\n&emsp; 对所有胶囊$j$(层$l+1$): $s\\_{j} = \\sum\\_{i=1}^{m}{c\\_{ij}\\boldsymbol{\\hat{\\textbf{u}}}\\_{j|i}}$ **(输出向量是预测向量的加权和)**\n&emsp; 对所有胶囊$j$(层$l+1$): $\\textbf{v}\\_{j} = \\textbf{squash}(\\textbf{s}\\_{j})$ **(应用激活函数)**\n&emsp; 对所有胶囊$i$(层$l$)和胶囊$j$(层$l+1$): $b_{ij}=b_{ij}+\\boldsymbol{\\hat{\\textbf{u}}}\\_{j|i} \\cdot \\textbf{v}_{j}$ \n返回 $\\textbf{v}_j$\n{% endnote %}\n\n循环中的最后一行非常重要.这是路由发生的地方.如果乘积$\\boldsymbol{\\hat{\\textbf{u}}}\\_{j|i} \\cdot \\textbf{v}\\_{j}$很大,它将增加$b\\_{ij}$,这将增加相应的耦合系数$c\\_{ij}$,这反过来将使乘积$\\boldsymbol{\\hat{\\textbf{u}}}\\_{j|i} \\cdot \\textbf{v}_{j}$更大.(在链接4里面博主有自己的想法)\n\n## 路由算法优势\n在CNN中,存在池化层.通常使用MaxPool,这是一种非常原始的路由机制.局部池化中最活跃的特征(比如4x4网格)被路由到更高层,而更高级别的检测器在路由中没有发言权.将其与CapsNet中引入的协议路由机制进行比较,只有那些与高级探测器一致的功能才会被路由.这是CapsNet优于CNN的优势.它具有卓越的动态路由机制(动态,因为要路由的信息是实时确定的).\n\n## 优点和缺点\n\n**Prons:**\n1. 更少训练数据\n2. 等效性保留了输入对象的位置信息\n3. 动态路由算法对重叠对象(特征)很有用\n4. 自动计算了输入物体的层次结构\n5. 激活向量可解释性更强\n\n**Cons:**\n1. 训练慢(因为动态路由的内循环)\n2. 没有在大数据集(如ImageNet)上测试\n3. 在复杂数据集CIFAR10上效果不好\n4. 不能区分彼此靠近的俩个相同类型的相同物体(Problem of crowding)\n\n## 参考链接\n\n* [Introducing capsule networks](https://www.oreilly.com/ideas/introducing-capsule-networks) \n* [Beginner's Guide to Capsule Networks](https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-capsule-networks) \n* [Understanding Capsule Network Architecture](https://software.intel.com/en-us/articles/understanding-capsule-network-architecture) \n* [揭开迷雾，来一顿美味的Capsule盛宴](https://kexue.fm/archives/4819)\n* [Understanding Hinton’s Capsule Networks. Part I: Intuition.](https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b) \n* [GitHub:awesome-capsule-networks](https://github.com/sekwiatkowski/awesome-capsule-networks#dynamic-routing-implementations) \n* [一个注释非常详细的tensorflow源码](https://github.com/ageron/handson-ml/blob/master/extra_capsnets-cn.ipynb) \n* [CSDN:Dynamic Routing Between Capsules（NIPS2017）](https://blog.csdn.net/xyj1536214199/article/details/78698326) ","slug":"Dynamic-Routing-Between-Capsules","published":1,"updated":"2019-03-19T15:03:24.855Z","layout":"post","photos":[],"link":"","_id":"cjtfwr9oq000y49zfj9vk7v7p","content":"<p>Capsule是一组<strong>vector in vector out</strong>的神经元,每一个胶囊代表某一特定实体,如对象或对象部分.其向量模长代表实体存在概率,向量参数代表实体实例化参数.低级别Capsule向量通过变化矩阵实现对高级别Capsule实体参数进行预测.当多个预测一致时,该高级别Capsule会更加活跃,借由动态路由算法加强预测.<br><a id=\"more\"></a></p><blockquote><p>本文综合多篇中文和外文博客(原文链接均在参考链接中给出),结合自己理解转述而来,<a href=\"https://arxiv.org/pdf/1710.09829.pdf\" target=\"_blank\" rel=\"noopener\">original paper</a> by Hinton</p></blockquote><h2 id=\"CNN-的局限性\"><a href=\"#CNN-的局限性\" class=\"headerlink\" title=\"CNN 的局限性\"></a>CNN 的局限性</h2><ol><li>需要<strong>大量训练数据</strong>(测试时只能识别已经训练的特征).Capsule能使用更少数据实现更好泛化</li><li>不能处理<strong>复杂场景</strong>(如特征重叠),而Capsule能很好处理复杂场景(crowded scenes,如数字重叠),因为Capsule能很好实现特征分配(通过动态路由)</li><li>Pooling层丢失大量信息,降低了空间分辨率(spatial resolution),所以输出对输入的微小变化不敏感,这在需要细节信息的任务(语义分割)上是糟糕的.当然通过构建复杂CNN网络能恢复信息损失.而Capsule能精确捕捉细节信息(pose information,如角度,厚度,大小,位置等)而不是丢失后再恢复信息.因此微小输入变化导致输出变化,这被称为<strong>等效性</strong>(equivariance).因此Capsule能在不同的视觉任务使用相同且简单的结构.</li><li>CNN需要额外的组件来自动识别不同物体属于哪个对象(如这个胳膊属于这只羊),而Capsule提供了对象的<strong>层次结构</strong>.</li><li>CNN<strong>忽略结构信息</strong>,仅仅考虑’有没有’,而没有考虑feature map的结构关系,包含位置,角度等.</li></ol><h2 id=\"人类视觉识别\"><a href=\"#人类视觉识别\" class=\"headerlink\" title=\"人类视觉识别\"></a>人类视觉识别</h2><p>任何物体都是由多个更小的实体组成.例如,树由树干,树冠和树根组成.这些部分形成层次结构.树冠还包括树枝和树叶.</p><center><br><img src=\"/uploads/images/Capsule-tree_parts_diagram.png\" title=\"Capsule tree parts diagram\" width=\"600\"><br></center><p>当我们看物体时,我们的眼睛就会形成一些<strong>固定点(fixation points)</strong>,这些固定点的相对位置和性质有助于我们的大脑识别这个物体.因此,我们的大脑不必处理每个细节.只要看到一些树叶和树枝,我们的大脑就会认出树冠.并且树冠在树干上.结合这种层次信息,我们的大脑知道有一棵树.从现在开始,我们将对象的各个部分称为实体.(the parts of the objects as entities)</p><center><br><img src=\"/uploads/images/Capsule-tree.png\" title=\"Capsule-tree\" width=\"600\"><br></center><h2 id=\"CapsNets-到底是什么\"><a href=\"#CapsNets-到底是什么\" class=\"headerlink\" title=\"CapsNets 到底是什么?\"></a>CapsNets 到底是什么?</h2><p>简短的说,CapsNet由capsule(胶囊)而非神经元组成.胶囊是一小组神经元,其学习检测图像的给定区域内的特定对象(如矩形),并且输出向量(如8维向量).其<strong>长度</strong>表示对象存在的概率,其<strong>方向</strong>编码对象的姿势参数(如位置,旋转等).如果稍微改变输入对象(如移位,旋转,调整大小等),胶囊将输出相同长度的矢量,但方向略微不同.因此.胶囊具有等效性.<br>与常规神经网络相似,CapsNet也有多个层.最下层的胶囊称为主胶囊(primary capsules),它们中的每一个都接收图像的一小部分作为输入(receptive field),并且它试图检测特定图案的存在和姿势.更高层的胶囊(routing capsules)可以检测更大和更复杂的物体(如船只).</p><center><br><img src=\"/uploads/images/two-layer CapsNet.png\" title=\"the primary capsule layer has two maps of 5x5 capsules, while the second capsule layer has two maps of 3x3 capsules. Each capsule outputs a vector\" width=\"600\"><br></center><p>使用常规卷积层即可实现主要胶囊层.例如,论文中,使用两个卷积层,输出256个包含标量的6x6特征映射.重塑此输出以获得包含8维向量的32个6x6特征映射.最后,使用新颖的squash函数来确保这些向量的长度在0和1之间(表示概率).这给出了主胶囊的输出.<br>下一层中的胶囊也会尝试检测物体及其姿势,但工作方式却截然不同,使用称为路由协议的算法.</p><h2 id=\"路由协议-Routing-by-agreement\"><a href=\"#路由协议-Routing-by-agreement\" class=\"headerlink\" title=\"路由协议(Routing by agreement)\"></a>路由协议(Routing by agreement)</h2><p>假设只有两个主要胶囊:一个矩形胶囊和一个三角形胶囊,并假设它们都检测实体.注意到矩形和三角形在船的姿势上是一致的,而他们对房子的姿势非常不同意.因此,矩形和三角形很可能是同一艘船的一部分.(值得注意的是实体的形状及整体/部分之间的关系是在训练中学习的)</p><center><br><img src=\"/uploads/images/Routing by agreement, step 1.png\" title=\"predict the presence and pose of objects based on the presence and pose of object parts, then look for agreement between the predictions. Image by Aurélien Géron.\" width=\"600\"><br></center><p>由于现在确信矩形和三角形是船的一部分,因此将矩形和三角形胶囊的输出更多地发送到船舱中是有意义的,而对于房屋舱更少:这样,船舱将接收更有用的输入信号,房屋胶囊将收到更少的噪音.即在达成一致时增加路由权重,并在出现不一致时减少路由权重.</p><center><br><img src=\"/uploads/images/Routing by agreement, step 2.png\" title=\"update the routing weights. Image by Aurélien Géron.\" width=\"600\"><br></center><p>路由协议算法涉及<strong>一致性检测+路由更新</strong>的几次迭代(注意,这发生在每次预测中,而不仅仅是训练时,不仅仅是一次).这在拥挤的场景中尤其有用.在混淆场景中很可能会收敛到一个更好的解释:底部的船,顶部的房子.模糊性被’解释了’:下方的矩形最好用船的存在来解释,这也解释了下三角形,一旦解释了这两个部分,其余部分很容易被解释为房屋.</p><center><br><img src=\"/uploads/images/Routing by agreement can parse crowded scenes.png\" title=\"Routing by agreement can parse crowded scenes, such as this ambiguous image, which could be misinterpreted as an upside-down house plus some unexplained parts. Instead, the lower rectangle will be routed to the boat, and this will also pull the lower triangle into the boat as well. Once that boat is “explained away,” it’s easy to interpret the top part as a house. Image by Aurélien Géron.\" width=\"600\"><br></center><h2 id=\"CapsNet背后的数学\"><a href=\"#CapsNet背后的数学\" class=\"headerlink\" title=\"CapsNet背后的数学\"></a>CapsNet背后的数学</h2><p>假设层$l$和$l+1$分别具有$m$和$n$个胶囊.我们的任务是在给定层$l$的激活向量下计算层$l+1$处胶囊的激活向量.设$u$表示第$l$层胶囊的激活向量.我们必须计算$v$,胶囊在$l+1$层的激活向量。<br>对于层$l+1$处的胶囊$j$:</p><ol><li>我们首先通过层$l$处的胶囊计算<strong>预测向量</strong>.胶囊$i$($l$层)对胶囊$j$($l+1$层)的预测向量由下式给出</li></ol><p>$$\\boldsymbol{\\hat{\\textbf{u}}}_{j|i} = \\boldsymbol{\\textbf{W}}_{ij}\\boldsymbol{\\textbf{u}}_{i}$$</p><p>$W_ {ij}$是权重矩阵</p><ol start=\"2\"><li>计算胶囊$j$的<strong>输出矢量</strong>.胶囊$j$输出向量是胶囊层$l$capsules胶囊给出的所有预测向量的加权和</li></ol><p>$$s_j = \\sum_{i=1}^{m}{c_{ij}\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}}$$</p><p>标量$c_ {ij}$称为胶囊$i$($l$层)对胶囊$j$($l+1$层)之间的<strong>耦合系数</strong>.系数由<strong>迭代动态路由</strong>算法确定</p><ol start=\"3\"><li>在输出向量上应用<strong>squashing</strong>函数来获得激活向量</li></ol><center><br><img src=\"/uploads/images/Capsule-squash.png\" title=\"squash\" width=\"600\"><br></center><h2 id=\"动态路由算法\"><a href=\"#动态路由算法\" class=\"headerlink\" title=\"动态路由算法\"></a>动态路由算法</h2><p>层$l+1$的激活向量将反馈信号发送到层$l$处的胶囊.如果胶囊$j$($l+1$层)的激活向量与胶囊$i$($l$层)的预测矢量一致,则它们的点积应该比较大.因此,预测向量的’权重’在$j$的输出向量中增加.换句话说,那些贡献越大的预测向量在输出向量(激活向量)中具有更多的权重.循环持续4-5轮.(这像一种聚类算法,可以参考链接4)<br>低级别胶囊对高级别胶囊的的预测权重总和应该为1<br>$$c_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{k}{\\exp(b_{ik})}}$$<br>显然<br>$$\\sum_{k}{c_{ik}} = 1$$<br>logit$b_{ij}$表示胶囊$i$($l$层)和胶囊$j$($l+1$层)是否具有强耦合.换句话说,它是由胶囊$i$解释胶囊$j$的存在性大小的量度.所有$b_{ij}$初始化应该是相等的.</p><p><strong>Routing algorithm:</strong></p><div class=\"note info\"><p>Given: 预测向量$\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}$, 迭代次数$r$<br>对胶囊$i$(层$l$)和胶囊$j$(层$l+1$):$b_{ij} = 0$<br>for $r$ iterations do:<br>&emsp; 对所有胶囊$i$(层$l$): $c_{i }= softmax(b_i)$ <strong>(对高级别胶囊预测权重总和为1)</strong><br>&emsp; 对所有胶囊$j$(层$l+1$): $s_{j} = \\sum_{i=1}^{m}{c_{ij}\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}}$ <strong>(输出向量是预测向量的加权和)</strong><br>&emsp; 对所有胶囊$j$(层$l+1$): $\\textbf{v}_{j} = \\textbf{squash}(\\textbf{s}_{j})$ <strong>(应用激活函数)</strong><br>&emsp; 对所有胶囊$i$(层$l$)和胶囊$j$(层$l+1$): $b_{ij}=b_{ij}+\\boldsymbol{\\hat{\\textbf{u}}}_{j|i} \\cdot \\textbf{v}_{j}$<br>返回 $\\textbf{v}_j$</p></div><p>循环中的最后一行非常重要.这是路由发生的地方.如果乘积$\\boldsymbol{\\hat{\\textbf{u}}}_{j|i} \\cdot \\textbf{v}_{j}$很大,它将增加$b_{ij}$,这将增加相应的耦合系数$c_{ij}$,这反过来将使乘积$\\boldsymbol{\\hat{\\textbf{u}}}_{j|i} \\cdot \\textbf{v}_{j}$更大.(在链接4里面博主有自己的想法)</p><h2 id=\"路由算法优势\"><a href=\"#路由算法优势\" class=\"headerlink\" title=\"路由算法优势\"></a>路由算法优势</h2><p>在CNN中,存在池化层.通常使用MaxPool,这是一种非常原始的路由机制.局部池化中最活跃的特征(比如4x4网格)被路由到更高层,而更高级别的检测器在路由中没有发言权.将其与CapsNet中引入的协议路由机制进行比较,只有那些与高级探测器一致的功能才会被路由.这是CapsNet优于CNN的优势.它具有卓越的动态路由机制(动态,因为要路由的信息是实时确定的).</p><h2 id=\"优点和缺点\"><a href=\"#优点和缺点\" class=\"headerlink\" title=\"优点和缺点\"></a>优点和缺点</h2><p><strong>Prons:</strong></p><ol><li>更少训练数据</li><li>等效性保留了输入对象的位置信息</li><li>动态路由算法对重叠对象(特征)很有用</li><li>自动计算了输入物体的层次结构</li><li>激活向量可解释性更强</li></ol><p><strong>Cons:</strong></p><ol><li>训练慢(因为动态路由的内循环)</li><li>没有在大数据集(如ImageNet)上测试</li><li>在复杂数据集CIFAR10上效果不好</li><li>不能区分彼此靠近的俩个相同类型的相同物体(Problem of crowding)</li></ol><h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><ul><li><a href=\"https://www.oreilly.com/ideas/introducing-capsule-networks\" target=\"_blank\" rel=\"noopener\">Introducing capsule networks</a></li><li><a href=\"https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-capsule-networks\" target=\"_blank\" rel=\"noopener\">Beginner’s Guide to Capsule Networks</a></li><li><a href=\"https://software.intel.com/en-us/articles/understanding-capsule-network-architecture\" target=\"_blank\" rel=\"noopener\">Understanding Capsule Network Architecture</a></li><li><a href=\"https://kexue.fm/archives/4819\" target=\"_blank\" rel=\"noopener\">揭开迷雾，来一顿美味的Capsule盛宴</a></li><li><a href=\"https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b\" target=\"_blank\" rel=\"noopener\">Understanding Hinton’s Capsule Networks. Part I: Intuition.</a></li><li><a href=\"https://github.com/sekwiatkowski/awesome-capsule-networks#dynamic-routing-implementations\" target=\"_blank\" rel=\"noopener\">GitHub:awesome-capsule-networks</a></li><li><a href=\"https://github.com/ageron/handson-ml/blob/master/extra_capsnets-cn.ipynb\" target=\"_blank\" rel=\"noopener\">一个注释非常详细的tensorflow源码</a></li><li><a href=\"https://blog.csdn.net/xyj1536214199/article/details/78698326\" target=\"_blank\" rel=\"noopener\">CSDN:Dynamic Routing Between Capsules（NIPS2017）</a></li></ul>","site":{"data":{}},"excerpt":"<p>Capsule是一组<strong>vector in vector out</strong>的神经元,每一个胶囊代表某一特定实体,如对象或对象部分.其向量模长代表实体存在概率,向量参数代表实体实例化参数.低级别Capsule向量通过变化矩阵实现对高级别Capsule实体参数进行预测.当多个预测一致时,该高级别Capsule会更加活跃,借由动态路由算法加强预测.<br>","more":"</p><blockquote><p>本文综合多篇中文和外文博客(原文链接均在参考链接中给出),结合自己理解转述而来,<a href=\"https://arxiv.org/pdf/1710.09829.pdf\" target=\"_blank\" rel=\"noopener\">original paper</a> by Hinton</p></blockquote><h2 id=\"CNN-的局限性\"><a href=\"#CNN-的局限性\" class=\"headerlink\" title=\"CNN 的局限性\"></a>CNN 的局限性</h2><ol><li>需要<strong>大量训练数据</strong>(测试时只能识别已经训练的特征).Capsule能使用更少数据实现更好泛化</li><li>不能处理<strong>复杂场景</strong>(如特征重叠),而Capsule能很好处理复杂场景(crowded scenes,如数字重叠),因为Capsule能很好实现特征分配(通过动态路由)</li><li>Pooling层丢失大量信息,降低了空间分辨率(spatial resolution),所以输出对输入的微小变化不敏感,这在需要细节信息的任务(语义分割)上是糟糕的.当然通过构建复杂CNN网络能恢复信息损失.而Capsule能精确捕捉细节信息(pose information,如角度,厚度,大小,位置等)而不是丢失后再恢复信息.因此微小输入变化导致输出变化,这被称为<strong>等效性</strong>(equivariance).因此Capsule能在不同的视觉任务使用相同且简单的结构.</li><li>CNN需要额外的组件来自动识别不同物体属于哪个对象(如这个胳膊属于这只羊),而Capsule提供了对象的<strong>层次结构</strong>.</li><li>CNN<strong>忽略结构信息</strong>,仅仅考虑’有没有’,而没有考虑feature map的结构关系,包含位置,角度等.</li></ol><h2 id=\"人类视觉识别\"><a href=\"#人类视觉识别\" class=\"headerlink\" title=\"人类视觉识别\"></a>人类视觉识别</h2><p>任何物体都是由多个更小的实体组成.例如,树由树干,树冠和树根组成.这些部分形成层次结构.树冠还包括树枝和树叶.</p><center><br><img src=\"/uploads/images/Capsule-tree_parts_diagram.png\" title=\"Capsule tree parts diagram\" width=\"600\"><br></center><p>当我们看物体时,我们的眼睛就会形成一些<strong>固定点(fixation points)</strong>,这些固定点的相对位置和性质有助于我们的大脑识别这个物体.因此,我们的大脑不必处理每个细节.只要看到一些树叶和树枝,我们的大脑就会认出树冠.并且树冠在树干上.结合这种层次信息,我们的大脑知道有一棵树.从现在开始,我们将对象的各个部分称为实体.(the parts of the objects as entities)</p><center><br><img src=\"/uploads/images/Capsule-tree.png\" title=\"Capsule-tree\" width=\"600\"><br></center><h2 id=\"CapsNets-到底是什么\"><a href=\"#CapsNets-到底是什么\" class=\"headerlink\" title=\"CapsNets 到底是什么?\"></a>CapsNets 到底是什么?</h2><p>简短的说,CapsNet由capsule(胶囊)而非神经元组成.胶囊是一小组神经元,其学习检测图像的给定区域内的特定对象(如矩形),并且输出向量(如8维向量).其<strong>长度</strong>表示对象存在的概率,其<strong>方向</strong>编码对象的姿势参数(如位置,旋转等).如果稍微改变输入对象(如移位,旋转,调整大小等),胶囊将输出相同长度的矢量,但方向略微不同.因此.胶囊具有等效性.<br>与常规神经网络相似,CapsNet也有多个层.最下层的胶囊称为主胶囊(primary capsules),它们中的每一个都接收图像的一小部分作为输入(receptive field),并且它试图检测特定图案的存在和姿势.更高层的胶囊(routing capsules)可以检测更大和更复杂的物体(如船只).</p><center><br><img src=\"/uploads/images/two-layer CapsNet.png\" title=\"the primary capsule layer has two maps of 5x5 capsules, while the second capsule layer has two maps of 3x3 capsules. Each capsule outputs a vector\" width=\"600\"><br></center><p>使用常规卷积层即可实现主要胶囊层.例如,论文中,使用两个卷积层,输出256个包含标量的6x6特征映射.重塑此输出以获得包含8维向量的32个6x6特征映射.最后,使用新颖的squash函数来确保这些向量的长度在0和1之间(表示概率).这给出了主胶囊的输出.<br>下一层中的胶囊也会尝试检测物体及其姿势,但工作方式却截然不同,使用称为路由协议的算法.</p><h2 id=\"路由协议-Routing-by-agreement\"><a href=\"#路由协议-Routing-by-agreement\" class=\"headerlink\" title=\"路由协议(Routing by agreement)\"></a>路由协议(Routing by agreement)</h2><p>假设只有两个主要胶囊:一个矩形胶囊和一个三角形胶囊,并假设它们都检测实体.注意到矩形和三角形在船的姿势上是一致的,而他们对房子的姿势非常不同意.因此,矩形和三角形很可能是同一艘船的一部分.(值得注意的是实体的形状及整体/部分之间的关系是在训练中学习的)</p><center><br><img src=\"/uploads/images/Routing by agreement, step 1.png\" title=\"predict the presence and pose of objects based on the presence and pose of object parts, then look for agreement between the predictions. Image by Aurélien Géron.\" width=\"600\"><br></center><p>由于现在确信矩形和三角形是船的一部分,因此将矩形和三角形胶囊的输出更多地发送到船舱中是有意义的,而对于房屋舱更少:这样,船舱将接收更有用的输入信号,房屋胶囊将收到更少的噪音.即在达成一致时增加路由权重,并在出现不一致时减少路由权重.</p><center><br><img src=\"/uploads/images/Routing by agreement, step 2.png\" title=\"update the routing weights. Image by Aurélien Géron.\" width=\"600\"><br></center><p>路由协议算法涉及<strong>一致性检测+路由更新</strong>的几次迭代(注意,这发生在每次预测中,而不仅仅是训练时,不仅仅是一次).这在拥挤的场景中尤其有用.在混淆场景中很可能会收敛到一个更好的解释:底部的船,顶部的房子.模糊性被’解释了’:下方的矩形最好用船的存在来解释,这也解释了下三角形,一旦解释了这两个部分,其余部分很容易被解释为房屋.</p><center><br><img src=\"/uploads/images/Routing by agreement can parse crowded scenes.png\" title=\"Routing by agreement can parse crowded scenes, such as this ambiguous image, which could be misinterpreted as an upside-down house plus some unexplained parts. Instead, the lower rectangle will be routed to the boat, and this will also pull the lower triangle into the boat as well. Once that boat is “explained away,” it’s easy to interpret the top part as a house. Image by Aurélien Géron.\" width=\"600\"><br></center><h2 id=\"CapsNet背后的数学\"><a href=\"#CapsNet背后的数学\" class=\"headerlink\" title=\"CapsNet背后的数学\"></a>CapsNet背后的数学</h2><p>假设层$l$和$l+1$分别具有$m$和$n$个胶囊.我们的任务是在给定层$l$的激活向量下计算层$l+1$处胶囊的激活向量.设$u$表示第$l$层胶囊的激活向量.我们必须计算$v$,胶囊在$l+1$层的激活向量。<br>对于层$l+1$处的胶囊$j$:</p><ol><li>我们首先通过层$l$处的胶囊计算<strong>预测向量</strong>.胶囊$i$($l$层)对胶囊$j$($l+1$层)的预测向量由下式给出</li></ol><p>$$\\boldsymbol{\\hat{\\textbf{u}}}_{j|i} = \\boldsymbol{\\textbf{W}}_{ij}\\boldsymbol{\\textbf{u}}_{i}$$</p><p>$W_ {ij}$是权重矩阵</p><ol start=\"2\"><li>计算胶囊$j$的<strong>输出矢量</strong>.胶囊$j$输出向量是胶囊层$l$capsules胶囊给出的所有预测向量的加权和</li></ol><p>$$s_j = \\sum_{i=1}^{m}{c_{ij}\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}}$$</p><p>标量$c_ {ij}$称为胶囊$i$($l$层)对胶囊$j$($l+1$层)之间的<strong>耦合系数</strong>.系数由<strong>迭代动态路由</strong>算法确定</p><ol start=\"3\"><li>在输出向量上应用<strong>squashing</strong>函数来获得激活向量</li></ol><center><br><img src=\"/uploads/images/Capsule-squash.png\" title=\"squash\" width=\"600\"><br></center><h2 id=\"动态路由算法\"><a href=\"#动态路由算法\" class=\"headerlink\" title=\"动态路由算法\"></a>动态路由算法</h2><p>层$l+1$的激活向量将反馈信号发送到层$l$处的胶囊.如果胶囊$j$($l+1$层)的激活向量与胶囊$i$($l$层)的预测矢量一致,则它们的点积应该比较大.因此,预测向量的’权重’在$j$的输出向量中增加.换句话说,那些贡献越大的预测向量在输出向量(激活向量)中具有更多的权重.循环持续4-5轮.(这像一种聚类算法,可以参考链接4)<br>低级别胶囊对高级别胶囊的的预测权重总和应该为1<br>$$c_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{k}{\\exp(b_{ik})}}$$<br>显然<br>$$\\sum_{k}{c_{ik}} = 1$$<br>logit$b_{ij}$表示胶囊$i$($l$层)和胶囊$j$($l+1$层)是否具有强耦合.换句话说,它是由胶囊$i$解释胶囊$j$的存在性大小的量度.所有$b_{ij}$初始化应该是相等的.</p><p><strong>Routing algorithm:</strong></p><div class=\"note info\"><p>Given: 预测向量$\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}$, 迭代次数$r$<br>对胶囊$i$(层$l$)和胶囊$j$(层$l+1$):$b_{ij} = 0$<br>for $r$ iterations do:<br>&emsp; 对所有胶囊$i$(层$l$): $c_{i }= softmax(b_i)$ <strong>(对高级别胶囊预测权重总和为1)</strong><br>&emsp; 对所有胶囊$j$(层$l+1$): $s_{j} = \\sum_{i=1}^{m}{c_{ij}\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}}$ <strong>(输出向量是预测向量的加权和)</strong><br>&emsp; 对所有胶囊$j$(层$l+1$): $\\textbf{v}_{j} = \\textbf{squash}(\\textbf{s}_{j})$ <strong>(应用激活函数)</strong><br>&emsp; 对所有胶囊$i$(层$l$)和胶囊$j$(层$l+1$): $b_{ij}=b_{ij}+\\boldsymbol{\\hat{\\textbf{u}}}_{j|i} \\cdot \\textbf{v}_{j}$<br>返回 $\\textbf{v}_j$</p></div><p>循环中的最后一行非常重要.这是路由发生的地方.如果乘积$\\boldsymbol{\\hat{\\textbf{u}}}_{j|i} \\cdot \\textbf{v}_{j}$很大,它将增加$b_{ij}$,这将增加相应的耦合系数$c_{ij}$,这反过来将使乘积$\\boldsymbol{\\hat{\\textbf{u}}}_{j|i} \\cdot \\textbf{v}_{j}$更大.(在链接4里面博主有自己的想法)</p><h2 id=\"路由算法优势\"><a href=\"#路由算法优势\" class=\"headerlink\" title=\"路由算法优势\"></a>路由算法优势</h2><p>在CNN中,存在池化层.通常使用MaxPool,这是一种非常原始的路由机制.局部池化中最活跃的特征(比如4x4网格)被路由到更高层,而更高级别的检测器在路由中没有发言权.将其与CapsNet中引入的协议路由机制进行比较,只有那些与高级探测器一致的功能才会被路由.这是CapsNet优于CNN的优势.它具有卓越的动态路由机制(动态,因为要路由的信息是实时确定的).</p><h2 id=\"优点和缺点\"><a href=\"#优点和缺点\" class=\"headerlink\" title=\"优点和缺点\"></a>优点和缺点</h2><p><strong>Prons:</strong></p><ol><li>更少训练数据</li><li>等效性保留了输入对象的位置信息</li><li>动态路由算法对重叠对象(特征)很有用</li><li>自动计算了输入物体的层次结构</li><li>激活向量可解释性更强</li></ol><p><strong>Cons:</strong></p><ol><li>训练慢(因为动态路由的内循环)</li><li>没有在大数据集(如ImageNet)上测试</li><li>在复杂数据集CIFAR10上效果不好</li><li>不能区分彼此靠近的俩个相同类型的相同物体(Problem of crowding)</li></ol><h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><ul><li><a href=\"https://www.oreilly.com/ideas/introducing-capsule-networks\" target=\"_blank\" rel=\"noopener\">Introducing capsule networks</a></li><li><a href=\"https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-capsule-networks\" target=\"_blank\" rel=\"noopener\">Beginner’s Guide to Capsule Networks</a></li><li><a href=\"https://software.intel.com/en-us/articles/understanding-capsule-network-architecture\" target=\"_blank\" rel=\"noopener\">Understanding Capsule Network Architecture</a></li><li><a href=\"https://kexue.fm/archives/4819\" target=\"_blank\" rel=\"noopener\">揭开迷雾，来一顿美味的Capsule盛宴</a></li><li><a href=\"https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b\" target=\"_blank\" rel=\"noopener\">Understanding Hinton’s Capsule Networks. Part I: Intuition.</a></li><li><a href=\"https://github.com/sekwiatkowski/awesome-capsule-networks#dynamic-routing-implementations\" target=\"_blank\" rel=\"noopener\">GitHub:awesome-capsule-networks</a></li><li><a href=\"https://github.com/ageron/handson-ml/blob/master/extra_capsnets-cn.ipynb\" target=\"_blank\" rel=\"noopener\">一个注释非常详细的tensorflow源码</a></li><li><a href=\"https://blog.csdn.net/xyj1536214199/article/details/78698326\" target=\"_blank\" rel=\"noopener\">CSDN:Dynamic Routing Between Capsules（NIPS2017）</a></li></ul>"},{"title":"序列模型","mathjax":true,"comments":1,"abbrlink":"94c569ba","date":"2018-12-21T02:48:09.000Z","_content":"\ndeeplearning.ai的第五课:Sequence Models.讲解了如基本的RNN网络,基本的循环单元到GRU,LSTM,再到双向RNN,还有深层版的模型.常用词嵌入的特性,不同词嵌入训练方法,集束搜索和Attention模型.\n<!-- more -->\n\n## Recurrent Neural Networks\n### Notation\n$X^{(i)<t\\>}$表示第i个训练样本的第t个输入元素\n$Y^{(i)<t\\>}$表示第i个训练样本的第t个输出元素\n$T_{x}^{(i)}$表示第i个训练样本的输入序列长度\n$T_{y}^{(i)}$表示第i个训练样本的输出序列长度\n\n### Recurrent Neural Network Model\n标准神经网络的问题:\n1. 输入输出长度可能不一致\n2. 不能很好共享文本不同位置学习到的特征\n\n<center>\n    <img src=\"/uploads/images/RNN Forward Propagation.png\" title='RNN Forward Propagation' width=\"600\">\n</center>\n\n### Backpropagation through time\n\n<center>\n    <img src=\"/uploads/images/Forward propagation and backpropagation.png\" title='Forward propagation and backpropagation' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/rnn cell backprop.png\" title='rnn cell backprop' width=\"600\">\n</center>\n \n### Different types of RNNs\n1. 多输入多输出模型(翻译模型)\n2. 多输入单输出模型(情感分析)\n3. 单输入多输出模型(音乐生成)\n4. 单输出单输出模型(简单神经网络)\n\n<center>\n    <img src=\"/uploads/images/RNN architectures.png\" title='RNN architectures' width=\"600\">\n</center>\n\n### Language model and sequence generation\n\n<center>\n    <img src=\"/uploads/images/RNN model.png\" title='RNN model' width=\"600\">\n</center>\n\n### Sampling novel sequences\n\n<center>\n    <img src=\"/uploads/images/Sampling a sequence from a trained RNN.png\" title='Sampling a sequence from a trained RNN' width=\"600\">\n</center>\n\n### Vanishing gradients with RNNs\n反向传播因为同样的梯度消失的问题,后面层的输出误差很难影响前面层的计算.不管输出是什么,不管是对的,还是错的,这个区域都很难反向传播到序列的前面部分,也因此网络很难调整序列前面的计算.如果不管的话,RNN会不擅长处理长期依赖的问题.\n梯度爆炸很容易发现,因为参数会大到崩溃,你会看到很多NaN,或者不是数字的情况,这意味着你的网络计算出现了数值溢出.\n\n### Gated Recurrent Unit (GRU)\n\n<center>\n    <img src=\"/uploads/images/RNN unit.png\" title='RNN unit' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/RNNunit.png\" title='RNNunit' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/GRU(simplified).png\" title='GRU(simplified)' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/fullGRU.png\" title='fullGRU' width=\"600\">\n</center>\n\n当$\\Gamma_{u}$很接近0,可能是0.000001或者更小,这就不会有梯度消失的问题了.因为$\\Gamma_{u}$很接近0,这就是说$c^{t}$几乎就等于$c^{t-1}$,而且$c^{t}$的值也很好地被维持了,即使经过很多很多的时间步.这就是缓解梯度消失问题的关键,因此允许神经网络运行在非常庞大的依赖词上\n\n### Long Short Term Memory (LSTM)\n\n<center>\n    <img src=\"/uploads/images/LSTM.png\" title='LSTM' width=\"600\">\n</center>\n\n> 最后公式应该为$a^{t\\}=\\Gamma_{o}*tanh(c^{t})$\n\n红线显示了只要你正确地设置了遗忘门和更新门,LSTM是相当容易把$c^{<0\\>}$的值一直往下传递到右边,比如$c^{<3\\>} = c^{<0\\>}$.这就是为什么LSTM和GRU非常擅长于长时间记忆某个值,对于存在记忆细胞中的某个值.\n\n### Deep RNNs\n\n<center>\n    <img src=\"/uploads/images/deepRNN.png\" title='deepRNN' width=\"600\">\n</center>\n\n对于RNN来说,有三层就已经不少了,不像卷积神经网络一样有大量的隐含层.或者每一个上面堆叠循环层,然后换成一些深的层,这些层并不水平连接,只是一个深层的网络.基本单元可以是最简单的RNN模型,也可以是GRU单元或者LSTM单元,并且,你也可以构建深层的双向RNN网络.\n\n## 自然语言处理与词嵌入\n\n### 词汇表征\n\nOne-hot向量表征的一大缺点是把每个词孤立起来(内积均为0),稀疏,泛化能力不强.词嵌入(Word Embedding)则可以学习到俩个词语相似之处.\n\n<center>\n    <img src=\"/uploads/images/Word Embedding.png\" title='Word Embedding' width=\"600\">\n</center>\n\n### 使用词嵌入\n\n词嵌入迁移学习:\n1. 从大量文本中学习词嵌入(1-100B words) or 下载预训练好的词嵌入模型\n2. 用词嵌入模型迁移到新的只有少量标注训练集的任务中(100k words)\n3. 可选: 继续微调(finetune)词嵌入(通常是数据集2比较大)\n\n注:语言模型和机器翻译使用词嵌入较少,因为这俩者数据集都较大\n\n### 词嵌入的特性\n\n词嵌入的一个显著成果就是,可学习的类比关系的一般性.举个例子,它能学会man对于woman相当于boy对于girl,因为man和woman之间和boy和girl之间的向量差在gender(性别)这一维都是一样的。\n\n<center>\n    <img src=\"/uploads/images/Analogies using word vectors.png\" title='Analogies using word vectors' width=\"600\">\n</center>\n\n### 嵌入矩阵\n\n<center>\n    <img src=\"/uploads/images/Embending matrix.png\" title='Embending matrix' width=\"600\">\n</center>\n\n### 学习词嵌入\n\n<center>\n    <img src=\"/uploads/images/Neural language model.png\" title='Neural language model' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/Other context-target pairs.png\" title='Other context-target pairs' width=\"600\">\n</center>\n\n研究发现,如果你想建立一个语言模型,用目标词的前几个单词作为上下文是常见做法.但如果目标是学习词嵌入,那么用这些其他类型的上下文,也能得到很好的词嵌入。\n\n### Word2Vec\n\n句子:'I want a glass of orange juice to go along with my cereal.'\nSkip-Gram模型: 抽取上下文和目标词配对,构造一个监督学习问题.随机选一个词作为上下文词,比如选orange这个词,然后随机在一定词距内选另一个词,比如在上下文词前后5或10个词范围内选择目标词.\n\n<center>\n    <img src=\"/uploads/images/Word2Vec.png\" title='词嵌入的简化模型和神经网络' width=\"600\">\n</center>\n\n关键是个softmax单元.矩阵$E$会有很多参数,所以矩阵$E$有对应所有嵌入向量$e_{c}$的参数,softmax单元也有$\\theta_{t}$的参数.优化这些参数的损失函数,就会得到一个较好的嵌入向量集,这个就叫做Skip-Gram模型.它把一个像orange这样的词作为输入,并预测这个输入词从左数或从右数的某个词是什么词.\n\n算法首要的问题就是计算速度.在softmax模型中,每次需对词汇表中的所有词做求和计算.同论文提出的还有CBOW模型.\n\n### 负采样\n\n<center>\n    <img src=\"/uploads/images/Negative Sampling.png\" title='Negative Sampling' width=\"600\">\n</center>\n\n生成数据的方式是选择一个上下文词(orange),再选一个目标词(juice),这就是表的第一行,它给了一个正样本并给定标签为1.然后给定$K$次,用相同的上下文词,再从字典中选取随机的词(king,book,the,of)等,并标记0,这些就会成为负样本.如果从字典中随机选到的词,正好出现在了词距内,比如说在上下文词orange正负10个词之内也没太大关系.**算法就是要分辨这两种不同的采样方式,这就是如何生成训练集的方法.**\n\n小数据集的话,$K$从5到20比较好.如果数据集很大,$K$就选的小一点.\n\n模型基于逻辑回归模型,不同的是将一个sigmoid函数作用于$\\theta_{t}^{T}e_{c}$,参数和之前一样.这可看做二分类逻辑回归分类器,但并不是每次迭代都训练全部10,000个词,只训练其中的5个(部分选出的词K+1个)\n\n采样负样本方法:$P\\left( w_{i} \\right) = \\frac{f\\left( w_{i} \\right)^{\\frac{3}{4}}}{\\sum_{j = 1}^{10,000}{f\\left( w_{j} \\right)^{\\frac{3}{4}}}}$\n\n### GloVe 词向量\n\n<center>\n    <img src=\"/uploads/images/Glove Model.png\" title='Glove Model' width=\"600\">\n</center>\n\nGloVe算法做的就是使上下文和目标词关系开始明确化.$X_{ij}$是单词$i$在单词$j$上下文中出现的次数,那么这里$i$和$j$就和$t$和$c$的功能一样.若上下文指左右几个词,则会得出$X_{ij}$等于$X_{ji}$这个结论.其他时候大致相等.加权因子$f\\left(X_{ij}\\right)$就可以是一个函数,$X_{ij}$为0是为0(启发性方法见GloVe算法论文).$\\theta_{i}$和$e_{j}$是对称的,而不像之前了解的模型,$\\theta$和$e$功能不一样,因此最后结果可以取平均$e_{w}^{(final)}= \\frac{e_{w} +\\theta_{w}}{2}$.\n\nGloVe差距最小化处理\n$$\\text{mini}\\text{mize}\\sum_{i = 1}^{10,000}{\\sum_{j = 1}^{10,000}{f\\left( X_{ij} \\right)\\left( \\theta_{i}^{T}e_{j} + b_{i} + b_{j}^{'} - logX_{ij} \\right)^{2}}}$$\n\n两个单词之间有多少联系,$t$和$c$之间有多紧密,$i$和$j$之间联系程度如何,换句话说就是他们同时出现的频率是多少,这是由这个$X_{ij}$影响的.然后梯度下降来最小化\n\n<center>\n    <img src=\"/uploads/images/featurization view of word embeddings.png\" title='featurization view of word embeddings' width=\"600\">\n</center>\n\n$$\\left( A\\theta_{i} \\right)^{T}\\left( A^{- T}e_{j} \\right) = \\theta_{i}^{T}A^{T}A^{- T}e_{j} = \\theta_{i}^{T}e_{j}$$\n通过GloVe算法得到的(关系)特征表示可能是原特征的潜在的任意线性变换,最终还是能学习出解决类似问题的平行四边形映射.\n\n> Word2Vec,负采样,GloVe 词向量是三种学习词向量嵌入的方法.\n\n### 情绪分类\n\n情感分类一个最大的挑战就是可能标记的训练集没有那么多.对于情感分类任务来说,训练集大小从10,000到100,000个单词都很常见,甚至有时会小于10,000个单词,采用了词嵌入能够带来更好的效果,尤其是只有很小的训练集时.\n\n<center>\n    <img src=\"/uploads/images/Simple sentiment classification model.png\" title='Simple sentiment classification model' width=\"600\">\n</center>\n\n该算法实际上会把所有单词的意思给平均.问题就是没考虑词序.\"Completely lacking in good taste, good service, and good ambiance.\",忽略词序,仅仅把所有单词的词嵌入加起来或者平均下来,分类器很可能认为这是一个好的评论.\n\n<center>\n    <img src=\"/uploads/images/RNN sentiment classification.png\" title='RNN sentiment classification' width=\"600\">\n</center>\n\n### 词嵌入除偏\n\n根据训练模型所使用的文本,词嵌入能够反映出性别、种族、年龄、性取向等其他方面的偏见,如Man对应Computer Programmer,那么Woman会对应?输出是Homemaker.\n\n<center>\n    <img src=\"/uploads/images/bias in word embedding.png\" title='bias in word embedding' width=\"600\">\n</center>\n\n1. 偏差求平均\n2. 中和.对于那些定义不确切的词可以将其处理一下,避免偏见.如doctor和babysitter想使之在性别方面是中立的,而girl、boy定义本身就含有性别\n3. 均衡步.防止又引入其他偏差.\n\n论文作者训练一个分类器尝试解决哪些词是中立的.\n\n## 序列模型和注意力机制\n\n### 基础模型\n\n1. 机器翻译到语音识别:seq2seq模型(Encoder-Decoder结构)\n2. 集束搜索(Beam search)和注意力模型(Attention Model)\n3. 音频模型\n\n### 选择最可能的句子\n\n<center>\n    <img src=\"/uploads/images/Machine translation.png\" title='Machine translation' width=\"600\">\n</center>\n\n机器翻译模型可以看作是条件语言模型,因为语言模型总是全0输入,随机地生成句子,机器翻译模型需要找到最可能的翻译,提供不同的输入(Encoder),目的是选择使句子出现可能性最大(Decoder),选择方法如Beam search,为什么不用贪心每次选择概率最大的一个词呢?这并不是最佳选择.\n\n### 集束搜索\n\n<center>\n    <img src=\"/uploads/images/Beam search algorithm.png\" title='Beam search algorithm' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/Beam search(B=3).png\" title='Beam search(B=3)' width=\"600\">\n</center>\n\n当B=3时表示每次只考虑三个可能结果,B=1即为贪心\n\n1. 在第一次词位置选出最可能的三个单词$y^{<1\\>}$\n2. 在第一步基础上计算最可能的三个单词对$P(y^{<1\\>},y^{<2\\>}|x)$\n3. 继续增加下一个单词重复上述步骤\n\n### 改进集束搜索\n\n最大化 \n$P(y^{< 1 \\>}\\ldots y^{< T_{y}\\>}|X)$=$P(y^{<1\\>}|X)$\\*$P(y^{< 2 \\>}|X,y^{< 1 \\>})$\\*$P(y^{< 3 \\>}|X,y^{< 1\\ >},y^{< 2\\>})\\ldots$$P(y^{< T_{y}\\ >}|X,y^{<1\\ >}\\ldots y^{< T_{y} - 1\\ >})$\n\n1. 改成最大化$logP(y|x)$,能防止数值下溢\n2. 原公式倾向于长度短小的翻译结果,因此可以长度归一化(除$T_{y}$)\n\n### 定向搜索的误差分析\n\n<center>\n    <img src=\"/uploads/images/Error analysis on beam search.png\" title='Error analysis on beam search' width=\"600\">\n</center>\n\n对结果将人工翻译和模型翻译对比,对比RNN模型出错率和集束搜索出错率,优化\n\n### Bleu 得分\n\n<center>\n    <img src=\"/uploads/images/Bleu score.png\" title='Bleu score' width=\"600\">\n</center>\n\n这个例子中$p_{1}=5/7   p_{2}=4/6$,最后计算Bleu会在不同n-gram上取平均,但这样会侧重较短语句,因此会加上一个 BP(brevity penalty) 的惩罚因子.这给了机器翻译领域一个单一实数评估指标.\n\n### Attention 模型\n\n<center>\n    <img src=\"/uploads/images/Attention.png\" title='Attention' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/Computing attention alpha.png\" title='Computing attention alpha' width=\"600\">\n</center>\n\n### 语音识别\n\n略,没看懂\n\n### 触发字检测\n\n把一个音频片段计算出它的声谱图特征得到特征向量\n\n## 参考链接\n* [网易云课堂](https://mooc.study.163.com/course/2001280004#/info) \n* [Coursera Deep Learning 专项课程](https://www.coursera.org/specializations/deep-learning) \n* [吴恩达《深度学习》系列课程笔记](https://github.com/bighuang624/Andrew-Ng-Deep-Learning-notes/blob/master/docs/README.md) ","source":"_posts/Sequence-Models.md","raw":"---\ntitle: 序列模型\ncategories:\n  - 深度学习\n  - NLP\nmathjax: true\ncomments: true\ntags:\n  - Coursera\n  - 深度学习\n  - 机器学习\n  - LSTM\n  - GRU\n  - RNN\n  - Attention\n  - Word Embedding\nabbrlink: 94c569ba\ndate: 2018-12-21 10:48:09\n---\n\ndeeplearning.ai的第五课:Sequence Models.讲解了如基本的RNN网络,基本的循环单元到GRU,LSTM,再到双向RNN,还有深层版的模型.常用词嵌入的特性,不同词嵌入训练方法,集束搜索和Attention模型.\n<!-- more -->\n\n## Recurrent Neural Networks\n### Notation\n$X^{(i)<t\\>}$表示第i个训练样本的第t个输入元素\n$Y^{(i)<t\\>}$表示第i个训练样本的第t个输出元素\n$T_{x}^{(i)}$表示第i个训练样本的输入序列长度\n$T_{y}^{(i)}$表示第i个训练样本的输出序列长度\n\n### Recurrent Neural Network Model\n标准神经网络的问题:\n1. 输入输出长度可能不一致\n2. 不能很好共享文本不同位置学习到的特征\n\n<center>\n    <img src=\"/uploads/images/RNN Forward Propagation.png\" title='RNN Forward Propagation' width=\"600\">\n</center>\n\n### Backpropagation through time\n\n<center>\n    <img src=\"/uploads/images/Forward propagation and backpropagation.png\" title='Forward propagation and backpropagation' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/rnn cell backprop.png\" title='rnn cell backprop' width=\"600\">\n</center>\n \n### Different types of RNNs\n1. 多输入多输出模型(翻译模型)\n2. 多输入单输出模型(情感分析)\n3. 单输入多输出模型(音乐生成)\n4. 单输出单输出模型(简单神经网络)\n\n<center>\n    <img src=\"/uploads/images/RNN architectures.png\" title='RNN architectures' width=\"600\">\n</center>\n\n### Language model and sequence generation\n\n<center>\n    <img src=\"/uploads/images/RNN model.png\" title='RNN model' width=\"600\">\n</center>\n\n### Sampling novel sequences\n\n<center>\n    <img src=\"/uploads/images/Sampling a sequence from a trained RNN.png\" title='Sampling a sequence from a trained RNN' width=\"600\">\n</center>\n\n### Vanishing gradients with RNNs\n反向传播因为同样的梯度消失的问题,后面层的输出误差很难影响前面层的计算.不管输出是什么,不管是对的,还是错的,这个区域都很难反向传播到序列的前面部分,也因此网络很难调整序列前面的计算.如果不管的话,RNN会不擅长处理长期依赖的问题.\n梯度爆炸很容易发现,因为参数会大到崩溃,你会看到很多NaN,或者不是数字的情况,这意味着你的网络计算出现了数值溢出.\n\n### Gated Recurrent Unit (GRU)\n\n<center>\n    <img src=\"/uploads/images/RNN unit.png\" title='RNN unit' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/RNNunit.png\" title='RNNunit' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/GRU(simplified).png\" title='GRU(simplified)' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/fullGRU.png\" title='fullGRU' width=\"600\">\n</center>\n\n当$\\Gamma_{u}$很接近0,可能是0.000001或者更小,这就不会有梯度消失的问题了.因为$\\Gamma_{u}$很接近0,这就是说$c^{t}$几乎就等于$c^{t-1}$,而且$c^{t}$的值也很好地被维持了,即使经过很多很多的时间步.这就是缓解梯度消失问题的关键,因此允许神经网络运行在非常庞大的依赖词上\n\n### Long Short Term Memory (LSTM)\n\n<center>\n    <img src=\"/uploads/images/LSTM.png\" title='LSTM' width=\"600\">\n</center>\n\n> 最后公式应该为$a^{t\\}=\\Gamma_{o}*tanh(c^{t})$\n\n红线显示了只要你正确地设置了遗忘门和更新门,LSTM是相当容易把$c^{<0\\>}$的值一直往下传递到右边,比如$c^{<3\\>} = c^{<0\\>}$.这就是为什么LSTM和GRU非常擅长于长时间记忆某个值,对于存在记忆细胞中的某个值.\n\n### Deep RNNs\n\n<center>\n    <img src=\"/uploads/images/deepRNN.png\" title='deepRNN' width=\"600\">\n</center>\n\n对于RNN来说,有三层就已经不少了,不像卷积神经网络一样有大量的隐含层.或者每一个上面堆叠循环层,然后换成一些深的层,这些层并不水平连接,只是一个深层的网络.基本单元可以是最简单的RNN模型,也可以是GRU单元或者LSTM单元,并且,你也可以构建深层的双向RNN网络.\n\n## 自然语言处理与词嵌入\n\n### 词汇表征\n\nOne-hot向量表征的一大缺点是把每个词孤立起来(内积均为0),稀疏,泛化能力不强.词嵌入(Word Embedding)则可以学习到俩个词语相似之处.\n\n<center>\n    <img src=\"/uploads/images/Word Embedding.png\" title='Word Embedding' width=\"600\">\n</center>\n\n### 使用词嵌入\n\n词嵌入迁移学习:\n1. 从大量文本中学习词嵌入(1-100B words) or 下载预训练好的词嵌入模型\n2. 用词嵌入模型迁移到新的只有少量标注训练集的任务中(100k words)\n3. 可选: 继续微调(finetune)词嵌入(通常是数据集2比较大)\n\n注:语言模型和机器翻译使用词嵌入较少,因为这俩者数据集都较大\n\n### 词嵌入的特性\n\n词嵌入的一个显著成果就是,可学习的类比关系的一般性.举个例子,它能学会man对于woman相当于boy对于girl,因为man和woman之间和boy和girl之间的向量差在gender(性别)这一维都是一样的。\n\n<center>\n    <img src=\"/uploads/images/Analogies using word vectors.png\" title='Analogies using word vectors' width=\"600\">\n</center>\n\n### 嵌入矩阵\n\n<center>\n    <img src=\"/uploads/images/Embending matrix.png\" title='Embending matrix' width=\"600\">\n</center>\n\n### 学习词嵌入\n\n<center>\n    <img src=\"/uploads/images/Neural language model.png\" title='Neural language model' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/Other context-target pairs.png\" title='Other context-target pairs' width=\"600\">\n</center>\n\n研究发现,如果你想建立一个语言模型,用目标词的前几个单词作为上下文是常见做法.但如果目标是学习词嵌入,那么用这些其他类型的上下文,也能得到很好的词嵌入。\n\n### Word2Vec\n\n句子:'I want a glass of orange juice to go along with my cereal.'\nSkip-Gram模型: 抽取上下文和目标词配对,构造一个监督学习问题.随机选一个词作为上下文词,比如选orange这个词,然后随机在一定词距内选另一个词,比如在上下文词前后5或10个词范围内选择目标词.\n\n<center>\n    <img src=\"/uploads/images/Word2Vec.png\" title='词嵌入的简化模型和神经网络' width=\"600\">\n</center>\n\n关键是个softmax单元.矩阵$E$会有很多参数,所以矩阵$E$有对应所有嵌入向量$e_{c}$的参数,softmax单元也有$\\theta_{t}$的参数.优化这些参数的损失函数,就会得到一个较好的嵌入向量集,这个就叫做Skip-Gram模型.它把一个像orange这样的词作为输入,并预测这个输入词从左数或从右数的某个词是什么词.\n\n算法首要的问题就是计算速度.在softmax模型中,每次需对词汇表中的所有词做求和计算.同论文提出的还有CBOW模型.\n\n### 负采样\n\n<center>\n    <img src=\"/uploads/images/Negative Sampling.png\" title='Negative Sampling' width=\"600\">\n</center>\n\n生成数据的方式是选择一个上下文词(orange),再选一个目标词(juice),这就是表的第一行,它给了一个正样本并给定标签为1.然后给定$K$次,用相同的上下文词,再从字典中选取随机的词(king,book,the,of)等,并标记0,这些就会成为负样本.如果从字典中随机选到的词,正好出现在了词距内,比如说在上下文词orange正负10个词之内也没太大关系.**算法就是要分辨这两种不同的采样方式,这就是如何生成训练集的方法.**\n\n小数据集的话,$K$从5到20比较好.如果数据集很大,$K$就选的小一点.\n\n模型基于逻辑回归模型,不同的是将一个sigmoid函数作用于$\\theta_{t}^{T}e_{c}$,参数和之前一样.这可看做二分类逻辑回归分类器,但并不是每次迭代都训练全部10,000个词,只训练其中的5个(部分选出的词K+1个)\n\n采样负样本方法:$P\\left( w_{i} \\right) = \\frac{f\\left( w_{i} \\right)^{\\frac{3}{4}}}{\\sum_{j = 1}^{10,000}{f\\left( w_{j} \\right)^{\\frac{3}{4}}}}$\n\n### GloVe 词向量\n\n<center>\n    <img src=\"/uploads/images/Glove Model.png\" title='Glove Model' width=\"600\">\n</center>\n\nGloVe算法做的就是使上下文和目标词关系开始明确化.$X_{ij}$是单词$i$在单词$j$上下文中出现的次数,那么这里$i$和$j$就和$t$和$c$的功能一样.若上下文指左右几个词,则会得出$X_{ij}$等于$X_{ji}$这个结论.其他时候大致相等.加权因子$f\\left(X_{ij}\\right)$就可以是一个函数,$X_{ij}$为0是为0(启发性方法见GloVe算法论文).$\\theta_{i}$和$e_{j}$是对称的,而不像之前了解的模型,$\\theta$和$e$功能不一样,因此最后结果可以取平均$e_{w}^{(final)}= \\frac{e_{w} +\\theta_{w}}{2}$.\n\nGloVe差距最小化处理\n$$\\text{mini}\\text{mize}\\sum_{i = 1}^{10,000}{\\sum_{j = 1}^{10,000}{f\\left( X_{ij} \\right)\\left( \\theta_{i}^{T}e_{j} + b_{i} + b_{j}^{'} - logX_{ij} \\right)^{2}}}$$\n\n两个单词之间有多少联系,$t$和$c$之间有多紧密,$i$和$j$之间联系程度如何,换句话说就是他们同时出现的频率是多少,这是由这个$X_{ij}$影响的.然后梯度下降来最小化\n\n<center>\n    <img src=\"/uploads/images/featurization view of word embeddings.png\" title='featurization view of word embeddings' width=\"600\">\n</center>\n\n$$\\left( A\\theta_{i} \\right)^{T}\\left( A^{- T}e_{j} \\right) = \\theta_{i}^{T}A^{T}A^{- T}e_{j} = \\theta_{i}^{T}e_{j}$$\n通过GloVe算法得到的(关系)特征表示可能是原特征的潜在的任意线性变换,最终还是能学习出解决类似问题的平行四边形映射.\n\n> Word2Vec,负采样,GloVe 词向量是三种学习词向量嵌入的方法.\n\n### 情绪分类\n\n情感分类一个最大的挑战就是可能标记的训练集没有那么多.对于情感分类任务来说,训练集大小从10,000到100,000个单词都很常见,甚至有时会小于10,000个单词,采用了词嵌入能够带来更好的效果,尤其是只有很小的训练集时.\n\n<center>\n    <img src=\"/uploads/images/Simple sentiment classification model.png\" title='Simple sentiment classification model' width=\"600\">\n</center>\n\n该算法实际上会把所有单词的意思给平均.问题就是没考虑词序.\"Completely lacking in good taste, good service, and good ambiance.\",忽略词序,仅仅把所有单词的词嵌入加起来或者平均下来,分类器很可能认为这是一个好的评论.\n\n<center>\n    <img src=\"/uploads/images/RNN sentiment classification.png\" title='RNN sentiment classification' width=\"600\">\n</center>\n\n### 词嵌入除偏\n\n根据训练模型所使用的文本,词嵌入能够反映出性别、种族、年龄、性取向等其他方面的偏见,如Man对应Computer Programmer,那么Woman会对应?输出是Homemaker.\n\n<center>\n    <img src=\"/uploads/images/bias in word embedding.png\" title='bias in word embedding' width=\"600\">\n</center>\n\n1. 偏差求平均\n2. 中和.对于那些定义不确切的词可以将其处理一下,避免偏见.如doctor和babysitter想使之在性别方面是中立的,而girl、boy定义本身就含有性别\n3. 均衡步.防止又引入其他偏差.\n\n论文作者训练一个分类器尝试解决哪些词是中立的.\n\n## 序列模型和注意力机制\n\n### 基础模型\n\n1. 机器翻译到语音识别:seq2seq模型(Encoder-Decoder结构)\n2. 集束搜索(Beam search)和注意力模型(Attention Model)\n3. 音频模型\n\n### 选择最可能的句子\n\n<center>\n    <img src=\"/uploads/images/Machine translation.png\" title='Machine translation' width=\"600\">\n</center>\n\n机器翻译模型可以看作是条件语言模型,因为语言模型总是全0输入,随机地生成句子,机器翻译模型需要找到最可能的翻译,提供不同的输入(Encoder),目的是选择使句子出现可能性最大(Decoder),选择方法如Beam search,为什么不用贪心每次选择概率最大的一个词呢?这并不是最佳选择.\n\n### 集束搜索\n\n<center>\n    <img src=\"/uploads/images/Beam search algorithm.png\" title='Beam search algorithm' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/Beam search(B=3).png\" title='Beam search(B=3)' width=\"600\">\n</center>\n\n当B=3时表示每次只考虑三个可能结果,B=1即为贪心\n\n1. 在第一次词位置选出最可能的三个单词$y^{<1\\>}$\n2. 在第一步基础上计算最可能的三个单词对$P(y^{<1\\>},y^{<2\\>}|x)$\n3. 继续增加下一个单词重复上述步骤\n\n### 改进集束搜索\n\n最大化 \n$P(y^{< 1 \\>}\\ldots y^{< T_{y}\\>}|X)$=$P(y^{<1\\>}|X)$\\*$P(y^{< 2 \\>}|X,y^{< 1 \\>})$\\*$P(y^{< 3 \\>}|X,y^{< 1\\ >},y^{< 2\\>})\\ldots$$P(y^{< T_{y}\\ >}|X,y^{<1\\ >}\\ldots y^{< T_{y} - 1\\ >})$\n\n1. 改成最大化$logP(y|x)$,能防止数值下溢\n2. 原公式倾向于长度短小的翻译结果,因此可以长度归一化(除$T_{y}$)\n\n### 定向搜索的误差分析\n\n<center>\n    <img src=\"/uploads/images/Error analysis on beam search.png\" title='Error analysis on beam search' width=\"600\">\n</center>\n\n对结果将人工翻译和模型翻译对比,对比RNN模型出错率和集束搜索出错率,优化\n\n### Bleu 得分\n\n<center>\n    <img src=\"/uploads/images/Bleu score.png\" title='Bleu score' width=\"600\">\n</center>\n\n这个例子中$p_{1}=5/7   p_{2}=4/6$,最后计算Bleu会在不同n-gram上取平均,但这样会侧重较短语句,因此会加上一个 BP(brevity penalty) 的惩罚因子.这给了机器翻译领域一个单一实数评估指标.\n\n### Attention 模型\n\n<center>\n    <img src=\"/uploads/images/Attention.png\" title='Attention' width=\"600\">\n</center>\n\n<center>\n    <img src=\"/uploads/images/Computing attention alpha.png\" title='Computing attention alpha' width=\"600\">\n</center>\n\n### 语音识别\n\n略,没看懂\n\n### 触发字检测\n\n把一个音频片段计算出它的声谱图特征得到特征向量\n\n## 参考链接\n* [网易云课堂](https://mooc.study.163.com/course/2001280004#/info) \n* [Coursera Deep Learning 专项课程](https://www.coursera.org/specializations/deep-learning) \n* [吴恩达《深度学习》系列课程笔记](https://github.com/bighuang624/Andrew-Ng-Deep-Learning-notes/blob/master/docs/README.md) ","slug":"Sequence-Models","published":1,"updated":"2019-03-19T14:14:08.682Z","layout":"post","photos":[],"link":"","_id":"cjtfwr9ot000z49zfbm0uts5m","content":"<p>deeplearning.ai的第五课:Sequence Models.讲解了如基本的RNN网络,基本的循环单元到GRU,LSTM,再到双向RNN,还有深层版的模型.常用词嵌入的特性,不同词嵌入训练方法,集束搜索和Attention模型.<br><a id=\"more\"></a></p><h2 id=\"Recurrent-Neural-Networks\"><a href=\"#Recurrent-Neural-Networks\" class=\"headerlink\" title=\"Recurrent Neural Networks\"></a>Recurrent Neural Networks</h2><h3 id=\"Notation\"><a href=\"#Notation\" class=\"headerlink\" title=\"Notation\"></a>Notation</h3><p>$X^{(i)&lt;t>}$表示第i个训练样本的第t个输入元素<br>$Y^{(i)&lt;t>}$表示第i个训练样本的第t个输出元素<br>$T_{x}^{(i)}$表示第i个训练样本的输入序列长度<br>$T_{y}^{(i)}$表示第i个训练样本的输出序列长度</p><h3 id=\"Recurrent-Neural-Network-Model\"><a href=\"#Recurrent-Neural-Network-Model\" class=\"headerlink\" title=\"Recurrent Neural Network Model\"></a>Recurrent Neural Network Model</h3><p>标准神经网络的问题:</p><ol><li>输入输出长度可能不一致</li><li>不能很好共享文本不同位置学习到的特征</li></ol><center><br><img src=\"/uploads/images/RNN Forward Propagation.png\" title=\"RNN Forward Propagation\" width=\"600\"><br></center><h3 id=\"Backpropagation-through-time\"><a href=\"#Backpropagation-through-time\" class=\"headerlink\" title=\"Backpropagation through time\"></a>Backpropagation through time</h3><center><br><img src=\"/uploads/images/Forward propagation and backpropagation.png\" title=\"Forward propagation and backpropagation\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/rnn cell backprop.png\" title=\"rnn cell backprop\" width=\"600\"><br></center><h3 id=\"Different-types-of-RNNs\"><a href=\"#Different-types-of-RNNs\" class=\"headerlink\" title=\"Different types of RNNs\"></a>Different types of RNNs</h3><ol><li>多输入多输出模型(翻译模型)</li><li>多输入单输出模型(情感分析)</li><li>单输入多输出模型(音乐生成)</li><li>单输出单输出模型(简单神经网络)</li></ol><center><br><img src=\"/uploads/images/RNN architectures.png\" title=\"RNN architectures\" width=\"600\"><br></center><h3 id=\"Language-model-and-sequence-generation\"><a href=\"#Language-model-and-sequence-generation\" class=\"headerlink\" title=\"Language model and sequence generation\"></a>Language model and sequence generation</h3><center><br><img src=\"/uploads/images/RNN model.png\" title=\"RNN model\" width=\"600\"><br></center><h3 id=\"Sampling-novel-sequences\"><a href=\"#Sampling-novel-sequences\" class=\"headerlink\" title=\"Sampling novel sequences\"></a>Sampling novel sequences</h3><center><br><img src=\"/uploads/images/Sampling a sequence from a trained RNN.png\" title=\"Sampling a sequence from a trained RNN\" width=\"600\"><br></center><h3 id=\"Vanishing-gradients-with-RNNs\"><a href=\"#Vanishing-gradients-with-RNNs\" class=\"headerlink\" title=\"Vanishing gradients with RNNs\"></a>Vanishing gradients with RNNs</h3><p>反向传播因为同样的梯度消失的问题,后面层的输出误差很难影响前面层的计算.不管输出是什么,不管是对的,还是错的,这个区域都很难反向传播到序列的前面部分,也因此网络很难调整序列前面的计算.如果不管的话,RNN会不擅长处理长期依赖的问题.<br>梯度爆炸很容易发现,因为参数会大到崩溃,你会看到很多NaN,或者不是数字的情况,这意味着你的网络计算出现了数值溢出.</p><h3 id=\"Gated-Recurrent-Unit-GRU\"><a href=\"#Gated-Recurrent-Unit-GRU\" class=\"headerlink\" title=\"Gated Recurrent Unit (GRU)\"></a>Gated Recurrent Unit (GRU)</h3><center><br><img src=\"/uploads/images/RNN unit.png\" title=\"RNN unit\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/RNNunit.png\" title=\"RNNunit\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/GRU(simplified).png\" title=\"GRU(simplified)\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/fullGRU.png\" title=\"fullGRU\" width=\"600\"><br></center><p>当$\\Gamma_{u}$很接近0,可能是0.000001或者更小,这就不会有梯度消失的问题了.因为$\\Gamma_{u}$很接近0,这就是说$c^{t}$几乎就等于$c^{t-1}$,而且$c^{t}$的值也很好地被维持了,即使经过很多很多的时间步.这就是缓解梯度消失问题的关键,因此允许神经网络运行在非常庞大的依赖词上</p><h3 id=\"Long-Short-Term-Memory-LSTM\"><a href=\"#Long-Short-Term-Memory-LSTM\" class=\"headerlink\" title=\"Long Short Term Memory (LSTM)\"></a>Long Short Term Memory (LSTM)</h3><center><br><img src=\"/uploads/images/LSTM.png\" title=\"LSTM\" width=\"600\"><br></center><blockquote><p>最后公式应该为$a^{t}=\\Gamma_{o}*tanh(c^{t})$</p></blockquote><p>红线显示了只要你正确地设置了遗忘门和更新门,LSTM是相当容易把$c^{&lt;0>}$的值一直往下传递到右边,比如$c^{&lt;3>} = c^{&lt;0>}$.这就是为什么LSTM和GRU非常擅长于长时间记忆某个值,对于存在记忆细胞中的某个值.</p><h3 id=\"Deep-RNNs\"><a href=\"#Deep-RNNs\" class=\"headerlink\" title=\"Deep RNNs\"></a>Deep RNNs</h3><center><br><img src=\"/uploads/images/deepRNN.png\" title=\"deepRNN\" width=\"600\"><br></center><p>对于RNN来说,有三层就已经不少了,不像卷积神经网络一样有大量的隐含层.或者每一个上面堆叠循环层,然后换成一些深的层,这些层并不水平连接,只是一个深层的网络.基本单元可以是最简单的RNN模型,也可以是GRU单元或者LSTM单元,并且,你也可以构建深层的双向RNN网络.</p><h2 id=\"自然语言处理与词嵌入\"><a href=\"#自然语言处理与词嵌入\" class=\"headerlink\" title=\"自然语言处理与词嵌入\"></a>自然语言处理与词嵌入</h2><h3 id=\"词汇表征\"><a href=\"#词汇表征\" class=\"headerlink\" title=\"词汇表征\"></a>词汇表征</h3><p>One-hot向量表征的一大缺点是把每个词孤立起来(内积均为0),稀疏,泛化能力不强.词嵌入(Word Embedding)则可以学习到俩个词语相似之处.</p><center><br><img src=\"/uploads/images/Word Embedding.png\" title=\"Word Embedding\" width=\"600\"><br></center><h3 id=\"使用词嵌入\"><a href=\"#使用词嵌入\" class=\"headerlink\" title=\"使用词嵌入\"></a>使用词嵌入</h3><p>词嵌入迁移学习:</p><ol><li>从大量文本中学习词嵌入(1-100B words) or 下载预训练好的词嵌入模型</li><li>用词嵌入模型迁移到新的只有少量标注训练集的任务中(100k words)</li><li>可选: 继续微调(finetune)词嵌入(通常是数据集2比较大)</li></ol><p>注:语言模型和机器翻译使用词嵌入较少,因为这俩者数据集都较大</p><h3 id=\"词嵌入的特性\"><a href=\"#词嵌入的特性\" class=\"headerlink\" title=\"词嵌入的特性\"></a>词嵌入的特性</h3><p>词嵌入的一个显著成果就是,可学习的类比关系的一般性.举个例子,它能学会man对于woman相当于boy对于girl,因为man和woman之间和boy和girl之间的向量差在gender(性别)这一维都是一样的。</p><center><br><img src=\"/uploads/images/Analogies using word vectors.png\" title=\"Analogies using word vectors\" width=\"600\"><br></center><h3 id=\"嵌入矩阵\"><a href=\"#嵌入矩阵\" class=\"headerlink\" title=\"嵌入矩阵\"></a>嵌入矩阵</h3><center><br><img src=\"/uploads/images/Embending matrix.png\" title=\"Embending matrix\" width=\"600\"><br></center><h3 id=\"学习词嵌入\"><a href=\"#学习词嵌入\" class=\"headerlink\" title=\"学习词嵌入\"></a>学习词嵌入</h3><center><br><img src=\"/uploads/images/Neural language model.png\" title=\"Neural language model\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/Other context-target pairs.png\" title=\"Other context-target pairs\" width=\"600\"><br></center><p>研究发现,如果你想建立一个语言模型,用目标词的前几个单词作为上下文是常见做法.但如果目标是学习词嵌入,那么用这些其他类型的上下文,也能得到很好的词嵌入。</p><h3 id=\"Word2Vec\"><a href=\"#Word2Vec\" class=\"headerlink\" title=\"Word2Vec\"></a>Word2Vec</h3><p>句子:’I want a glass of orange juice to go along with my cereal.’<br>Skip-Gram模型: 抽取上下文和目标词配对,构造一个监督学习问题.随机选一个词作为上下文词,比如选orange这个词,然后随机在一定词距内选另一个词,比如在上下文词前后5或10个词范围内选择目标词.</p><center><br><img src=\"/uploads/images/Word2Vec.png\" title=\"词嵌入的简化模型和神经网络\" width=\"600\"><br></center><p>关键是个softmax单元.矩阵$E$会有很多参数,所以矩阵$E$有对应所有嵌入向量$e_{c}$的参数,softmax单元也有$\\theta_{t}$的参数.优化这些参数的损失函数,就会得到一个较好的嵌入向量集,这个就叫做Skip-Gram模型.它把一个像orange这样的词作为输入,并预测这个输入词从左数或从右数的某个词是什么词.</p><p>算法首要的问题就是计算速度.在softmax模型中,每次需对词汇表中的所有词做求和计算.同论文提出的还有CBOW模型.</p><h3 id=\"负采样\"><a href=\"#负采样\" class=\"headerlink\" title=\"负采样\"></a>负采样</h3><center><br><img src=\"/uploads/images/Negative Sampling.png\" title=\"Negative Sampling\" width=\"600\"><br></center><p>生成数据的方式是选择一个上下文词(orange),再选一个目标词(juice),这就是表的第一行,它给了一个正样本并给定标签为1.然后给定$K$次,用相同的上下文词,再从字典中选取随机的词(king,book,the,of)等,并标记0,这些就会成为负样本.如果从字典中随机选到的词,正好出现在了词距内,比如说在上下文词orange正负10个词之内也没太大关系.<strong>算法就是要分辨这两种不同的采样方式,这就是如何生成训练集的方法.</strong></p><p>小数据集的话,$K$从5到20比较好.如果数据集很大,$K$就选的小一点.</p><p>模型基于逻辑回归模型,不同的是将一个sigmoid函数作用于$\\theta_{t}^{T}e_{c}$,参数和之前一样.这可看做二分类逻辑回归分类器,但并不是每次迭代都训练全部10,000个词,只训练其中的5个(部分选出的词K+1个)</p><p>采样负样本方法:$P\\left( w_{i} \\right) = \\frac{f\\left( w_{i} \\right)^{\\frac{3}{4}}}{\\sum_{j = 1}^{10,000}{f\\left( w_{j} \\right)^{\\frac{3}{4}}}}$</p><h3 id=\"GloVe-词向量\"><a href=\"#GloVe-词向量\" class=\"headerlink\" title=\"GloVe 词向量\"></a>GloVe 词向量</h3><center><br><img src=\"/uploads/images/Glove Model.png\" title=\"Glove Model\" width=\"600\"><br></center><p>GloVe算法做的就是使上下文和目标词关系开始明确化.$X_{ij}$是单词$i$在单词$j$上下文中出现的次数,那么这里$i$和$j$就和$t$和$c$的功能一样.若上下文指左右几个词,则会得出$X_{ij}$等于$X_{ji}$这个结论.其他时候大致相等.加权因子$f\\left(X_{ij}\\right)$就可以是一个函数,$X_{ij}$为0是为0(启发性方法见GloVe算法论文).$\\theta_{i}$和$e_{j}$是对称的,而不像之前了解的模型,$\\theta$和$e$功能不一样,因此最后结果可以取平均$e_{w}^{(final)}= \\frac{e_{w} +\\theta_{w}}{2}$.</p><p>GloVe差距最小化处理<br>$$\\text{mini}\\text{mize}\\sum_{i = 1}^{10,000}{\\sum_{j = 1}^{10,000}{f\\left( X_{ij} \\right)\\left( \\theta_{i}^{T}e_{j} + b_{i} + b_{j}^{‘} - logX_{ij} \\right)^{2}}}$$</p><p>两个单词之间有多少联系,$t$和$c$之间有多紧密,$i$和$j$之间联系程度如何,换句话说就是他们同时出现的频率是多少,这是由这个$X_{ij}$影响的.然后梯度下降来最小化</p><center><br><img src=\"/uploads/images/featurization view of word embeddings.png\" title=\"featurization view of word embeddings\" width=\"600\"><br></center><p>$$\\left( A\\theta_{i} \\right)^{T}\\left( A^{- T}e_{j} \\right) = \\theta_{i}^{T}A^{T}A^{- T}e_{j} = \\theta_{i}^{T}e_{j}$$<br>通过GloVe算法得到的(关系)特征表示可能是原特征的潜在的任意线性变换,最终还是能学习出解决类似问题的平行四边形映射.</p><blockquote><p>Word2Vec,负采样,GloVe 词向量是三种学习词向量嵌入的方法.</p></blockquote><h3 id=\"情绪分类\"><a href=\"#情绪分类\" class=\"headerlink\" title=\"情绪分类\"></a>情绪分类</h3><p>情感分类一个最大的挑战就是可能标记的训练集没有那么多.对于情感分类任务来说,训练集大小从10,000到100,000个单词都很常见,甚至有时会小于10,000个单词,采用了词嵌入能够带来更好的效果,尤其是只有很小的训练集时.</p><center><br><img src=\"/uploads/images/Simple sentiment classification model.png\" title=\"Simple sentiment classification model\" width=\"600\"><br></center><p>该算法实际上会把所有单词的意思给平均.问题就是没考虑词序.”Completely lacking in good taste, good service, and good ambiance.”,忽略词序,仅仅把所有单词的词嵌入加起来或者平均下来,分类器很可能认为这是一个好的评论.</p><center><br><img src=\"/uploads/images/RNN sentiment classification.png\" title=\"RNN sentiment classification\" width=\"600\"><br></center><h3 id=\"词嵌入除偏\"><a href=\"#词嵌入除偏\" class=\"headerlink\" title=\"词嵌入除偏\"></a>词嵌入除偏</h3><p>根据训练模型所使用的文本,词嵌入能够反映出性别、种族、年龄、性取向等其他方面的偏见,如Man对应Computer Programmer,那么Woman会对应?输出是Homemaker.</p><center><br><img src=\"/uploads/images/bias in word embedding.png\" title=\"bias in word embedding\" width=\"600\"><br></center><ol><li>偏差求平均</li><li>中和.对于那些定义不确切的词可以将其处理一下,避免偏见.如doctor和babysitter想使之在性别方面是中立的,而girl、boy定义本身就含有性别</li><li>均衡步.防止又引入其他偏差.</li></ol><p>论文作者训练一个分类器尝试解决哪些词是中立的.</p><h2 id=\"序列模型和注意力机制\"><a href=\"#序列模型和注意力机制\" class=\"headerlink\" title=\"序列模型和注意力机制\"></a>序列模型和注意力机制</h2><h3 id=\"基础模型\"><a href=\"#基础模型\" class=\"headerlink\" title=\"基础模型\"></a>基础模型</h3><ol><li>机器翻译到语音识别:seq2seq模型(Encoder-Decoder结构)</li><li>集束搜索(Beam search)和注意力模型(Attention Model)</li><li>音频模型</li></ol><h3 id=\"选择最可能的句子\"><a href=\"#选择最可能的句子\" class=\"headerlink\" title=\"选择最可能的句子\"></a>选择最可能的句子</h3><center><br><img src=\"/uploads/images/Machine translation.png\" title=\"Machine translation\" width=\"600\"><br></center><p>机器翻译模型可以看作是条件语言模型,因为语言模型总是全0输入,随机地生成句子,机器翻译模型需要找到最可能的翻译,提供不同的输入(Encoder),目的是选择使句子出现可能性最大(Decoder),选择方法如Beam search,为什么不用贪心每次选择概率最大的一个词呢?这并不是最佳选择.</p><h3 id=\"集束搜索\"><a href=\"#集束搜索\" class=\"headerlink\" title=\"集束搜索\"></a>集束搜索</h3><center><br><img src=\"/uploads/images/Beam search algorithm.png\" title=\"Beam search algorithm\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/Beam search(B=3).png\" title=\"Beam search(B=3)\" width=\"600\"><br></center><p>当B=3时表示每次只考虑三个可能结果,B=1即为贪心</p><ol><li>在第一次词位置选出最可能的三个单词$y^{&lt;1>}$</li><li>在第一步基础上计算最可能的三个单词对$P(y^{&lt;1>},y^{&lt;2>}|x)$</li><li>继续增加下一个单词重复上述步骤</li></ol><h3 id=\"改进集束搜索\"><a href=\"#改进集束搜索\" class=\"headerlink\" title=\"改进集束搜索\"></a>改进集束搜索</h3><p>最大化<br>$P(y^{&lt; 1 >}\\ldots y^{&lt; T_{y}>}|X)$=$P(y^{&lt;1>}|X)$*$P(y^{&lt; 2 >}|X,y^{&lt; 1 >})$*$P(y^{&lt; 3 >}|X,y^{&lt; 1\\ &gt;},y^{&lt; 2>})\\ldots$$P(y^{&lt; T_{y}\\ &gt;}|X,y^{&lt;1\\ &gt;}\\ldots y^{&lt; T_{y} - 1\\ &gt;})$</p><ol><li>改成最大化$logP(y|x)$,能防止数值下溢</li><li>原公式倾向于长度短小的翻译结果,因此可以长度归一化(除$T_{y}$)</li></ol><h3 id=\"定向搜索的误差分析\"><a href=\"#定向搜索的误差分析\" class=\"headerlink\" title=\"定向搜索的误差分析\"></a>定向搜索的误差分析</h3><center><br><img src=\"/uploads/images/Error analysis on beam search.png\" title=\"Error analysis on beam search\" width=\"600\"><br></center><p>对结果将人工翻译和模型翻译对比,对比RNN模型出错率和集束搜索出错率,优化</p><h3 id=\"Bleu-得分\"><a href=\"#Bleu-得分\" class=\"headerlink\" title=\"Bleu 得分\"></a>Bleu 得分</h3><center><br><img src=\"/uploads/images/Bleu score.png\" title=\"Bleu score\" width=\"600\"><br></center><p>这个例子中$p_{1}=5/7 p_{2}=4/6$,最后计算Bleu会在不同n-gram上取平均,但这样会侧重较短语句,因此会加上一个 BP(brevity penalty) 的惩罚因子.这给了机器翻译领域一个单一实数评估指标.</p><h3 id=\"Attention-模型\"><a href=\"#Attention-模型\" class=\"headerlink\" title=\"Attention 模型\"></a>Attention 模型</h3><center><br><img src=\"/uploads/images/Attention.png\" title=\"Attention\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/Computing attention alpha.png\" title=\"Computing attention alpha\" width=\"600\"><br></center><h3 id=\"语音识别\"><a href=\"#语音识别\" class=\"headerlink\" title=\"语音识别\"></a>语音识别</h3><p>略,没看懂</p><h3 id=\"触发字检测\"><a href=\"#触发字检测\" class=\"headerlink\" title=\"触发字检测\"></a>触发字检测</h3><p>把一个音频片段计算出它的声谱图特征得到特征向量</p><h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><ul><li><a href=\"https://mooc.study.163.com/course/2001280004#/info\" target=\"_blank\" rel=\"noopener\">网易云课堂</a></li><li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Coursera Deep Learning 专项课程</a></li><li><a href=\"https://github.com/bighuang624/Andrew-Ng-Deep-Learning-notes/blob/master/docs/README.md\" target=\"_blank\" rel=\"noopener\">吴恩达《深度学习》系列课程笔记</a></li></ul>","site":{"data":{}},"excerpt":"<p>deeplearning.ai的第五课:Sequence Models.讲解了如基本的RNN网络,基本的循环单元到GRU,LSTM,再到双向RNN,还有深层版的模型.常用词嵌入的特性,不同词嵌入训练方法,集束搜索和Attention模型.<br>","more":"</p><h2 id=\"Recurrent-Neural-Networks\"><a href=\"#Recurrent-Neural-Networks\" class=\"headerlink\" title=\"Recurrent Neural Networks\"></a>Recurrent Neural Networks</h2><h3 id=\"Notation\"><a href=\"#Notation\" class=\"headerlink\" title=\"Notation\"></a>Notation</h3><p>$X^{(i)&lt;t>}$表示第i个训练样本的第t个输入元素<br>$Y^{(i)&lt;t>}$表示第i个训练样本的第t个输出元素<br>$T_{x}^{(i)}$表示第i个训练样本的输入序列长度<br>$T_{y}^{(i)}$表示第i个训练样本的输出序列长度</p><h3 id=\"Recurrent-Neural-Network-Model\"><a href=\"#Recurrent-Neural-Network-Model\" class=\"headerlink\" title=\"Recurrent Neural Network Model\"></a>Recurrent Neural Network Model</h3><p>标准神经网络的问题:</p><ol><li>输入输出长度可能不一致</li><li>不能很好共享文本不同位置学习到的特征</li></ol><center><br><img src=\"/uploads/images/RNN Forward Propagation.png\" title=\"RNN Forward Propagation\" width=\"600\"><br></center><h3 id=\"Backpropagation-through-time\"><a href=\"#Backpropagation-through-time\" class=\"headerlink\" title=\"Backpropagation through time\"></a>Backpropagation through time</h3><center><br><img src=\"/uploads/images/Forward propagation and backpropagation.png\" title=\"Forward propagation and backpropagation\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/rnn cell backprop.png\" title=\"rnn cell backprop\" width=\"600\"><br></center><h3 id=\"Different-types-of-RNNs\"><a href=\"#Different-types-of-RNNs\" class=\"headerlink\" title=\"Different types of RNNs\"></a>Different types of RNNs</h3><ol><li>多输入多输出模型(翻译模型)</li><li>多输入单输出模型(情感分析)</li><li>单输入多输出模型(音乐生成)</li><li>单输出单输出模型(简单神经网络)</li></ol><center><br><img src=\"/uploads/images/RNN architectures.png\" title=\"RNN architectures\" width=\"600\"><br></center><h3 id=\"Language-model-and-sequence-generation\"><a href=\"#Language-model-and-sequence-generation\" class=\"headerlink\" title=\"Language model and sequence generation\"></a>Language model and sequence generation</h3><center><br><img src=\"/uploads/images/RNN model.png\" title=\"RNN model\" width=\"600\"><br></center><h3 id=\"Sampling-novel-sequences\"><a href=\"#Sampling-novel-sequences\" class=\"headerlink\" title=\"Sampling novel sequences\"></a>Sampling novel sequences</h3><center><br><img src=\"/uploads/images/Sampling a sequence from a trained RNN.png\" title=\"Sampling a sequence from a trained RNN\" width=\"600\"><br></center><h3 id=\"Vanishing-gradients-with-RNNs\"><a href=\"#Vanishing-gradients-with-RNNs\" class=\"headerlink\" title=\"Vanishing gradients with RNNs\"></a>Vanishing gradients with RNNs</h3><p>反向传播因为同样的梯度消失的问题,后面层的输出误差很难影响前面层的计算.不管输出是什么,不管是对的,还是错的,这个区域都很难反向传播到序列的前面部分,也因此网络很难调整序列前面的计算.如果不管的话,RNN会不擅长处理长期依赖的问题.<br>梯度爆炸很容易发现,因为参数会大到崩溃,你会看到很多NaN,或者不是数字的情况,这意味着你的网络计算出现了数值溢出.</p><h3 id=\"Gated-Recurrent-Unit-GRU\"><a href=\"#Gated-Recurrent-Unit-GRU\" class=\"headerlink\" title=\"Gated Recurrent Unit (GRU)\"></a>Gated Recurrent Unit (GRU)</h3><center><br><img src=\"/uploads/images/RNN unit.png\" title=\"RNN unit\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/RNNunit.png\" title=\"RNNunit\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/GRU(simplified).png\" title=\"GRU(simplified)\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/fullGRU.png\" title=\"fullGRU\" width=\"600\"><br></center><p>当$\\Gamma_{u}$很接近0,可能是0.000001或者更小,这就不会有梯度消失的问题了.因为$\\Gamma_{u}$很接近0,这就是说$c^{t}$几乎就等于$c^{t-1}$,而且$c^{t}$的值也很好地被维持了,即使经过很多很多的时间步.这就是缓解梯度消失问题的关键,因此允许神经网络运行在非常庞大的依赖词上</p><h3 id=\"Long-Short-Term-Memory-LSTM\"><a href=\"#Long-Short-Term-Memory-LSTM\" class=\"headerlink\" title=\"Long Short Term Memory (LSTM)\"></a>Long Short Term Memory (LSTM)</h3><center><br><img src=\"/uploads/images/LSTM.png\" title=\"LSTM\" width=\"600\"><br></center><blockquote><p>最后公式应该为$a^{t}=\\Gamma_{o}*tanh(c^{t})$</p></blockquote><p>红线显示了只要你正确地设置了遗忘门和更新门,LSTM是相当容易把$c^{&lt;0>}$的值一直往下传递到右边,比如$c^{&lt;3>} = c^{&lt;0>}$.这就是为什么LSTM和GRU非常擅长于长时间记忆某个值,对于存在记忆细胞中的某个值.</p><h3 id=\"Deep-RNNs\"><a href=\"#Deep-RNNs\" class=\"headerlink\" title=\"Deep RNNs\"></a>Deep RNNs</h3><center><br><img src=\"/uploads/images/deepRNN.png\" title=\"deepRNN\" width=\"600\"><br></center><p>对于RNN来说,有三层就已经不少了,不像卷积神经网络一样有大量的隐含层.或者每一个上面堆叠循环层,然后换成一些深的层,这些层并不水平连接,只是一个深层的网络.基本单元可以是最简单的RNN模型,也可以是GRU单元或者LSTM单元,并且,你也可以构建深层的双向RNN网络.</p><h2 id=\"自然语言处理与词嵌入\"><a href=\"#自然语言处理与词嵌入\" class=\"headerlink\" title=\"自然语言处理与词嵌入\"></a>自然语言处理与词嵌入</h2><h3 id=\"词汇表征\"><a href=\"#词汇表征\" class=\"headerlink\" title=\"词汇表征\"></a>词汇表征</h3><p>One-hot向量表征的一大缺点是把每个词孤立起来(内积均为0),稀疏,泛化能力不强.词嵌入(Word Embedding)则可以学习到俩个词语相似之处.</p><center><br><img src=\"/uploads/images/Word Embedding.png\" title=\"Word Embedding\" width=\"600\"><br></center><h3 id=\"使用词嵌入\"><a href=\"#使用词嵌入\" class=\"headerlink\" title=\"使用词嵌入\"></a>使用词嵌入</h3><p>词嵌入迁移学习:</p><ol><li>从大量文本中学习词嵌入(1-100B words) or 下载预训练好的词嵌入模型</li><li>用词嵌入模型迁移到新的只有少量标注训练集的任务中(100k words)</li><li>可选: 继续微调(finetune)词嵌入(通常是数据集2比较大)</li></ol><p>注:语言模型和机器翻译使用词嵌入较少,因为这俩者数据集都较大</p><h3 id=\"词嵌入的特性\"><a href=\"#词嵌入的特性\" class=\"headerlink\" title=\"词嵌入的特性\"></a>词嵌入的特性</h3><p>词嵌入的一个显著成果就是,可学习的类比关系的一般性.举个例子,它能学会man对于woman相当于boy对于girl,因为man和woman之间和boy和girl之间的向量差在gender(性别)这一维都是一样的。</p><center><br><img src=\"/uploads/images/Analogies using word vectors.png\" title=\"Analogies using word vectors\" width=\"600\"><br></center><h3 id=\"嵌入矩阵\"><a href=\"#嵌入矩阵\" class=\"headerlink\" title=\"嵌入矩阵\"></a>嵌入矩阵</h3><center><br><img src=\"/uploads/images/Embending matrix.png\" title=\"Embending matrix\" width=\"600\"><br></center><h3 id=\"学习词嵌入\"><a href=\"#学习词嵌入\" class=\"headerlink\" title=\"学习词嵌入\"></a>学习词嵌入</h3><center><br><img src=\"/uploads/images/Neural language model.png\" title=\"Neural language model\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/Other context-target pairs.png\" title=\"Other context-target pairs\" width=\"600\"><br></center><p>研究发现,如果你想建立一个语言模型,用目标词的前几个单词作为上下文是常见做法.但如果目标是学习词嵌入,那么用这些其他类型的上下文,也能得到很好的词嵌入。</p><h3 id=\"Word2Vec\"><a href=\"#Word2Vec\" class=\"headerlink\" title=\"Word2Vec\"></a>Word2Vec</h3><p>句子:’I want a glass of orange juice to go along with my cereal.’<br>Skip-Gram模型: 抽取上下文和目标词配对,构造一个监督学习问题.随机选一个词作为上下文词,比如选orange这个词,然后随机在一定词距内选另一个词,比如在上下文词前后5或10个词范围内选择目标词.</p><center><br><img src=\"/uploads/images/Word2Vec.png\" title=\"词嵌入的简化模型和神经网络\" width=\"600\"><br></center><p>关键是个softmax单元.矩阵$E$会有很多参数,所以矩阵$E$有对应所有嵌入向量$e_{c}$的参数,softmax单元也有$\\theta_{t}$的参数.优化这些参数的损失函数,就会得到一个较好的嵌入向量集,这个就叫做Skip-Gram模型.它把一个像orange这样的词作为输入,并预测这个输入词从左数或从右数的某个词是什么词.</p><p>算法首要的问题就是计算速度.在softmax模型中,每次需对词汇表中的所有词做求和计算.同论文提出的还有CBOW模型.</p><h3 id=\"负采样\"><a href=\"#负采样\" class=\"headerlink\" title=\"负采样\"></a>负采样</h3><center><br><img src=\"/uploads/images/Negative Sampling.png\" title=\"Negative Sampling\" width=\"600\"><br></center><p>生成数据的方式是选择一个上下文词(orange),再选一个目标词(juice),这就是表的第一行,它给了一个正样本并给定标签为1.然后给定$K$次,用相同的上下文词,再从字典中选取随机的词(king,book,the,of)等,并标记0,这些就会成为负样本.如果从字典中随机选到的词,正好出现在了词距内,比如说在上下文词orange正负10个词之内也没太大关系.<strong>算法就是要分辨这两种不同的采样方式,这就是如何生成训练集的方法.</strong></p><p>小数据集的话,$K$从5到20比较好.如果数据集很大,$K$就选的小一点.</p><p>模型基于逻辑回归模型,不同的是将一个sigmoid函数作用于$\\theta_{t}^{T}e_{c}$,参数和之前一样.这可看做二分类逻辑回归分类器,但并不是每次迭代都训练全部10,000个词,只训练其中的5个(部分选出的词K+1个)</p><p>采样负样本方法:$P\\left( w_{i} \\right) = \\frac{f\\left( w_{i} \\right)^{\\frac{3}{4}}}{\\sum_{j = 1}^{10,000}{f\\left( w_{j} \\right)^{\\frac{3}{4}}}}$</p><h3 id=\"GloVe-词向量\"><a href=\"#GloVe-词向量\" class=\"headerlink\" title=\"GloVe 词向量\"></a>GloVe 词向量</h3><center><br><img src=\"/uploads/images/Glove Model.png\" title=\"Glove Model\" width=\"600\"><br></center><p>GloVe算法做的就是使上下文和目标词关系开始明确化.$X_{ij}$是单词$i$在单词$j$上下文中出现的次数,那么这里$i$和$j$就和$t$和$c$的功能一样.若上下文指左右几个词,则会得出$X_{ij}$等于$X_{ji}$这个结论.其他时候大致相等.加权因子$f\\left(X_{ij}\\right)$就可以是一个函数,$X_{ij}$为0是为0(启发性方法见GloVe算法论文).$\\theta_{i}$和$e_{j}$是对称的,而不像之前了解的模型,$\\theta$和$e$功能不一样,因此最后结果可以取平均$e_{w}^{(final)}= \\frac{e_{w} +\\theta_{w}}{2}$.</p><p>GloVe差距最小化处理<br>$$\\text{mini}\\text{mize}\\sum_{i = 1}^{10,000}{\\sum_{j = 1}^{10,000}{f\\left( X_{ij} \\right)\\left( \\theta_{i}^{T}e_{j} + b_{i} + b_{j}^{‘} - logX_{ij} \\right)^{2}}}$$</p><p>两个单词之间有多少联系,$t$和$c$之间有多紧密,$i$和$j$之间联系程度如何,换句话说就是他们同时出现的频率是多少,这是由这个$X_{ij}$影响的.然后梯度下降来最小化</p><center><br><img src=\"/uploads/images/featurization view of word embeddings.png\" title=\"featurization view of word embeddings\" width=\"600\"><br></center><p>$$\\left( A\\theta_{i} \\right)^{T}\\left( A^{- T}e_{j} \\right) = \\theta_{i}^{T}A^{T}A^{- T}e_{j} = \\theta_{i}^{T}e_{j}$$<br>通过GloVe算法得到的(关系)特征表示可能是原特征的潜在的任意线性变换,最终还是能学习出解决类似问题的平行四边形映射.</p><blockquote><p>Word2Vec,负采样,GloVe 词向量是三种学习词向量嵌入的方法.</p></blockquote><h3 id=\"情绪分类\"><a href=\"#情绪分类\" class=\"headerlink\" title=\"情绪分类\"></a>情绪分类</h3><p>情感分类一个最大的挑战就是可能标记的训练集没有那么多.对于情感分类任务来说,训练集大小从10,000到100,000个单词都很常见,甚至有时会小于10,000个单词,采用了词嵌入能够带来更好的效果,尤其是只有很小的训练集时.</p><center><br><img src=\"/uploads/images/Simple sentiment classification model.png\" title=\"Simple sentiment classification model\" width=\"600\"><br></center><p>该算法实际上会把所有单词的意思给平均.问题就是没考虑词序.”Completely lacking in good taste, good service, and good ambiance.”,忽略词序,仅仅把所有单词的词嵌入加起来或者平均下来,分类器很可能认为这是一个好的评论.</p><center><br><img src=\"/uploads/images/RNN sentiment classification.png\" title=\"RNN sentiment classification\" width=\"600\"><br></center><h3 id=\"词嵌入除偏\"><a href=\"#词嵌入除偏\" class=\"headerlink\" title=\"词嵌入除偏\"></a>词嵌入除偏</h3><p>根据训练模型所使用的文本,词嵌入能够反映出性别、种族、年龄、性取向等其他方面的偏见,如Man对应Computer Programmer,那么Woman会对应?输出是Homemaker.</p><center><br><img src=\"/uploads/images/bias in word embedding.png\" title=\"bias in word embedding\" width=\"600\"><br></center><ol><li>偏差求平均</li><li>中和.对于那些定义不确切的词可以将其处理一下,避免偏见.如doctor和babysitter想使之在性别方面是中立的,而girl、boy定义本身就含有性别</li><li>均衡步.防止又引入其他偏差.</li></ol><p>论文作者训练一个分类器尝试解决哪些词是中立的.</p><h2 id=\"序列模型和注意力机制\"><a href=\"#序列模型和注意力机制\" class=\"headerlink\" title=\"序列模型和注意力机制\"></a>序列模型和注意力机制</h2><h3 id=\"基础模型\"><a href=\"#基础模型\" class=\"headerlink\" title=\"基础模型\"></a>基础模型</h3><ol><li>机器翻译到语音识别:seq2seq模型(Encoder-Decoder结构)</li><li>集束搜索(Beam search)和注意力模型(Attention Model)</li><li>音频模型</li></ol><h3 id=\"选择最可能的句子\"><a href=\"#选择最可能的句子\" class=\"headerlink\" title=\"选择最可能的句子\"></a>选择最可能的句子</h3><center><br><img src=\"/uploads/images/Machine translation.png\" title=\"Machine translation\" width=\"600\"><br></center><p>机器翻译模型可以看作是条件语言模型,因为语言模型总是全0输入,随机地生成句子,机器翻译模型需要找到最可能的翻译,提供不同的输入(Encoder),目的是选择使句子出现可能性最大(Decoder),选择方法如Beam search,为什么不用贪心每次选择概率最大的一个词呢?这并不是最佳选择.</p><h3 id=\"集束搜索\"><a href=\"#集束搜索\" class=\"headerlink\" title=\"集束搜索\"></a>集束搜索</h3><center><br><img src=\"/uploads/images/Beam search algorithm.png\" title=\"Beam search algorithm\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/Beam search(B=3).png\" title=\"Beam search(B=3)\" width=\"600\"><br></center><p>当B=3时表示每次只考虑三个可能结果,B=1即为贪心</p><ol><li>在第一次词位置选出最可能的三个单词$y^{&lt;1>}$</li><li>在第一步基础上计算最可能的三个单词对$P(y^{&lt;1>},y^{&lt;2>}|x)$</li><li>继续增加下一个单词重复上述步骤</li></ol><h3 id=\"改进集束搜索\"><a href=\"#改进集束搜索\" class=\"headerlink\" title=\"改进集束搜索\"></a>改进集束搜索</h3><p>最大化<br>$P(y^{&lt; 1 >}\\ldots y^{&lt; T_{y}>}|X)$=$P(y^{&lt;1>}|X)$*$P(y^{&lt; 2 >}|X,y^{&lt; 1 >})$*$P(y^{&lt; 3 >}|X,y^{&lt; 1\\ &gt;},y^{&lt; 2>})\\ldots$$P(y^{&lt; T_{y}\\ &gt;}|X,y^{&lt;1\\ &gt;}\\ldots y^{&lt; T_{y} - 1\\ &gt;})$</p><ol><li>改成最大化$logP(y|x)$,能防止数值下溢</li><li>原公式倾向于长度短小的翻译结果,因此可以长度归一化(除$T_{y}$)</li></ol><h3 id=\"定向搜索的误差分析\"><a href=\"#定向搜索的误差分析\" class=\"headerlink\" title=\"定向搜索的误差分析\"></a>定向搜索的误差分析</h3><center><br><img src=\"/uploads/images/Error analysis on beam search.png\" title=\"Error analysis on beam search\" width=\"600\"><br></center><p>对结果将人工翻译和模型翻译对比,对比RNN模型出错率和集束搜索出错率,优化</p><h3 id=\"Bleu-得分\"><a href=\"#Bleu-得分\" class=\"headerlink\" title=\"Bleu 得分\"></a>Bleu 得分</h3><center><br><img src=\"/uploads/images/Bleu score.png\" title=\"Bleu score\" width=\"600\"><br></center><p>这个例子中$p_{1}=5/7 p_{2}=4/6$,最后计算Bleu会在不同n-gram上取平均,但这样会侧重较短语句,因此会加上一个 BP(brevity penalty) 的惩罚因子.这给了机器翻译领域一个单一实数评估指标.</p><h3 id=\"Attention-模型\"><a href=\"#Attention-模型\" class=\"headerlink\" title=\"Attention 模型\"></a>Attention 模型</h3><center><br><img src=\"/uploads/images/Attention.png\" title=\"Attention\" width=\"600\"><br></center><center><br><img src=\"/uploads/images/Computing attention alpha.png\" title=\"Computing attention alpha\" width=\"600\"><br></center><h3 id=\"语音识别\"><a href=\"#语音识别\" class=\"headerlink\" title=\"语音识别\"></a>语音识别</h3><p>略,没看懂</p><h3 id=\"触发字检测\"><a href=\"#触发字检测\" class=\"headerlink\" title=\"触发字检测\"></a>触发字检测</h3><p>把一个音频片段计算出它的声谱图特征得到特征向量</p><h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><ul><li><a href=\"https://mooc.study.163.com/course/2001280004#/info\" target=\"_blank\" rel=\"noopener\">网易云课堂</a></li><li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Coursera Deep Learning 专项课程</a></li><li><a href=\"https://github.com/bighuang624/Andrew-Ng-Deep-Learning-notes/blob/master/docs/README.md\" target=\"_blank\" rel=\"noopener\">吴恩达《深度学习》系列课程笔记</a></li></ul>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjtfwr9hs000249zf9kti1urx","category_id":"cjtfwr9i2000849zfj54eczz7","_id":"cjtfwr9ia000d49zfm6uldxhd"},{"post_id":"cjtfwr9hz000649zfu14h29kp","category_id":"cjtfwr9hw000449zfrz0x17mw","_id":"cjtfwr9ib000f49zf6en8is6u"},{"post_id":"cjtfwr9hj000049zfj5ywgtis","category_id":"cjtfwr9hw000449zfrz0x17mw","_id":"cjtfwr9ic000h49zfs4miwfa1"},{"post_id":"cjtfwr9hj000049zfj5ywgtis","category_id":"cjtfwr9i9000c49zf5c54r7y8","_id":"cjtfwr9id000j49zf3a8y4dj4"},{"post_id":"cjtfwr9ot000z49zfbm0uts5m","category_id":"cjtfwr9hw000449zfrz0x17mw","_id":"cjtfwr9oy001349zfbc3izov8"},{"post_id":"cjtfwr9ot000z49zfbm0uts5m","category_id":"cjtfwr9i9000c49zf5c54r7y8","_id":"cjtfwr9oy001449zfrseq3nj0"},{"post_id":"cjtfwr9oq000y49zfj9vk7v7p","category_id":"cjtfwr9hw000449zfrz0x17mw","_id":"cjtfwr9oy001549zfa19utctq"},{"post_id":"cjtfwr9oq000y49zfj9vk7v7p","category_id":"cjtfwr9ow001149zfbcrvas7k","_id":"cjtfwr9oz001749zfiqqmyuqa"}],"PostTag":[{"post_id":"cjtfwr9hj000049zfj5ywgtis","tag_id":"cjtfwr9hy000549zf5etsuike","_id":"cjtfwr9id000k49zfheznxcyb"},{"post_id":"cjtfwr9hj000049zfj5ywgtis","tag_id":"cjtfwr9i3000949zfvew8itp1","_id":"cjtfwr9ie000l49zflc9tjquv"},{"post_id":"cjtfwr9hj000049zfj5ywgtis","tag_id":"cjtfwr9i6000b49zfi78it3ba","_id":"cjtfwr9ie000n49zfqdogirxw"},{"post_id":"cjtfwr9hj000049zfj5ywgtis","tag_id":"cjtfwr9ia000e49zfu6vs2syg","_id":"cjtfwr9if000o49zfe0noi84x"},{"post_id":"cjtfwr9hj000049zfj5ywgtis","tag_id":"cjtfwr9ib000g49zfmq62ga9i","_id":"cjtfwr9if000q49zfya92q950"},{"post_id":"cjtfwr9hs000249zf9kti1urx","tag_id":"cjtfwr9id000i49zfl7zgswwl","_id":"cjtfwr9ih000r49zfnobbcork"},{"post_id":"cjtfwr9hs000249zf9kti1urx","tag_id":"cjtfwr9ie000m49zfuwb75rh9","_id":"cjtfwr9ih000t49zftouu06qz"},{"post_id":"cjtfwr9hz000649zfu14h29kp","tag_id":"cjtfwr9if000p49zfqc6y23ko","_id":"cjtfwr9ij000v49zf4avbg976"},{"post_id":"cjtfwr9hz000649zfu14h29kp","tag_id":"cjtfwr9ia000e49zfu6vs2syg","_id":"cjtfwr9ij000w49zfofpn13pn"},{"post_id":"cjtfwr9hz000649zfu14h29kp","tag_id":"cjtfwr9ii000u49zff6he746o","_id":"cjtfwr9ij000x49zfsdd7qx0z"},{"post_id":"cjtfwr9oq000y49zfj9vk7v7p","tag_id":"cjtfwr9hy000549zf5etsuike","_id":"cjtfwr9oz001949zftmwtfpr4"},{"post_id":"cjtfwr9oq000y49zfj9vk7v7p","tag_id":"cjtfwr9ow001049zfdf4jjggb","_id":"cjtfwr9p0001a49zfo32u3kx3"},{"post_id":"cjtfwr9oq000y49zfj9vk7v7p","tag_id":"cjtfwr9ia000e49zfu6vs2syg","_id":"cjtfwr9p0001c49zf07wf1li7"},{"post_id":"cjtfwr9oq000y49zfj9vk7v7p","tag_id":"cjtfwr9ox001249zfvnqidc1d","_id":"cjtfwr9p1001d49zf778yjog6"},{"post_id":"cjtfwr9oq000y49zfj9vk7v7p","tag_id":"cjtfwr9oy001649zf8ywzribj","_id":"cjtfwr9p1001f49zf1glmdghj"},{"post_id":"cjtfwr9ot000z49zfbm0uts5m","tag_id":"cjtfwr9if000p49zfqc6y23ko","_id":"cjtfwr9p8001i49zfzuwg4wkb"},{"post_id":"cjtfwr9ot000z49zfbm0uts5m","tag_id":"cjtfwr9ia000e49zfu6vs2syg","_id":"cjtfwr9p8001j49zfrgrdq7ci"},{"post_id":"cjtfwr9ot000z49zfbm0uts5m","tag_id":"cjtfwr9ii000u49zff6he746o","_id":"cjtfwr9p8001k49zf67m20p13"},{"post_id":"cjtfwr9ot000z49zfbm0uts5m","tag_id":"cjtfwr9oz001849zfpqj83rsa","_id":"cjtfwr9p8001l49zfx0g5b0sw"},{"post_id":"cjtfwr9ot000z49zfbm0uts5m","tag_id":"cjtfwr9p0001b49zf01jy7vf2","_id":"cjtfwr9p9001m49zffhwww1k5"},{"post_id":"cjtfwr9ot000z49zfbm0uts5m","tag_id":"cjtfwr9p1001e49zfqg8r37nv","_id":"cjtfwr9p9001n49zfok9yx6ip"},{"post_id":"cjtfwr9ot000z49zfbm0uts5m","tag_id":"cjtfwr9p2001g49zf8qm7r9e2","_id":"cjtfwr9p9001o49zfhp3zsoc6"},{"post_id":"cjtfwr9ot000z49zfbm0uts5m","tag_id":"cjtfwr9p3001h49zf0iurgpwb","_id":"cjtfwr9p9001p49zfttl3iiks"}],"Tag":[{"name":"paper","_id":"cjtfwr9hy000549zf5etsuike"},{"name":"TextCNN","_id":"cjtfwr9i3000949zfvew8itp1"},{"name":"CharCNN","_id":"cjtfwr9i6000b49zfi78it3ba"},{"name":"深度学习","_id":"cjtfwr9ia000e49zfu6vs2syg"},{"name":"文本分类","_id":"cjtfwr9ib000g49zfmq62ga9i"},{"name":"Hexo","_id":"cjtfwr9id000i49zfl7zgswwl"},{"name":"NexT","_id":"cjtfwr9ie000m49zfuwb75rh9"},{"name":"Coursera","_id":"cjtfwr9if000p49zfqc6y23ko"},{"name":"机器学习","_id":"cjtfwr9ii000u49zff6he746o"},{"name":"CNN","_id":"cjtfwr9ow001049zfdf4jjggb"},{"name":"Capsule","_id":"cjtfwr9ox001249zfvnqidc1d"},{"name":"图像处理","_id":"cjtfwr9oy001649zf8ywzribj"},{"name":"LSTM","_id":"cjtfwr9oz001849zfpqj83rsa"},{"name":"GRU","_id":"cjtfwr9p0001b49zf01jy7vf2"},{"name":"RNN","_id":"cjtfwr9p1001e49zfqg8r37nv"},{"name":"Attention","_id":"cjtfwr9p2001g49zf8qm7r9e2"},{"name":"Word Embedding","_id":"cjtfwr9p3001h49zf0iurgpwb"}]}}