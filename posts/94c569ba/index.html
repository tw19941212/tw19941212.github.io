<!DOCTYPE html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="Coursera,深度学习,机器学习,LSTM,GRU,RNN,Attention,Word Embedding,"><meta name="description" content="deeplearning.ai的第五课:Sequence Models.讲解了如基本的RNN网络,基本的循环单元到GRU,LSTM,再到双向RNN,还有深层版的模型.常用词嵌入的特性,不同词嵌入训练方法,集束搜索和Attention模型."><meta name="keywords" content="Coursera,深度学习,机器学习,LSTM,GRU,RNN,Attention,Word Embedding"><meta property="og:type" content="article"><meta property="og:title" content="序列模型"><meta property="og:url" content="https://tw19941212.github.io/posts/94c569ba/index.html"><meta property="og:site_name" content="Let&#39;s Smile"><meta property="og:description" content="deeplearning.ai的第五课:Sequence Models.讲解了如基本的RNN网络,基本的循环单元到GRU,LSTM,再到双向RNN,还有深层版的模型.常用词嵌入的特性,不同词嵌入训练方法,集束搜索和Attention模型."><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/RNN Forward Propagation.png?imageView2/2/w/600"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Forward propagation and backpropagation.png?imageView2/2/w/600"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/rnn cell backprop.png?imageView2/2/w/600"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/RNN architectures.png?imageView2/2/w/600"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/RNN model.png?imageView2/2/w/600"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Sampling a sequence from a trained RNN.png?imageView2/2/w/600"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/RNN unit.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/RNNunit.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/GRU(simplified).png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/fullGRU.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/LSTM.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/deepRNN.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Word Embedding.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Analogies using word vectors.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Embending matrix.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Neeural language model.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Other context-target pairs.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Word2Vec.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Negative Sampling.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Glove Model.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/featurization view of word embeddings.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Simple sentiment classification model.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/RNN sentiment classification.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/bias in word embedding.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Machine translation.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Beam search algorithm.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Beam search(B=3).png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Error analysis on beam search.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Bleu score.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Attention.png"><meta property="og:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/Computing attention alpha.png"><meta property="og:updated_time" content="2019-01-07T12:27:09.746Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="序列模型"><meta name="twitter:description" content="deeplearning.ai的第五课:Sequence Models.讲解了如基本的RNN网络,基本的循环单元到GRU,LSTM,再到双向RNN,还有深层版的模型.常用词嵌入的特性,不同词嵌入训练方法,集束搜索和Attention模型."><meta name="twitter:image" content="http://pjvkppqit.bkt.clouddn.com/static/images/RNN Forward Propagation.png?imageView2/2/w/600"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"U2IG18192U",apiKey:"e7525f7ed6c0cbbbea577cc1d18edc7e",indexName:"blog",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://tw19941212.github.io/posts/94c569ba/"><title>序列模型 | Let's Smile</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><a href="https://github.com/tw19941212" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513;color:#fff;position:absolute;top:0;border:0;right:0" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Let's Smile</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">缘,妙不可言</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-top10"><a href="/top10/" rel="section"><i class="menu-item-icon fa fa-fw fa-signal"></i><br>top10</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="algolia-popup popup search-popup"><div class="algolia-search"><div class="algolia-search-input-icon"><i class="fa fa-search"></i></div><div class="algolia-search-input" id="algolia-search-input"></div></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://tw19941212.github.io/posts/94c569ba/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="唐朝"><meta itemprop="description" content=""><meta itemprop="image" content="http://pjvkppqit.bkt.clouddn.com/static/images/avatar.jpeg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Let's Smile"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">序列模型</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-21T10:48:09+08:00">2018-12-21 </time></span><span class="post-updated">&nbsp; | &nbsp; 更新于 <time itemprop="dateUpdated" datetime="2019-01-07T20:27:09+08:00" content="2019-01-07">2019-01-07 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span> </a></span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度学习/RNN/" itemprop="url" rel="index"><span itemprop="name">RNN</span> </a></span></span><span id="/posts/94c569ba/" class="leancloud_visitors" data-flag-title="序列模型"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">热度&#58;</span> <span class="leancloud-visitors-count"></span>℃</span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">2.9k</span></div></div></header><div class="post-body" itemprop="articleBody"><p>deeplearning.ai的第五课:Sequence Models.讲解了如基本的RNN网络,基本的循环单元到GRU,LSTM,再到双向RNN,还有深层版的模型.常用词嵌入的特性,不同词嵌入训练方法,集束搜索和Attention模型.<br><a id="more"></a></p><h2 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h2><h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><p>$X^{(i)&lt;t>}$表示第i个训练样本的第t个输入元素<br>$Y^{(i)&lt;t>}$表示第i个训练样本的第t个输出元素<br>$T_{x}^{(i)}$表示第i个训练样本的输入序列长度<br>$T_{y}^{(i)}$表示第i个训练样本的输出序列长度</p><h3 id="Recurrent-Neural-Network-Model"><a href="#Recurrent-Neural-Network-Model" class="headerlink" title="Recurrent Neural Network Model"></a>Recurrent Neural Network Model</h3><p>标准神经网络的问题:</p><ol><li>输入输出长度可能不一致</li><li>不能很好共享文本不同位置学习到的特征</li></ol><img title="RNN Forward Propagation" src="http://pjvkppqit.bkt.clouddn.com/static/images/RNN Forward Propagation.png?imageView2/2/w/600"><h3 id="Backpropagation-through-time"><a href="#Backpropagation-through-time" class="headerlink" title="Backpropagation through time"></a>Backpropagation through time</h3><img title="Forward propagation and backpropagation" src="http://pjvkppqit.bkt.clouddn.com/static/images/Forward propagation and backpropagation.png?imageView2/2/w/600"> <img title="rnn cell backprop" src="http://pjvkppqit.bkt.clouddn.com/static/images/rnn cell backprop.png?imageView2/2/w/600"><h3 id="Different-types-of-RNNs"><a href="#Different-types-of-RNNs" class="headerlink" title="Different types of RNNs"></a>Different types of RNNs</h3><ol><li>多输入多输出模型(翻译模型)</li><li>多输入单输出模型(情感分析)</li><li>单输入多输出模型(音乐生成)</li><li>单输出单输出模型(简单神经网络)</li></ol><img title="RNN architectures" src="http://pjvkppqit.bkt.clouddn.com/static/images/RNN architectures.png?imageView2/2/w/600"><h3 id="Language-model-and-sequence-generation"><a href="#Language-model-and-sequence-generation" class="headerlink" title="Language model and sequence generation"></a>Language model and sequence generation</h3><img title="RNN model" src="http://pjvkppqit.bkt.clouddn.com/static/images/RNN model.png?imageView2/2/w/600"><h3 id="Sampling-novel-sequences"><a href="#Sampling-novel-sequences" class="headerlink" title="Sampling novel sequences"></a>Sampling novel sequences</h3><img title="Sampling a sequence from a trained RNN" src="http://pjvkppqit.bkt.clouddn.com/static/images/Sampling a sequence from a trained RNN.png?imageView2/2/w/600"><h3 id="Vanishing-gradients-with-RNNs"><a href="#Vanishing-gradients-with-RNNs" class="headerlink" title="Vanishing gradients with RNNs"></a>Vanishing gradients with RNNs</h3><p>反向传播因为同样的梯度消失的问题,后面层的输出误差很难影响前面层的计算.不管输出是什么,不管是对的,还是错的,这个区域都很难反向传播到序列的前面部分,也因此网络很难调整序列前面的计算.如果不管的话,RNN会不擅长处理长期依赖的问题.<br>梯度爆炸很容易发现,因为参数会大到崩溃,你会看到很多NaN,或者不是数字的情况,这意味着你的网络计算出现了数值溢出.</p><h3 id="Gated-Recurrent-Unit-GRU"><a href="#Gated-Recurrent-Unit-GRU" class="headerlink" title="Gated Recurrent Unit (GRU)"></a>Gated Recurrent Unit (GRU)</h3><img title="RNN unit" src="http://pjvkppqit.bkt.clouddn.com/static/images/RNN unit.png"> <img title="RNNunit" src="http://pjvkppqit.bkt.clouddn.com/static/images/RNNunit.png"> <img title="GRU(simplified)" src="http://pjvkppqit.bkt.clouddn.com/static/images/GRU(simplified).png"> <img title="fullGRU" src="http://pjvkppqit.bkt.clouddn.com/static/images/fullGRU.png"><p>当$\Gamma_{u}$很接近0,可能是0.000001或者更小,这就不会有梯度消失的问题了.因为$\Gamma_{u}$很接近0,这就是说$c^{t}$几乎就等于$c^{t-1}$,而且$c^{t}$的值也很好地被维持了,即使经过很多很多的时间步.这就是缓解梯度消失问题的关键,因此允许神经网络运行在非常庞大的依赖词上</p><h3 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long Short Term Memory (LSTM)"></a>Long Short Term Memory (LSTM)</h3><img title="LSTM" src="http://pjvkppqit.bkt.clouddn.com/static/images/LSTM.png"><blockquote><p>最后公式应该为$a^{t}=\Gamma_{o}*tanh(c^{t})$</p></blockquote><p>红线显示了只要你正确地设置了遗忘门和更新门,LSTM是相当容易把$c^{&lt;0>}$的值一直往下传递到右边,比如$c^{&lt;3>} = c^{&lt;0>}$.这就是为什么LSTM和GRU非常擅长于长时间记忆某个值,对于存在记忆细胞中的某个值.</p><h3 id="Deep-RNNs"><a href="#Deep-RNNs" class="headerlink" title="Deep RNNs"></a>Deep RNNs</h3><img title="deepRNN" src="http://pjvkppqit.bkt.clouddn.com/static/images/deepRNN.png"><p>对于RNN来说,有三层就已经不少了,不像卷积神经网络一样有大量的隐含层.或者每一个上面堆叠循环层,然后换成一些深的层,这些层并不水平连接,只是一个深层的网络.基本单元可以是最简单的RNN模型,也可以是GRU单元或者LSTM单元,并且,你也可以构建深层的双向RNN网络.</p><h2 id="自然语言处理与词嵌入"><a href="#自然语言处理与词嵌入" class="headerlink" title="自然语言处理与词嵌入"></a>自然语言处理与词嵌入</h2><h3 id="词汇表征"><a href="#词汇表征" class="headerlink" title="词汇表征"></a>词汇表征</h3><p>One-hot向量表征的一大缺点是把每个词孤立起来(内积均为0),稀疏,泛化能力不强.词嵌入(Word Embedding)则可以学习到俩个词语相似之处.<br><img title="Word Embedding" src="http://pjvkppqit.bkt.clouddn.com/static/images/Word Embedding.png"></p><h3 id="使用词嵌入"><a href="#使用词嵌入" class="headerlink" title="使用词嵌入"></a>使用词嵌入</h3><p>词嵌入迁移学习:</p><ol><li>从大量文本中学习词嵌入(1-100B words) or 下载预训练好的词嵌入模型</li><li>用词嵌入模型迁移到新的只有少量标注训练集的任务中(100k words)</li><li>可选: 继续微调(finetune)词嵌入(通常是数据集2比较大)</li></ol><p>注:语言模型和机器翻译使用词嵌入较少,因为这俩者数据集都较大</p><h3 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h3><p>词嵌入的一个显著成果就是,可学习的类比关系的一般性.举个例子,它能学会man对于woman相当于boy对于girl,因为man和woman之间和boy和girl之间的向量差在gender(性别)这一维都是一样的。<br><img title="Analogies using word vectors" src="http://pjvkppqit.bkt.clouddn.com/static/images/Analogies using word vectors.png"></p><h3 id="嵌入矩阵"><a href="#嵌入矩阵" class="headerlink" title="嵌入矩阵"></a>嵌入矩阵</h3><img title="Embending matrix" src="http://pjvkppqit.bkt.clouddn.com/static/images/Embending matrix.png"><h3 id="学习词嵌入"><a href="#学习词嵌入" class="headerlink" title="学习词嵌入"></a>学习词嵌入</h3><img title="Neeural language model" src="http://pjvkppqit.bkt.clouddn.com/static/images/Neeural language model.png"> <img title="Other context-target pairs" src="http://pjvkppqit.bkt.clouddn.com/static/images/Other context-target pairs.png"><p>研究发现,如果你想建立一个语言模型,用目标词的前几个单词作为上下文是常见做法.但如果目标是学习词嵌入,那么用这些其他类型的上下文,也能得到很好的词嵌入。</p><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>句子:’I want a glass of orange juice to go along with my cereal.’<br>Skip-Gram模型: 抽取上下文和目标词配对,构造一个监督学习问题.随机选一个词作为上下文词,比如选orange这个词,然后随机在一定词距内选另一个词,比如在上下文词前后5或10个词范围内选择目标词.</p><img title="词嵌入的简化模型和神经网络" src="http://pjvkppqit.bkt.clouddn.com/static/images/Word2Vec.png"><p>关键是个softmax单元.矩阵$E$会有很多参数,所以矩阵$E$有对应所有嵌入向量$e_{c}$的参数,softmax单元也有$\theta_{t}$的参数.优化这些参数的损失函数,就会得到一个较好的嵌入向量集,这个就叫做Skip-Gram模型.它把一个像orange这样的词作为输入,并预测这个输入词从左数或从右数的某个词是什么词.</p><p>算法首要的问题就是计算速度.在softmax模型中,每次需对词汇表中的所有词做求和计算.同论文提出的还有CBOW模型.</p><h3 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h3><img title="Negative Sampling" src="http://pjvkppqit.bkt.clouddn.com/static/images/Negative Sampling.png"><p>生成数据的方式是选择一个上下文词(orange),再选一个目标词(juice),这就是表的第一行,它给了一个正样本并给定标签为1.然后给定$K$次,用相同的上下文词,再从字典中选取随机的词(king,book,the,of)等,并标记0,这些就会成为负样本.如果从字典中随机选到的词,正好出现在了词距内,比如说在上下文词orange正负10个词之内也没太大关系.<strong>算法就是要分辨这两种不同的采样方式,这就是如何生成训练集的方法.</strong></p><p>小数据集的话,$K$从5到20比较好.如果数据集很大,$K$就选的小一点.</p><p>模型基于逻辑回归模型,不同的是将一个sigmoid函数作用于$\theta_{t}^{T}e_{c}$,参数和之前一样.这可看做二分类逻辑回归分类器,但并不是每次迭代都训练全部10,000个词,只训练其中的5个(部分选出的词K+1个)</p><p>采样负样本方法:$P\left( w_{i} \right) = \frac{f\left( w_{i} \right)^{\frac{3}{4}}}{\sum_{j = 1}^{10,000}{f\left( w_{j} \right)^{\frac{3}{4}}}}$</p><h3 id="GloVe-词向量"><a href="#GloVe-词向量" class="headerlink" title="GloVe 词向量"></a>GloVe 词向量</h3><img title="Glove Model" src="http://pjvkppqit.bkt.clouddn.com/static/images/Glove Model.png"><p>GloVe算法做的就是使上下文和目标词关系开始明确化.$X_{ij}$是单词$i$在单词$j$上下文中出现的次数,那么这里$i$和$j$就和$t$和$c$的功能一样.若上下文指左右几个词,则会得出$X_{ij}$等于$X_{ji}$这个结论.其他时候大致相等.加权因子$f\left(X_{ij}\right)$就可以是一个函数,$X_{ij}$为0是为0(启发性方法见GloVe算法论文).$\theta_{i}$和$e_{j}$是对称的,而不像之前了解的模型,$\theta$和$e$功能不一样,因此最后结果可以取平均$e_{w}^{(final)}= \frac{e_{w} +\theta_{w}}{2}$.</p><p>GloVe差距最小化处理<br>$$\text{mini}\text{mize}\sum_{i = 1}^{10,000}{\sum_{j = 1}^{10,000}{f\left( X_{ij} \right)\left( \theta_{i}^{T}e_{j} + b_{i} + b_{j}^{‘} - logX_{ij} \right)^{2}}}$$</p><p>两个单词之间有多少联系,$t$和$c$之间有多紧密,$i$和$j$之间联系程度如何,换句话说就是他们同时出现的频率是多少,这是由这个$X_{ij}$影响的.然后梯度下降来最小化</p><img title="featurization view of word embeddings" src="http://pjvkppqit.bkt.clouddn.com/static/images/featurization view of word embeddings.png"><p>$$\left( A\theta_{i} \right)^{T}\left( A^{- T}e_{j} \right) = \theta_{i}^{T}A^{T}A^{- T}e_{j} = \theta_{i}^{T}e_{j}$$<br>通过GloVe算法得到的(关系)特征表示可能是原特征的潜在的任意线性变换,最终还是能学习出解决类似问题的平行四边形映射.</p><blockquote><p>Word2Vec,负采样,GloVe 词向量是三种学习词向量嵌入的方法.</p></blockquote><h3 id="情绪分类"><a href="#情绪分类" class="headerlink" title="情绪分类"></a>情绪分类</h3><p>情感分类一个最大的挑战就是可能标记的训练集没有那么多.对于情感分类任务来说,训练集大小从10,000到100,000个单词都很常见,甚至有时会小于10,000个单词,采用了词嵌入能够带来更好的效果,尤其是只有很小的训练集时.</p><img title="Simple sentiment classification model" src="http://pjvkppqit.bkt.clouddn.com/static/images/Simple sentiment classification model.png"><p>该算法实际上会把所有单词的意思给平均.问题就是没考虑词序.”Completely lacking in good taste, good service, and good ambiance.”,忽略词序,仅仅把所有单词的词嵌入加起来或者平均下来,分类器很可能认为这是一个好的评论.<br><img title="RNN sentiment classification" src="http://pjvkppqit.bkt.clouddn.com/static/images/RNN sentiment classification.png"></p><h3 id="词嵌入除偏"><a href="#词嵌入除偏" class="headerlink" title="词嵌入除偏"></a>词嵌入除偏</h3><p>根据训练模型所使用的文本,词嵌入能够反映出性别、种族、年龄、性取向等其他方面的偏见,如Man对应Computer Programmer,那么Woman会对应?输出是Homemaker.</p><img title="bias in word embedding" src="http://pjvkppqit.bkt.clouddn.com/static/images/bias in word embedding.png"><ol><li>偏差求平均</li><li>中和.对于那些定义不确切的词可以将其处理一下,避免偏见.如doctor和babysitter想使之在性别方面是中立的,而girl、boy定义本身就含有性别</li><li>均衡步.防止又引入其他偏差.</li></ol><p>论文作者训练一个分类器尝试解决哪些词是中立的.</p><h2 id="序列模型和注意力机制"><a href="#序列模型和注意力机制" class="headerlink" title="序列模型和注意力机制"></a>序列模型和注意力机制</h2><h3 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h3><ol><li>机器翻译到语音识别:seq2seq模型(Encoder-Decoder结构)</li><li>集束搜索(Beam search)和注意力模型(Attention Model)</li><li>音频模型</li></ol><h3 id="选择最可能的句子"><a href="#选择最可能的句子" class="headerlink" title="选择最可能的句子"></a>选择最可能的句子</h3><img title="Machine translation" src="http://pjvkppqit.bkt.clouddn.com/static/images/Machine translation.png"><p>机器翻译模型可以看作是条件语言模型,因为语言模型总是全0输入,随机地生成句子,机器翻译模型需要找到最可能的翻译,提供不同的输入(Encoder),目的是选择使句子出现可能性最大(Decoder),选择方法如Beam search,为什么不用贪心每次选择概率最大的一个词呢?这并不是最佳选择.</p><h3 id="集束搜索"><a href="#集束搜索" class="headerlink" title="集束搜索"></a>集束搜索</h3><img title="Beam search algorithm" src="http://pjvkppqit.bkt.clouddn.com/static/images/Beam search algorithm.png"> <img title="Beam search(B=3)" src="http://pjvkppqit.bkt.clouddn.com/static/images/Beam search(B=3).png"><p>当B=3时表示每次只考虑三个可能结果,B=1即为贪心</p><ol><li>在第一次词位置选出最可能的三个单词$y^{&lt;1>}$</li><li>在第一步基础上计算最可能的三个单词对$P(y^{&lt;1>},y^{&lt;2>}|x)$</li><li>继续增加下一个单词重复上述步骤</li></ol><h3 id="改进集束搜索"><a href="#改进集束搜索" class="headerlink" title="改进集束搜索"></a>改进集束搜索</h3><p>最大化<br>$P(y^{&lt; 1 >}\ldots y^{&lt; T_{y}>}|X)$=$P(y^{&lt;1>}|X)$*$P(y^{&lt; 2 >}|X,y^{&lt; 1 >})$*$P(y^{&lt; 3 >}|X,y^{&lt; 1\ &gt;},y^{&lt; 2>})\ldots$$P(y^{&lt; T_{y}\ &gt;}|X,y^{&lt;1\ &gt;}\ldots y^{&lt; T_{y} - 1\ &gt;})$</p><ol><li>改成最大化$logP(y|x)$,能防止数值下溢</li><li>原公式倾向于长度短小的翻译结果,因此可以长度归一化(除$T_{y}$)</li></ol><h3 id="定向搜索的误差分析"><a href="#定向搜索的误差分析" class="headerlink" title="定向搜索的误差分析"></a>定向搜索的误差分析</h3><img title="Error analysis on beam search" src="http://pjvkppqit.bkt.clouddn.com/static/images/Error analysis on beam search.png"><p>对结果将人工翻译和模型翻译对比,对比RNN模型出错率和集束搜索出错率,优化</p><h3 id="Bleu-得分"><a href="#Bleu-得分" class="headerlink" title="Bleu 得分"></a>Bleu 得分</h3><img title="Bleu score" src="http://pjvkppqit.bkt.clouddn.com/static/images/Bleu score.png"><p>这个例子中$p_{1}=5/7 p_{2}=4/6$,最后计算Bleu会在不同n-gram上取平均,但这样会侧重较短语句,因此会加上一个 BP(brevity penalty) 的惩罚因子.这给了机器翻译领域一个单一实数评估指标.</p><h3 id="Attention-模型"><a href="#Attention-模型" class="headerlink" title="Attention 模型"></a>Attention 模型</h3><img title="Attention" src="http://pjvkppqit.bkt.clouddn.com/static/images/Attention.png"> <img title="Computing attention alpha" src="http://pjvkppqit.bkt.clouddn.com/static/images/Computing attention alpha.png"><h3 id="语音识别"><a href="#语音识别" class="headerlink" title="语音识别"></a>语音识别</h3><p>略,没看懂</p><h3 id="触发字检测"><a href="#触发字检测" class="headerlink" title="触发字检测"></a>触发字检测</h3><p>把一个音频片段计算出它的声谱图特征得到特征向量</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://mooc.study.163.com/course/2001280004#/info" target="_blank" rel="noopener">网易云课堂</a></li><li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">Coursera Deep Learning 专项课程</a></li><li><a href="https://github.com/bighuang624/Andrew-Ng-Deep-Learning-notes/blob/master/docs/README.md" target="_blank" rel="noopener">吴恩达《深度学习》系列课程笔记</a></li></ul></div><div><div><div style="text-align:center;color:#ccc;font-size:16px;font-family:cursive">-------------纸短情长 <i class="fa fa-umbrella"></i> 下次再见-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong> 唐朝</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://tw19941212.github.io/posts/94c569ba/" title="序列模型">https://tw19941212.github.io/posts/94c569ba/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Coursera/" rel="tag"><i class="fa fa-tag"></i> Coursera</a> <a href="/tags/深度学习/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a> <a href="/tags/机器学习/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a> <a href="/tags/LSTM/" rel="tag"><i class="fa fa-tag"></i> LSTM</a> <a href="/tags/GRU/" rel="tag"><i class="fa fa-tag"></i> GRU</a> <a href="/tags/RNN/" rel="tag"><i class="fa fa-tag"></i> RNN</a> <a href="/tags/Attention/" rel="tag"><i class="fa fa-tag"></i> Attention</a> <a href="/tags/Word-Embedding/" rel="tag"><i class="fa fa-tag"></i> Word Embedding</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/563918f4/" rel="next" title="结构化机器学习项目"><i class="fa fa-chevron-left"></i> 结构化机器学习项目</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/posts/9a4ff7b0/" rel="prev" title="Dynamic Routing Between Capsules">Dynamic Routing Between Capsules <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC80MTY5Mi8xODIzOA=="></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="http://pjvkppqit.bkt.clouddn.com/static/images/avatar.jpeg" alt="唐朝"><p class="site-author-name" itemprop="name">唐朝</p><p class="site-description motion-element" itemprop="description">一只想转行的狗,超越姐姐保佑我</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">4</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">4</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">13</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tw19941212" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://weibo.com/p/1005053776889767" target="_blank" title="微博"><i class="fa fa-fw fa-weibo"></i>微博</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Recurrent-Neural-Networks"><span class="nav-number">1.</span> <span class="nav-text">Recurrent Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Notation"><span class="nav-number">1.1.</span> <span class="nav-text">Notation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Recurrent-Neural-Network-Model"><span class="nav-number">1.2.</span> <span class="nav-text">Recurrent Neural Network Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Backpropagation-through-time"><span class="nav-number">1.3.</span> <span class="nav-text">Backpropagation through time</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Different-types-of-RNNs"><span class="nav-number">1.4.</span> <span class="nav-text">Different types of RNNs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Language-model-and-sequence-generation"><span class="nav-number">1.5.</span> <span class="nav-text">Language model and sequence generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sampling-novel-sequences"><span class="nav-number">1.6.</span> <span class="nav-text">Sampling novel sequences</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanishing-gradients-with-RNNs"><span class="nav-number">1.7.</span> <span class="nav-text">Vanishing gradients with RNNs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gated-Recurrent-Unit-GRU"><span class="nav-number">1.8.</span> <span class="nav-text">Gated Recurrent Unit (GRU)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Long-Short-Term-Memory-LSTM"><span class="nav-number">1.9.</span> <span class="nav-text">Long Short Term Memory (LSTM)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-RNNs"><span class="nav-number">1.10.</span> <span class="nav-text">Deep RNNs</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#自然语言处理与词嵌入"><span class="nav-number">2.</span> <span class="nav-text">自然语言处理与词嵌入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#词汇表征"><span class="nav-number">2.1.</span> <span class="nav-text">词汇表征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用词嵌入"><span class="nav-number">2.2.</span> <span class="nav-text">使用词嵌入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#词嵌入的特性"><span class="nav-number">2.3.</span> <span class="nav-text">词嵌入的特性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#嵌入矩阵"><span class="nav-number">2.4.</span> <span class="nav-text">嵌入矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习词嵌入"><span class="nav-number">2.5.</span> <span class="nav-text">学习词嵌入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Word2Vec"><span class="nav-number">2.6.</span> <span class="nav-text">Word2Vec</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#负采样"><span class="nav-number">2.7.</span> <span class="nav-text">负采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GloVe-词向量"><span class="nav-number">2.8.</span> <span class="nav-text">GloVe 词向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#情绪分类"><span class="nav-number">2.9.</span> <span class="nav-text">情绪分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#词嵌入除偏"><span class="nav-number">2.10.</span> <span class="nav-text">词嵌入除偏</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#序列模型和注意力机制"><span class="nav-number">3.</span> <span class="nav-text">序列模型和注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基础模型"><span class="nav-number">3.1.</span> <span class="nav-text">基础模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#选择最可能的句子"><span class="nav-number">3.2.</span> <span class="nav-text">选择最可能的句子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#集束搜索"><span class="nav-number">3.3.</span> <span class="nav-text">集束搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#改进集束搜索"><span class="nav-number">3.4.</span> <span class="nav-text">改进集束搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#定向搜索的误差分析"><span class="nav-number">3.5.</span> <span class="nav-text">定向搜索的误差分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bleu-得分"><span class="nav-number">3.6.</span> <span class="nav-text">Bleu 得分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention-模型"><span class="nav-number">3.7.</span> <span class="nav-text">Attention 模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#语音识别"><span class="nav-number">3.8.</span> <span class="nav-text">语音识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#触发字检测"><span class="nav-number">3.9.</span> <span class="nav-text">触发字检测</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考链接"><span class="nav-number">4.</span> <span class="nav-text">参考链接</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-"></i> </span><span class="author" itemprop="copyrightHolder">唐朝</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">6.8k</span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_pv">本站总访问量<i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span>次 </span><span id="busuanzi_container_site_uv">本站访客数<i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span>人次</span></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">!function(e,t){var n,c=e.getElementsByTagName(t)[0];"function"!=typeof LivereTower&&((n=e.createElement(t)).src="https://cdn-city.livere.com/js/embed.dist.js",n.async=!0,c.parentNode.insertBefore(n,c))}(document,"script")</script><link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css"><script src="/lib/algolia-instant-search/instantsearch.min.js"></script><script src="/js/src/algolia-search.js?v=5.1.4"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("mW6MTVtJX0wR4pitdPFEST98-gzGzoHsz","hH4sKpU65t4ptsnIyX0z34l6")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),t.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"left","width":80,"height":160},"mobile":{"show":false}});</script></body></html><script type="text/javascript" src="/js/src/clicklove.js"></script>